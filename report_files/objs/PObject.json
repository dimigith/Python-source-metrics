{
    "rl": {
        "type": "directory",
        "code": null,
        "branches": [
            "rl.core",
            "rl.experiments",
            "rl.tests",
            "rl.__init__"
        ]
    },
    "rl.core": {
        "type": "directory",
        "code": null,
        "branches": [
            "rl.core.commands",
            "rl.core.configs",
            "rl.core.platform",
            "rl.core.rl",
            "rl.core.storage",
            "rl.core.utilities",
            "rl.core.__init__"
        ]
    },
    "rl.core.commands": {
        "type": "directory",
        "code": null,
        "branches": [
            "rl.core.commands.commands",
            "rl.core.commands.process_results",
            "rl.core.commands.run_episode_command",
            "rl.core.commands.__init__"
        ]
    },
    "rl.core.commands.commands": {
        "type": "module",
        "code": "from rl.core.commands.run_episode_command import RunEpisodeCommand\nfrom rl.core.commands.process_results import ProcessResultsCommand\n\nCOMMANDS = [\n    RunEpisodeCommand,\n    ProcessResultsCommand\n]\n\n\ndef get_command(alias):\n    for cmd in COMMANDS:\n        if cmd.alias == alias:\n            return cmd\n    raise ValueError(f\"Command {alias} does not exist. Command options: {[cmd.alias for cmd in COMMANDS]}\")\n",
        "branches": [
            "rl.core.commands.commands.get_command"
        ]
    },
    "rl.core.commands.commands.get_command": {
        "type": "function",
        "code": "def get_command(alias):\n    for cmd in COMMANDS:\n        if cmd.alias == alias:\n            return cmd\n    raise ValueError(f\"Command {alias} does not exist. Command options: {[cmd.alias for cmd in COMMANDS]}\")",
        "branches": []
    },
    "rl.core.commands.process_results": {
        "type": "module",
        "code": "from rl.core.configs.path_confgis import EXPERIMENT_DATAFRAMES_DIRECTORY_ABSPATH,\\\n    EXPERIMENT_DATABASE_OBJECT_ABSPATH\nfrom rl.core.configs.storage_configs import DB_EXPERIMENT_TABLE_NAME_COL_EXPID\nfrom rl.core.platform.command import AbstractCommand\nfrom rl.core.storage.storage import compress_data_df\nfrom rl.core.utilities.logging import Logger\n\n# TODO\n# process and transform results\n# store results in db\nfrom rl.core.utilities.profiling import cprofile\n\n\nclass ProcessResultsCommand(AbstractCommand):\n\n    alias = 'pr'\n    logger = Logger(f\"Command {alias}\")\n\n    def __init__(self):\n        super().__init__(input_dir=EXPERIMENT_DATAFRAMES_DIRECTORY_ABSPATH,\n                         output_dir=EXPERIMENT_DATABASE_OBJECT_ABSPATH)\n\n    def input(self):\n        files = self.read_file()\n        self._file = files[0]\n        self._input_df = self.read_file(self._file)\n\n    @cprofile\n    @logger.log_func_call()\n    def run(self):\n        self.transformed_df = compress_data_df(self._input_df)\n        self.transformed_df.loc[:, DB_EXPERIMENT_TABLE_NAME_COL_EXPID] = self._file\n\n        # self.agg_df = self.transformed_df.agg()\n        pass\n\n    def output(self):\n        pass\n",
        "branches": [
            "rl.core.commands.process_results.ProcessResultsCommand"
        ]
    },
    "rl.core.commands.process_results.ProcessResultsCommand": {
        "type": "class",
        "code": "class ProcessResultsCommand(AbstractCommand):\n\n    alias = 'pr'\n    logger = Logger(f\"Command {alias}\")\n\n    def __init__(self):\n        super().__init__(input_dir=EXPERIMENT_DATAFRAMES_DIRECTORY_ABSPATH,\n                         output_dir=EXPERIMENT_DATABASE_OBJECT_ABSPATH)\n\n    def input(self):\n        files = self.read_file()\n        self._file = files[0]\n        self._input_df = self.read_file(self._file)\n\n    @cprofile\n    @logger.log_func_call()\n    def run(self):\n        self.transformed_df = compress_data_df(self._input_df)\n        self.transformed_df.loc[:, DB_EXPERIMENT_TABLE_NAME_COL_EXPID] = self._file\n\n        # self.agg_df = self.transformed_df.agg()\n        pass\n\n    def output(self):\n        pass",
        "branches": [
            "rl.core.commands.process_results.ProcessResultsCommand.__init__",
            "rl.core.commands.process_results.ProcessResultsCommand.input",
            "rl.core.commands.process_results.ProcessResultsCommand.run",
            "rl.core.commands.process_results.ProcessResultsCommand.output"
        ]
    },
    "rl.core.commands.process_results.ProcessResultsCommand.__init__": {
        "type": "class_method",
        "code": "def __init__(self):\n    super().__init__(input_dir=EXPERIMENT_DATAFRAMES_DIRECTORY_ABSPATH,\n                     output_dir=EXPERIMENT_DATABASE_OBJECT_ABSPATH)",
        "branches": []
    },
    "rl.core.commands.process_results.ProcessResultsCommand.input": {
        "type": "class_method",
        "code": "def input(self):\n    files = self.read_file()\n    self._file = files[0]\n    self._input_df = self.read_file(self._file)",
        "branches": []
    },
    "rl.core.commands.process_results.ProcessResultsCommand.run": {
        "type": "class_method",
        "code": "@cprofile\n@logger.log_func_call()\ndef run(self):\n    self.transformed_df = compress_data_df(self._input_df)\n    self.transformed_df.loc[:, DB_EXPERIMENT_TABLE_NAME_COL_EXPID] = self._file\n\n    # self.agg_df = self.transformed_df.agg()\n    pass",
        "branches": []
    },
    "rl.core.commands.process_results.ProcessResultsCommand.output": {
        "type": "class_method",
        "code": "def output(self):\n    pass",
        "branches": []
    },
    "rl.core.commands.run_episode_command": {
        "type": "module",
        "code": "from rl.core.configs.path_confgis import EXPERIMENT_EXP_INPUTS_DIRECTORY_ABSPATH, \\\n    EXPERIMENT_DATAFRAMES_DIRECTORY_ABSPATH\nfrom rl.core.platform.command import AbstractCommand\nfrom rl.core.utilities.profiling import cprofile\nfrom rl.core.utilities.logging import Logger\nfrom rl.core.rl.agents import get_agent\nfrom rl.core.rl.envs import get_env\nfrom rl.core.rl.engine import run_episodes\nfrom rl.core.utilities.timestamp import timestamp_unique_str\nfrom rl.core.storage.storage import data_to_df\n\n\nclass RunEpisodeCommand(AbstractCommand):\n\n    alias = 're'\n    logger = Logger(f\"Command {alias}\")\n\n    def __init__(self):\n        super().__init__(input_dir=EXPERIMENT_EXP_INPUTS_DIRECTORY_ABSPATH,\n                         output_dir=EXPERIMENT_DATAFRAMES_DIRECTORY_ABSPATH)\n\n    def input(self):\n        run_params = self.read_file('run_parameters.json')\n        self.__agent = get_agent(run_params['agent_name'])(**run_params['agent_params'])\n        self.__env = get_env(run_params['env_name'])(**run_params['env_params'])\n        self.__run_kwargs = run_params['run_kwargs']\n        self.__n_repeat = int(run_params['n_repeat']) if 'n_repeat' in run_params.keys() else 1\n\n    @cprofile\n    @logger.log_func_call()\n    def run(self):\n        agent_id = self.__agent.name\n        env_id = self.__env.name\n        run_id = timestamp_unique_str()\n        self.res_dict = {}\n        for i in range(self.__n_repeat):\n            exp_id = f\"{agent_id}__{env_id}__{run_id}__{i}\"\n            res = run_episodes(self.__env,\n                                self.__agent,\n                                self.__run_kwargs['n_episodes'],\n                                storage_dict=None,\n                                #storage_dict=res_dict,\n                                render=False\n                                )\n            self.res_dict[exp_id] = data_to_df(res)\n\n    def output(self):\n        for key, df in self.res_dict.items():\n            self.write_to_file(df, f\"{key}.csv\")\n\n\n\n\n",
        "branches": [
            "rl.core.commands.run_episode_command.RunEpisodeCommand"
        ]
    },
    "rl.core.commands.run_episode_command.RunEpisodeCommand": {
        "type": "class",
        "code": "class RunEpisodeCommand(AbstractCommand):\n\n    alias = 're'\n    logger = Logger(f\"Command {alias}\")\n\n    def __init__(self):\n        super().__init__(input_dir=EXPERIMENT_EXP_INPUTS_DIRECTORY_ABSPATH,\n                         output_dir=EXPERIMENT_DATAFRAMES_DIRECTORY_ABSPATH)\n\n    def input(self):\n        run_params = self.read_file('run_parameters.json')\n        self.__agent = get_agent(run_params['agent_name'])(**run_params['agent_params'])\n        self.__env = get_env(run_params['env_name'])(**run_params['env_params'])\n        self.__run_kwargs = run_params['run_kwargs']\n        self.__n_repeat = int(run_params['n_repeat']) if 'n_repeat' in run_params.keys() else 1\n\n    @cprofile\n    @logger.log_func_call()\n    def run(self):\n        agent_id = self.__agent.name\n        env_id = self.__env.name\n        run_id = timestamp_unique_str()\n        self.res_dict = {}\n        for i in range(self.__n_repeat):\n            exp_id = f\"{agent_id}__{env_id}__{run_id}__{i}\"\n            res = run_episodes(self.__env,\n                                self.__agent,\n                                self.__run_kwargs['n_episodes'],\n                                storage_dict=None,\n                                #storage_dict=res_dict,\n                                render=False\n                                )\n            self.res_dict[exp_id] = data_to_df(res)\n\n    def output(self):\n        for key, df in self.res_dict.items():\n            self.write_to_file(df, f\"{key}.csv\")",
        "branches": [
            "rl.core.commands.run_episode_command.RunEpisodeCommand.__init__",
            "rl.core.commands.run_episode_command.RunEpisodeCommand.input",
            "rl.core.commands.run_episode_command.RunEpisodeCommand.run",
            "rl.core.commands.run_episode_command.RunEpisodeCommand.output"
        ]
    },
    "rl.core.commands.run_episode_command.RunEpisodeCommand.__init__": {
        "type": "class_method",
        "code": "def __init__(self):\n    super().__init__(input_dir=EXPERIMENT_EXP_INPUTS_DIRECTORY_ABSPATH,\n                     output_dir=EXPERIMENT_DATAFRAMES_DIRECTORY_ABSPATH)",
        "branches": []
    },
    "rl.core.commands.run_episode_command.RunEpisodeCommand.input": {
        "type": "class_method",
        "code": "def input(self):\n    run_params = self.read_file('run_parameters.json')\n    self.__agent = get_agent(run_params['agent_name'])(**run_params['agent_params'])\n    self.__env = get_env(run_params['env_name'])(**run_params['env_params'])\n    self.__run_kwargs = run_params['run_kwargs']\n    self.__n_repeat = int(run_params['n_repeat']) if 'n_repeat' in run_params.keys() else 1",
        "branches": []
    },
    "rl.core.commands.run_episode_command.RunEpisodeCommand.run": {
        "type": "class_method",
        "code": "@cprofile\n@logger.log_func_call()\ndef run(self):\n    agent_id = self.__agent.name\n    env_id = self.__env.name\n    run_id = timestamp_unique_str()\n    self.res_dict = {}\n    for i in range(self.__n_repeat):\n        exp_id = f\"{agent_id}__{env_id}__{run_id}__{i}\"\n        res = run_episodes(self.__env,\n                            self.__agent,\n                            self.__run_kwargs['n_episodes'],\n                            storage_dict=None,\n                            #storage_dict=res_dict,\n                            render=False\n                            )\n        self.res_dict[exp_id] = data_to_df(res)",
        "branches": []
    },
    "rl.core.commands.run_episode_command.RunEpisodeCommand.output": {
        "type": "class_method",
        "code": "def output(self):\n    for key, df in self.res_dict.items():\n        self.write_to_file(df, f\"{key}.csv\")",
        "branches": []
    },
    "rl.core.commands.__init__": {
        "type": "module",
        "code": "",
        "branches": []
    },
    "rl.core.configs": {
        "type": "directory",
        "code": null,
        "branches": [
            "rl.core.configs.experiment_configs",
            "rl.core.configs.general_configs",
            "rl.core.configs.log_configs",
            "rl.core.configs.path_confgis",
            "rl.core.configs.storage_configs",
            "rl.core.configs.__init__"
        ]
    },
    "rl.core.configs.experiment_configs": {
        "type": "module",
        "code": "from os.path import join\n\nfrom rl.src import PROJECT_ROOT_ABSPATH,\\\n    DEFAULT_STORE_DATAFRAMES_DIRECTORY_PATH,\\\n    DEFAULT_STORE_DATABASES_DIRECTORY_PATH,\\\n    DEFAULT_STORE_DATABASE_OBJECT_PATH\n\nDEFAULT_EXPERIMENT_PATH = 'experiments/test/experiment/'\nEXPERIMENT_PATH = DEFAULT_EXPERIMENT_PATH\n\nSTORE_DATAFRAMES_DIRECTORY_ABSPATH = join(PROJECT_ROOT_ABSPATH,\n                                          EXPERIMENT_PATH,\n                                          DEFAULT_STORE_DATAFRAMES_DIRECTORY_PATH)\nSTORE_DATABASES_DIRECTORY_ABSPATH = join(PROJECT_ROOT_ABSPATH,\n                                          EXPERIMENT_PATH,\n                                          DEFAULT_STORE_DATABASES_DIRECTORY_PATH)\nSTORE_DATABASE_OBJECT_ABSPATH = join(PROJECT_ROOT_ABSPATH,\n                                          EXPERIMENT_PATH,\n                                          DEFAULT_STORE_DATABASE_OBJECT_PATH)\n\n\n\n",
        "branches": []
    },
    "rl.core.configs.general_configs": {
        "type": "module",
        "code": "from os.path import join\nfrom configparser import ConfigParser\n\n\n# PROJECT_ROOT_DIRECTORY = \"Reinforcement-learning-sandbox\"\n# PROJECT_ROOT_ABSPATH = join(__file__.split(PROJECT_ROOT_DIRECTORY)[0], PROJECT_ROOT_DIRECTORY)\n# print(\"PROJECT_DIRECTORY\", PROJECT_ROOT_ABSPATH)\n#\n#\n# RUN_CONFIGS_FILENAME = 'run_config.ini'\n# RUN_CONFIGS_ABSPATH = join(PROJECT_ROOT_ABSPATH, RUN_CONFIGS_FILENAME)\n# print(\"RUN_CONFIGS_ABSPATH\", RUN_CONFIGS_ABSPATH)\n#\n# __configs = ConfigParser()\n# __configs.read(RUN_CONFIGS_ABSPATH)\n#\n# EXPERIMENT_ROOT_DIRECTORY = join(__configs['DEFAULTS']['experiments_dir'],\n#                                  __configs['EXPERIMENT']['module'])\n# EXPERIMENT_ROOT_ABSPATH = join(PROJECT_ROOT_ABSPATH, EXPERIMENT_ROOT_DIRECTORY)\n# print(\"EXPERIMENT_ROOT_ABSPATH\", EXPERIMENT_ROOT_ABSPATH)\n\nSTORE_COMPRESSED_DATA = False\n# DB_DEFAULT_DB_NAME = 'rl.sqlite3'\n#\n# DEFAULT_STORE_FILES_DIRECTORY_PATH = 'files/'\n# DEFAULT_STORE_RESULTS_DIRECTORY_PATH = join(DEFAULT_STORE_FILES_DIRECTORY_PATH, 'results/')\n# DEFAULT_STORE_DATAFRAMES_DIRECTORY_PATH = join(DEFAULT_STORE_RESULTS_DIRECTORY_PATH, 'dataframes/')\n# DEFAULT_STORE_DATABASE_DIRECTORY_PATH = join(DEFAULT_STORE_RESULTS_DIRECTORY_PATH, 'databases/')\n# DEFAULT_STORE_DATABASE_OBJECT_PATH = join(DEFAULT_STORE_DATABASE_DIRECTORY_PATH, DB_DEFAULT_DB_NAME)\n# DEFAULT_STORE_LOGS_DIRECTORY_PATH = join(DEFAULT_STORE_FILES_DIRECTORY_PATH, 'logs/')\n# DEFAULT_STORE_INPUTS_OUTPUTS_DIRECTORY_PATH = join(DEFAULT_STORE_FILES_DIRECTORY_PATH, 'transient/')\n# DEFAULT_STORE_PERFMONITORING_DIRECTORY_PATH = join(DEFAULT_STORE_LOGS_DIRECTORY_PATH, 'performance_monitoring/')\n#\n#\n# EXPERIMENT_STORE_DATAFRAMES_DIRECTORY_ABSPATH = join(EXPERIMENT_ROOT_ABSPATH, DEFAULT_STORE_DATAFRAMES_DIRECTORY_PATH)\n# EXPERIMENT_STORE_DATABASE_DIRECTORY_ABSPATH = join(EXPERIMENT_ROOT_ABSPATH, DEFAULT_STORE_DATABASE_DIRECTORY_PATH)\n# EXPERIMENT_STORE_DATABASE_OBJECT_ABSPATH = join(EXPERIMENT_STORE_DATABASE_DIRECTORY_ABSPATH, DB_DEFAULT_DB_NAME)\n# EXPERIMENT_STORE_LOGS_DIRECTORY_ABSPATH = join(EXPERIMENT_ROOT_ABSPATH, DEFAULT_STORE_LOGS_DIRECTORY_PATH)\n# EXPERIMENT_STORE_INPUTS_OUTPUTS_DIRECTORY_ABSPATH = join(EXPERIMENT_ROOT_ABSPATH, DEFAULT_STORE_INPUTS_OUTPUTS_DIRECTORY_PATH)\n# EXPERIMENT_STORE_PERFMONITORING_DIRECTORY_ABSPATH = join(EXPERIMENT_ROOT_ABSPATH, DEFAULT_STORE_PERFMONITORING_DIRECTORY_PATH)\n\n\nTIMESTAMP_STRING_FORMAT = \"%Y-%d-%m_%H-%M-%S\"\nTIMESTAMP_LONG_STRING_FORMAT = TIMESTAMP_STRING_FORMAT+\"_%f\"\n\nCPROFILE_COMMAND_EXECUTION_FLAG = True\n\n\n",
        "branches": []
    },
    "rl.core.configs.log_configs": {
        "type": "module",
        "code": "GENERAL_LOG_FLAG = True\nGENERAL_LOG_STDOUT_FLAG = True\nGENERAL_LOG_TIMINGS_FLAG = True\nLOGGING_ALLOW_GRAPHS = True\nLOGGING_VERBOSITY = 3\n\nLOG_CSV_DIR_PATH = 'log_csvs'\nLOG_HTML_DIR_PATH = 'log_html'\nPERFORMANCE_MONITORING_DIR_PATH = 'performance_monitoring'\nLOG_IMAGES_DIR_PATH = 'images'\n\nCSV_FILENAME_EXTENSION_FUNCTION_TIMES_CSV = 'function_times'\nCSV_FILENAME_EXTENSION_LOGS_CSV = 'logs'\n",
        "branches": []
    },
    "rl.core.configs.path_confgis": {
        "type": "module",
        "code": "from os import makedirs\nfrom os.path import exists, split, join, isfile\nfrom configparser import ConfigParser\n\n\ndef create_path(path, *args):\n    path = join(path, *args)\n\n    if isfile(path):\n        path = split(path)[0]\n\n    if not exists(path):\n        makedirs(path)\n\n    return path\n\n\nROOT_DIRECTORY = \"Reinforcement-learning-sandbox\"\nROOT_ABSPATH = join(__file__.split(ROOT_DIRECTORY)[0], ROOT_DIRECTORY)\nprint(\"PROJECT_DIRECTORY\", ROOT_ABSPATH)\n\nRUN_CONFIGS_FILENAME = 'run_config.ini'\nRUN_CONFIGS_ABSPATH = join(ROOT_ABSPATH, RUN_CONFIGS_FILENAME)\nprint(\"RUN_CONFIGS_ABSPATH\", RUN_CONFIGS_ABSPATH)\n\n__configs = ConfigParser()\n__configs.read(RUN_CONFIGS_ABSPATH)\n\n\nEXPERIMENT_ROOT_DIRECTORY = join(__configs['DEFAULTS']['experiments_dir'],\n                                 __configs['EXPERIMENT']['module'])\nEXPERIMENT_ROOT_ABSPATH = join(ROOT_ABSPATH, EXPERIMENT_ROOT_DIRECTORY)\nprint(\"EXPERIMENT_ROOT_ABSPATH\", EXPERIMENT_ROOT_ABSPATH)\n\nDB_DEFAULT_DB_NAME = 'rl.sqlite3'\n\nEXPERIMENT_RESULTS_DIRECTORY_ABSPATH = create_path(EXPERIMENT_ROOT_ABSPATH, 'exp_results/')\nEXPERIMENT_DATAFRAMES_DIRECTORY_ABSPATH = create_path(EXPERIMENT_RESULTS_DIRECTORY_ABSPATH, 'dataframes/')\nEXPERIMENT_DATABASE_DIRECTORY_ABSPATH = create_path(EXPERIMENT_RESULTS_DIRECTORY_ABSPATH, 'databases/')\nEXPERIMENT_DATABASE_OBJECT_ABSPATH = join(EXPERIMENT_DATABASE_DIRECTORY_ABSPATH, DB_DEFAULT_DB_NAME)\n\nEXPERIMENT_LOGS_DIRECTORY_ABSPATH = create_path(EXPERIMENT_ROOT_ABSPATH, 'logs/')\nEXPERIMENT_LOG_IMAGES_DIRECTORY_ABSPATH = create_path(EXPERIMENT_LOGS_DIRECTORY_ABSPATH, 'images/')\n\nEXPERIMENT_REPORTS_DIRECTORY_ABSPATH = create_path(EXPERIMENT_ROOT_ABSPATH, 'reports/')\nEXPERIMENT_EXP_INPUTS_DIRECTORY_ABSPATH = create_path(EXPERIMENT_ROOT_ABSPATH, 'input_params/')\nEXPERIMENT_PERFMONITORING_DIRECTORY_ABSPATH = create_path(EXPERIMENT_ROOT_ABSPATH, 'performance_monitoring/')\n\n\n",
        "branches": [
            "rl.core.configs.path_confgis.create_path"
        ]
    },
    "rl.core.configs.path_confgis.create_path": {
        "type": "function",
        "code": "def create_path(path, *args):\n    path = join(path, *args)\n\n    if isfile(path):\n        path = split(path)[0]\n\n    if not exists(path):\n        makedirs(path)\n\n    return path",
        "branches": []
    },
    "rl.core.configs.storage_configs": {
        "type": "module",
        "code": "UNIQUE_STRING_LENGHT = 10\n\nDB_EXPERIMENT_TABLE_NAME_COL_EXPID = \"experiment_id\"\nDB_EXPERIMENT_TABLE_NAME_COL_EPISODES = \"episodes\"\nDB_EXPERIMENT_TABLE_NAME_COL_STEPS = \"steps_list\"\nDB_EXPERIMENT_TABLE_NAME_COL_STATES = \"states\"\nDB_EXPERIMENT_TABLE_NAME_COL_NEXTSTATES = \"next_states\"\nDB_EXPERIMENT_TABLE_NAME_COL_ACTIONS = \"actions\"\nDB_EXPERIMENT_TABLE_NAME_COL_REWARDS = \"rewards\"\nDB_EXPERIMENT_TABLE_NAME_COL_DONE = \"dones\"\n\n\nDATA_COLUMNS = [DB_EXPERIMENT_TABLE_NAME_COL_EPISODES,\n                DB_EXPERIMENT_TABLE_NAME_COL_STEPS,\n                DB_EXPERIMENT_TABLE_NAME_COL_STATES,\n                DB_EXPERIMENT_TABLE_NAME_COL_NEXTSTATES,\n                DB_EXPERIMENT_TABLE_NAME_COL_ACTIONS,\n                DB_EXPERIMENT_TABLE_NAME_COL_REWARDS,\n                DB_EXPERIMENT_TABLE_NAME_COL_DONE]\n",
        "branches": []
    },
    "rl.core.configs.__init__": {
        "type": "module",
        "code": "",
        "branches": []
    },
    "rl.core.platform": {
        "type": "directory",
        "code": null,
        "branches": [
            "rl.core.platform.command",
            "rl.core.platform.__init__"
        ]
    },
    "rl.core.platform.command": {
        "type": "module",
        "code": "from os import listdir\nfrom os.path import splitext, join\n\nimport pandas as pd\n\nfrom rl.core.configs.path_confgis import EXPERIMENT_ROOT_ABSPATH\nfrom rl.core.utilities.file_utils import read_json_file, store_df, read_df\n\n\n# def get_configs_for_command(command):\n#     #TODO enable multiple comfigs for different commands\n#     # configs = read_run_configs()\n#     path = EXPERIMENT_INPUTS_OUTPUTS_DIRECTORY_ABSPATH\n#     input_dir, output_dir = path, path\n#     return input_dir, output_dir\n\n\ndef run_command(command):\n    # try: TODO\n    # input_dir, output_dir = get_configs_for_command(command.alias)\n    # cmd_obj = command(input_dir, output_dir)\n    cmd_obj = command()\n    cmd_obj.input()\n    cmd_obj.run()\n    cmd_obj.output()\n    # except Exception as e:\n    #     print(e)\n    #     pass\n\n\n# task decorator (celery or aws)\nclass AbstractCommand:\n\n    def __init__(self, input_dir, output_dir):\n        self.__input_dir = join(EXPERIMENT_ROOT_ABSPATH, input_dir)\n        self.__output_dir = join(EXPERIMENT_ROOT_ABSPATH, output_dir)\n        pass\n\n    @property\n    def input_dir(self):\n        return self.__input_dir\n\n    @property\n    def output_dir(self):\n        return self.__output_dir\n\n    def input(self):\n        \"\"\"\n        only file input operations (read/load/etc)\n        \"\"\"\n        #TODO read input files\n        pass\n\n    def run(self):\n        pass\n\n    def output(self):\n        \"\"\"\n        only file output operations (write/upload to db/etc)\n        \"\"\"\n        #TODO write output to files\n        pass\n\n    def __repr__(self):\n        return str(self.__class__)\n\n    def read_file(self, file=None):\n        if file is None:\n            return listdir(self.__input_dir)\n\n        filename, ext = splitext(file)\n        ext = ext[1:] if len(ext) > 1 else None\n        abspath = join(self.__input_dir, file)\n        if ext is None:\n            raise ValueError(f\"Not recognized file {file}. Extension: {ext}\")\n        elif ext == 'json':\n            result = read_json_file(abspath)\n        elif ext == 'csv':\n            result = read_df(abspath)\n        elif ext == 'txt':\n            raise ValueError(f\"Not recognized file {file}. Extension: {ext}\")\n        else:\n            raise ValueError(f\"Not recognized file {file}. Extension: {ext}\")\n\n        return result\n\n    def write_to_file(self, data, filename):\n        if isinstance(data, pd.DataFrame):\n            abspath = join(self.__output_dir, filename)\n            store_df(data, abspath)\n        else:\n            raise ValueError(f\"Not recognized data type {type(data)}. end file: {filename}\")\n\n    def clean_file(self, abspath):\n        # TODO append filename to be cleaned after execution\n        pass\n",
        "branches": [
            "rl.core.platform.command.run_command",
            "rl.core.platform.command.AbstractCommand"
        ]
    },
    "rl.core.platform.command.run_command": {
        "type": "function",
        "code": "def run_command(command):\n    # try: TODO\n    # input_dir, output_dir = get_configs_for_command(command.alias)\n    # cmd_obj = command(input_dir, output_dir)\n    cmd_obj = command()\n    cmd_obj.input()\n    cmd_obj.run()\n    cmd_obj.output()",
        "branches": []
    },
    "rl.core.platform.command.AbstractCommand": {
        "type": "class",
        "code": "class AbstractCommand:\n\n    def __init__(self, input_dir, output_dir):\n        self.__input_dir = join(EXPERIMENT_ROOT_ABSPATH, input_dir)\n        self.__output_dir = join(EXPERIMENT_ROOT_ABSPATH, output_dir)\n        pass\n\n    @property\n    def input_dir(self):\n        return self.__input_dir\n\n    @property\n    def output_dir(self):\n        return self.__output_dir\n\n    def input(self):\n        \"\"\"\n        only file input operations (read/load/etc)\n        \"\"\"\n        #TODO read input files\n        pass\n\n    def run(self):\n        pass\n\n    def output(self):\n        \"\"\"\n        only file output operations (write/upload to db/etc)\n        \"\"\"\n        #TODO write output to files\n        pass\n\n    def __repr__(self):\n        return str(self.__class__)\n\n    def read_file(self, file=None):\n        if file is None:\n            return listdir(self.__input_dir)\n\n        filename, ext = splitext(file)\n        ext = ext[1:] if len(ext) > 1 else None\n        abspath = join(self.__input_dir, file)\n        if ext is None:\n            raise ValueError(f\"Not recognized file {file}. Extension: {ext}\")\n        elif ext == 'json':\n            result = read_json_file(abspath)\n        elif ext == 'csv':\n            result = read_df(abspath)\n        elif ext == 'txt':\n            raise ValueError(f\"Not recognized file {file}. Extension: {ext}\")\n        else:\n            raise ValueError(f\"Not recognized file {file}. Extension: {ext}\")\n\n        return result\n\n    def write_to_file(self, data, filename):\n        if isinstance(data, pd.DataFrame):\n            abspath = join(self.__output_dir, filename)\n            store_df(data, abspath)\n        else:\n            raise ValueError(f\"Not recognized data type {type(data)}. end file: {filename}\")\n\n    def clean_file(self, abspath):\n        # TODO append filename to be cleaned after execution\n        pass",
        "branches": [
            "rl.core.platform.command.AbstractCommand.__init__",
            "rl.core.platform.command.AbstractCommand.input_dir",
            "rl.core.platform.command.AbstractCommand.output_dir",
            "rl.core.platform.command.AbstractCommand.input",
            "rl.core.platform.command.AbstractCommand.run",
            "rl.core.platform.command.AbstractCommand.output",
            "rl.core.platform.command.AbstractCommand.__repr__",
            "rl.core.platform.command.AbstractCommand.read_file",
            "rl.core.platform.command.AbstractCommand.write_to_file",
            "rl.core.platform.command.AbstractCommand.clean_file"
        ]
    },
    "rl.core.platform.command.AbstractCommand.__init__": {
        "type": "class_method",
        "code": "def __init__(self, input_dir, output_dir):\n    self.__input_dir = join(EXPERIMENT_ROOT_ABSPATH, input_dir)\n    self.__output_dir = join(EXPERIMENT_ROOT_ABSPATH, output_dir)\n    pass",
        "branches": []
    },
    "rl.core.platform.command.AbstractCommand.input_dir": {
        "type": "class_method",
        "code": "@property\ndef input_dir(self):\n    return self.__input_dir",
        "branches": []
    },
    "rl.core.platform.command.AbstractCommand.output_dir": {
        "type": "class_method",
        "code": "@property\ndef output_dir(self):\n    return self.__output_dir",
        "branches": []
    },
    "rl.core.platform.command.AbstractCommand.input": {
        "type": "class_method",
        "code": "def input(self):\n    \"\"\"\n    only file input operations (read/load/etc)\n    \"\"\"\n    #TODO read input files\n    pass",
        "branches": []
    },
    "rl.core.platform.command.AbstractCommand.run": {
        "type": "class_method",
        "code": "def run(self):\n    pass",
        "branches": []
    },
    "rl.core.platform.command.AbstractCommand.output": {
        "type": "class_method",
        "code": "def output(self):\n    \"\"\"\n    only file output operations (write/upload to db/etc)\n    \"\"\"\n    #TODO write output to files\n    pass",
        "branches": []
    },
    "rl.core.platform.command.AbstractCommand.__repr__": {
        "type": "class_method",
        "code": "def __repr__(self):\n    return str(self.__class__)",
        "branches": []
    },
    "rl.core.platform.command.AbstractCommand.read_file": {
        "type": "class_method",
        "code": "def read_file(self, file=None):\n    if file is None:\n        return listdir(self.__input_dir)\n\n    filename, ext = splitext(file)\n    ext = ext[1:] if len(ext) > 1 else None\n    abspath = join(self.__input_dir, file)\n    if ext is None:\n        raise ValueError(f\"Not recognized file {file}. Extension: {ext}\")\n    elif ext == 'json':\n        result = read_json_file(abspath)\n    elif ext == 'csv':\n        result = read_df(abspath)\n    elif ext == 'txt':\n        raise ValueError(f\"Not recognized file {file}. Extension: {ext}\")\n    else:\n        raise ValueError(f\"Not recognized file {file}. Extension: {ext}\")\n\n    return result",
        "branches": []
    },
    "rl.core.platform.command.AbstractCommand.write_to_file": {
        "type": "class_method",
        "code": "def write_to_file(self, data, filename):\n    if isinstance(data, pd.DataFrame):\n        abspath = join(self.__output_dir, filename)\n        store_df(data, abspath)\n    else:\n        raise ValueError(f\"Not recognized data type {type(data)}. end file: {filename}\")",
        "branches": []
    },
    "rl.core.platform.command.AbstractCommand.clean_file": {
        "type": "class_method",
        "code": "def clean_file(self, abspath):\n    # TODO append filename to be cleaned after execution\n    pass",
        "branches": []
    },
    "rl.core.platform.__init__": {
        "type": "module",
        "code": "",
        "branches": []
    },
    "rl.core.rl": {
        "type": "directory",
        "code": null,
        "branches": [
            "rl.core.rl.agent",
            "rl.core.rl.agents",
            "rl.core.rl.engine",
            "rl.core.rl.env",
            "rl.core.rl.envs",
            "rl.core.rl.__init__"
        ]
    },
    "rl.core.rl.agent": {
        "type": "module",
        "code": "class AbstractAgent:\n\n    name = None\n\n    def set_env(self, env):\n        pass\n\n    def act(self, state):\n        raise NotImplementedError\n\n    def observe(self, state, action, reward, next_state, done):\n        pass\n",
        "branches": [
            "rl.core.rl.agent.AbstractAgent"
        ]
    },
    "rl.core.rl.agent.AbstractAgent": {
        "type": "class",
        "code": "class AbstractAgent:\n\n    name = None\n\n    def set_env(self, env):\n        pass\n\n    def act(self, state):\n        raise NotImplementedError\n\n    def observe(self, state, action, reward, next_state, done):\n        pass",
        "branches": [
            "rl.core.rl.agent.AbstractAgent.set_env",
            "rl.core.rl.agent.AbstractAgent.act",
            "rl.core.rl.agent.AbstractAgent.observe"
        ]
    },
    "rl.core.rl.agent.AbstractAgent.set_env": {
        "type": "class_method",
        "code": "def set_env(self, env):\n    pass",
        "branches": []
    },
    "rl.core.rl.agent.AbstractAgent.act": {
        "type": "class_method",
        "code": "def act(self, state):\n    raise NotImplementedError",
        "branches": []
    },
    "rl.core.rl.agent.AbstractAgent.observe": {
        "type": "class_method",
        "code": "def observe(self, state, action, reward, next_state, done):\n    pass",
        "branches": []
    },
    "rl.core.rl.agents": {
        "type": "module",
        "code": "from rl.experiments.simple_2d.my_agent import *\n\nAGENTS = {\n    MyAgent_Greedy_SE_POC\n}\n\n\ndef get_agent(agent_name):\n    for agent in AGENTS:\n        if agent.name == agent_name:\n            return agent\n",
        "branches": [
            "rl.core.rl.agents.get_agent"
        ]
    },
    "rl.core.rl.agents.get_agent": {
        "type": "function",
        "code": "def get_agent(agent_name):\n    for agent in AGENTS:\n        if agent.name == agent_name:\n            return agent",
        "branches": []
    },
    "rl.core.rl.engine": {
        "type": "module",
        "code": "import time\nfrom tqdm import tqdm\n\nimport numpy as np\n\nfrom rl.core.utilities.logging import log\nfrom rl.core.utilities.timestamp import timestamp_str\nfrom rl.core.configs.storage_configs import DATA_COLUMNS,\\\n                                        DB_EXPERIMENT_TABLE_NAME_COL_EPISODES,\\\n                                        DB_EXPERIMENT_TABLE_NAME_COL_STEPS,\\\n                                        DB_EXPERIMENT_TABLE_NAME_COL_STATES, \\\n                                        DB_EXPERIMENT_TABLE_NAME_COL_ACTIONS,\\\n                                        DB_EXPERIMENT_TABLE_NAME_COL_REWARDS,\\\n                                        DB_EXPERIMENT_TABLE_NAME_COL_DONE\n\n\"\"\"\nTODO\nsave expreiment stats (name, total reward, total steps, time_start, time_end) in experiments table\n\"\"\"\n\n\ndef __env_name_to_table(s):\n    if '<' in s and '>' in s:\n        s = s.split('<')[-1]\n        s = s.split('>')[0]\n\n    s = s.replace('-', '_')\n    return s\n\n\ndef __verbosity_string_to_num(verbosity):\n    if isinstance(verbosity, str):\n        verbosity = {\n            'progress': 1,\n            'total': 2,\n            'episode': 3,\n            'step': 4\n        }[verbosity]\n    return verbosity\n\n\ndef run_episodes(env, agent, n_episodes, storage_dict=None, render=False, verbosity='progress'):\n    \"\"\"\n    storage_dict(optional): dict to store the transition data in the form of {'episodes': [...], 'steps_list': [...], 'states': [...], 'actions': [...], 'rewards': [...], 'dones': [...]}\n    verbosity: None or 0, 'progress' or 1, 'total' or 2, 'episode' or 3, 'step' or 4\n    \"\"\"\n\n    start_time = time.time()\n\n    verbosity = __verbosity_string_to_num(verbosity)\n\n    agent.set_env(env)\n\n    if not storage_dict:\n        storage_dict = {col: [] for col in DATA_COLUMNS}\n\n    episode_rewards = []\n\n    total_reward, total_steps = 0, 0\n\n    n_actions = None\n\n    if verbosity == 1:\n        _iter = tqdm(range(n_episodes), f\"Agent {agent.name} execution in {str(env)} environment\")\n    else:\n        _iter = range(n_episodes)\n\n    episode = 0\n    for episode in _iter:\n        episode_reward = 0\n        step = 0\n        state = env.reset()\n        done = False\n\n        while not done:\n\n            action = agent.act(state)\n\n            if not n_actions:\n                n_actions = len(np.atleast_1d(action))\n\n            next_state, reward, done, _ = env.step(action)\n\n            agent.observe(state, action, reward, next_state, done)\n\n            episode_reward += reward\n\n            if verbosity >= 4:\n                log (\n                    f\"Episode:{episode}, Step:{step}, state:{state}, action:{action}, reward:{reward}, next state:{next_state}, done:{done}\",\n                    tags='markdown_heading3')\n\n            action = np.atleast_1d(action)\n\n            storage_dict[DB_EXPERIMENT_TABLE_NAME_COL_EPISODES].append(episode)\n            storage_dict[DB_EXPERIMENT_TABLE_NAME_COL_STEPS].append(step)\n            storage_dict[DB_EXPERIMENT_TABLE_NAME_COL_STATES].append(state)\n            storage_dict[DB_EXPERIMENT_TABLE_NAME_COL_ACTIONS].append(action)\n            storage_dict[DB_EXPERIMENT_TABLE_NAME_COL_REWARDS].append(reward)\n            storage_dict[DB_EXPERIMENT_TABLE_NAME_COL_DONE].append(done)\n\n            if done:\n                storage_dict[DB_EXPERIMENT_TABLE_NAME_COL_EPISODES].append(episode)\n                storage_dict[DB_EXPERIMENT_TABLE_NAME_COL_STEPS].append(step+1)\n                storage_dict[DB_EXPERIMENT_TABLE_NAME_COL_STATES].append(next_state)\n                storage_dict[DB_EXPERIMENT_TABLE_NAME_COL_ACTIONS].append(['null'] * n_actions)\n                storage_dict[DB_EXPERIMENT_TABLE_NAME_COL_REWARDS].append(.0)\n                storage_dict[DB_EXPERIMENT_TABLE_NAME_COL_DONE].append(-1)\n                step += 1\n            else:\n                state = next_state\n                step += 1\n\n            if render and hasattr(env, 'render'):\n                env.render()\n\n        episode_rewards.append(episode_reward)\n        total_reward += episode_reward\n        total_steps += step\n        if verbosity >= 3:\n            avg_rewards = np.mean(episode_rewards[int(0.1 * len(episode_rewards)):])\n            log (\n                f\"Agent {agent.name} completed the episode {episode}. Steps {step}, Total reward {episode_reward}, rolling avg reward(10%) {avg_rewards:.02f}\",\n                tags='markdown_heading3')\n\n    elapsed_time = time.time() - start_time\n    if verbosity >= 1:\n        episode += 1\n        avg_rewards = np.mean(episode_rewards[int(0.1 * len(episode_rewards)):])\n        log (\n            f\"Agent {agent.name} completed {episode} episodes in {elapsed_time:.02f} seconds in {str(env)}. Total reward {total_reward} (avg ep. reward(100%) {total_reward / episode}, rolling avg ep. reward(10%) {avg_rewards:.02f}). Steps {total_steps}\",\n            tags='markdown_heading3')\n\n    return storage_dict\n\n# TODO DELETE\n# def run_and_store_episodes(env, agent, n_episodes, experiment_name=None, store_results_func=None, verbosity='progress'):\n#     if experiment_name is None:\n#         experiment_name = f\"{agent.name}_{__env_name_to_table(str(env))}\"\n#\n#     res_dict = {col: [] for col in DATA_COLUMNS}\n#     start_time, duration_secs = None, None\n#     start_time_str = timestamp_str()\n#     try:\n#         start_time = time.time()\n#         res_dict = run_episodes(env,\n#                                 agent,\n#                                 n_episodes,\n#                                 storage_dict=res_dict,\n#                                 render=False,\n#                                 verbosity=verbosity)\n#         duration_secs = time.time()-start_time\n#     except Exception as e:\n#         pass\n#     finally:\n#         if store_results_func is None:\n#             pass\n#         elif store_results_func.lower() == 'dataframe':\n#             store_results_as_dataframe(res_dict,\n#                                        name=experiment_name)\n#         elif store_results_func.lower() == 'database':\n#             store_results_in_database(res_dict,\n#                                       to_table=__env_name_to_table(str(env)),\n#                                       experiment_id=experiment_name,\n#                                       agent_id=agent.name(),\n#                                       env_id=str(env),\n#                                       start_time=start_time_str,\n#                                       duration_secs=duration_secs,\n#                                       comment=None)\n\n",
        "branches": [
            "rl.core.rl.engine.__env_name_to_table",
            "rl.core.rl.engine.__verbosity_string_to_num",
            "rl.core.rl.engine.run_episodes"
        ]
    },
    "rl.core.rl.engine.__env_name_to_table": {
        "type": "function",
        "code": "def __env_name_to_table(s):\n    if '<' in s and '>' in s:\n        s = s.split('<')[-1]\n        s = s.split('>')[0]\n\n    s = s.replace('-', '_')\n    return s",
        "branches": []
    },
    "rl.core.rl.engine.__verbosity_string_to_num": {
        "type": "function",
        "code": "def __verbosity_string_to_num(verbosity):\n    if isinstance(verbosity, str):\n        verbosity = {\n            'progress': 1,\n            'total': 2,\n            'episode': 3,\n            'step': 4\n        }[verbosity]\n    return verbosity",
        "branches": []
    },
    "rl.core.rl.engine.run_episodes": {
        "type": "function",
        "code": "def run_episodes(env, agent, n_episodes, storage_dict=None, render=False, verbosity='progress'):\n    \"\"\"\n    storage_dict(optional): dict to store the transition data in the form of {'episodes': [...], 'steps_list': [...], 'states': [...], 'actions': [...], 'rewards': [...], 'dones': [...]}\n    verbosity: None or 0, 'progress' or 1, 'total' or 2, 'episode' or 3, 'step' or 4\n    \"\"\"\n\n    start_time = time.time()\n\n    verbosity = __verbosity_string_to_num(verbosity)\n\n    agent.set_env(env)\n\n    if not storage_dict:\n        storage_dict = {col: [] for col in DATA_COLUMNS}\n\n    episode_rewards = []\n\n    total_reward, total_steps = 0, 0\n\n    n_actions = None\n\n    if verbosity == 1:\n        _iter = tqdm(range(n_episodes), f\"Agent {agent.name} execution in {str(env)} environment\")\n    else:\n        _iter = range(n_episodes)\n\n    episode = 0\n    for episode in _iter:\n        episode_reward = 0\n        step = 0\n        state = env.reset()\n        done = False\n\n        while not done:\n\n            action = agent.act(state)\n\n            if not n_actions:\n                n_actions = len(np.atleast_1d(action))\n\n            next_state, reward, done, _ = env.step(action)\n\n            agent.observe(state, action, reward, next_state, done)\n\n            episode_reward += reward\n\n            if verbosity >= 4:\n                log (\n                    f\"Episode:{episode}, Step:{step}, state:{state}, action:{action}, reward:{reward}, next state:{next_state}, done:{done}\",\n                    tags='markdown_heading3')\n\n            action = np.atleast_1d(action)\n\n            storage_dict[DB_EXPERIMENT_TABLE_NAME_COL_EPISODES].append(episode)\n            storage_dict[DB_EXPERIMENT_TABLE_NAME_COL_STEPS].append(step)\n            storage_dict[DB_EXPERIMENT_TABLE_NAME_COL_STATES].append(state)\n            storage_dict[DB_EXPERIMENT_TABLE_NAME_COL_ACTIONS].append(action)\n            storage_dict[DB_EXPERIMENT_TABLE_NAME_COL_REWARDS].append(reward)\n            storage_dict[DB_EXPERIMENT_TABLE_NAME_COL_DONE].append(done)\n\n            if done:\n                storage_dict[DB_EXPERIMENT_TABLE_NAME_COL_EPISODES].append(episode)\n                storage_dict[DB_EXPERIMENT_TABLE_NAME_COL_STEPS].append(step+1)\n                storage_dict[DB_EXPERIMENT_TABLE_NAME_COL_STATES].append(next_state)\n                storage_dict[DB_EXPERIMENT_TABLE_NAME_COL_ACTIONS].append(['null'] * n_actions)\n                storage_dict[DB_EXPERIMENT_TABLE_NAME_COL_REWARDS].append(.0)\n                storage_dict[DB_EXPERIMENT_TABLE_NAME_COL_DONE].append(-1)\n                step += 1\n            else:\n                state = next_state\n                step += 1\n\n            if render and hasattr(env, 'render'):\n                env.render()\n\n        episode_rewards.append(episode_reward)\n        total_reward += episode_reward\n        total_steps += step\n        if verbosity >= 3:\n            avg_rewards = np.mean(episode_rewards[int(0.1 * len(episode_rewards)):])\n            log (\n                f\"Agent {agent.name} completed the episode {episode}. Steps {step}, Total reward {episode_reward}, rolling avg reward(10%) {avg_rewards:.02f}\",\n                tags='markdown_heading3')\n\n    elapsed_time = time.time() - start_time\n    if verbosity >= 1:\n        episode += 1\n        avg_rewards = np.mean(episode_rewards[int(0.1 * len(episode_rewards)):])\n        log (\n            f\"Agent {agent.name} completed {episode} episodes in {elapsed_time:.02f} seconds in {str(env)}. Total reward {total_reward} (avg ep. reward(100%) {total_reward / episode}, rolling avg ep. reward(10%) {avg_rewards:.02f}). Steps {total_steps}\",\n            tags='markdown_heading3')\n\n    return storage_dict",
        "branches": []
    },
    "rl.core.rl.env": {
        "type": "module",
        "code": "import numpy as np\nimport gym\n\nfrom functools import lru_cache\nfrom collections.abc import Iterable\n\n\nclass AbstractEnv:\n\n    name = None\n\n    def reset(self):\n        raise NotImplementedError\n\n    def step(self, action):\n        raise NotImplementedError\n\n    def render(self):\n        pass\n\n    def __str__(self):\n        return self.__class__.name\n\n\n# TODO redo envwrapper\nclass EnvWrapper:\n    def __init__(self, env):\n        raise NotImplementedError\n        self.gym_env = env if not isinstance(env, str) else gym.make(env)\n        self.gym_env.reset()\n        assert self.gym_env.state is not None\n        assert self.gym_env.observation_space is not None\n        assert self.gym_env.action_space is not None\n\n        observation_space = self.gym_env.observation_space\n        if not isinstance(observation_space, gym.spaces.box.Box) and not isinstance(observation_space, gym.spaces.discrete.Discrete):\n            raise Exception('Unknown observation space type for this environment: '+str(type(observation_space)))\n\n        self.state_low = observation_space.low if isinstance(observation_space, gym.spaces.box.Box) else [0]\n        self.state_high = observation_space.high if isinstance(observation_space, gym.spaces.box.Box) else [observation_space.n-1]\n\n        action_space = self.gym_env.action_space\n        if not isinstance(action_space, gym.spaces.box.Box) and not isinstance(action_space,\n                                                                                    gym.spaces.discrete.Discrete):\n            raise Exception('Unknown action space type for this environment: ' + str(type(action_space)))\n\n        self.action_low = action_space.low if isinstance(action_space, gym.spaces.box.Box) else [0]\n        self.action_high = action_space.high if isinstance(action_space, gym.spaces.box.Box) else [\n                action_space.n-1]\n\n    @lru_cache()\n    def is_action_space_discrete(self):\n        return isinstance(self.gym_env.action_space, gym.spaces.discrete.Discrete)\n\n    @lru_cache()\n    def state_dims(self):\n        return len(self.state_low)\n\n    @lru_cache()\n    def state_limits(self):\n        res = np.array([self.state_low, self.state_high])\n        return res\n\n    @lru_cache()\n    def action_dims(self):\n        return len(self.action_low)\n\n    @lru_cache()\n    def action_limits(self):\n        res = np.array([self.action_low, self.action_high])\n        return res\n\n    def __step(self, state, action):\n        self.gym_env.state = state\n        if isinstance(action, Iterable):\n            next_state, reward, done, _ = self.gym_env.step(tuple(action))\n        else:\n            next_state, reward, done, _ = self.gym_env.step(action)\n        return next_state, reward, done\n\n    # walks\n    def init_state(self):\n        return self.gym_env.reset()\n\n    def transition(self, state, action):\n        next_state, reward, done = self.__step(tuple(state), action)\n        return next_state\n\n    def reward(self, state, action):\n        next_state, reward, done = self.__step(state, action)\n        return reward\n\n    def is_done(self, state, action):\n        next_state, reward, done = self.__step(tuple(state), action)\n        return done\n\n    def info(self):\n        print_format = \"%(env_id)s: state_dims:%(state_dims)s state_limits:%(state_limits)s action_dims:%(action_dims)s action_limits:%(action_limits)s\"\n        env_id = str(self)\n        state_dims = self.state_dims()\n        state_limits = \"(%s %s)\" % (self.state_low, self.state_high)\n        action_dims = self.action_dims()\n        action_limits = \"(%s %s)\" % (self.action_low, self.action_high)\n        print(print_format % locals())\n\n    def random_state(self):\n        return self.gym_env.observation_space.sample()\n\n    def random_action(self):\n        return self.gym_env.action_space.sample()\n\n\n@lru_cache()\ndef gym_envs_list():\n    envids = sorted([spec.id for spec in gym.envs.registry.all()])\n    return envids\n\n\"\"\"['Acrobot-v1', 'CartPole-v0', 'CartPole-v1', 'MountainCar-v0', 'MountainCarContinuous-v0', 'NChain-v0', 'Pendulum-v0']\"\"\"\n@lru_cache()\ndef wrappable_gym_envs():\n    res = []\n    for env_id in gym_envs_list():\n        try:\n            EnvWrapper(env_id)\n        except Exception as e:\n            pass\n        else:\n            res.append(env_id)\n    return res\n\n\ndef wrap_env(env):\n    try:\n        if isinstance(env, str):\n            return EnvWrapper(gym.make(env))\n        else:\n            return EnvWrapper(env)\n    except Exception as e:\n        return None\n",
        "branches": [
            "rl.core.rl.env.AbstractEnv",
            "rl.core.rl.env.EnvWrapper",
            "rl.core.rl.env.gym_envs_list",
            "rl.core.rl.env.wrappable_gym_envs",
            "rl.core.rl.env.wrap_env"
        ]
    },
    "rl.core.rl.env.AbstractEnv": {
        "type": "class",
        "code": "class AbstractEnv:\n\n    name = None\n\n    def reset(self):\n        raise NotImplementedError\n\n    def step(self, action):\n        raise NotImplementedError\n\n    def render(self):\n        pass\n\n    def __str__(self):\n        return self.__class__.name",
        "branches": [
            "rl.core.rl.env.AbstractEnv.reset",
            "rl.core.rl.env.AbstractEnv.step",
            "rl.core.rl.env.AbstractEnv.render",
            "rl.core.rl.env.AbstractEnv.__str__"
        ]
    },
    "rl.core.rl.env.AbstractEnv.reset": {
        "type": "class_method",
        "code": "def reset(self):\n    raise NotImplementedError",
        "branches": []
    },
    "rl.core.rl.env.AbstractEnv.step": {
        "type": "class_method",
        "code": "def step(self, action):\n    raise NotImplementedError",
        "branches": []
    },
    "rl.core.rl.env.AbstractEnv.render": {
        "type": "class_method",
        "code": "def render(self):\n    pass",
        "branches": []
    },
    "rl.core.rl.env.AbstractEnv.__str__": {
        "type": "class_method",
        "code": "def __str__(self):\n    return self.__class__.name",
        "branches": []
    },
    "rl.core.rl.env.EnvWrapper": {
        "type": "class",
        "code": "class EnvWrapper:\n    def __init__(self, env):\n        raise NotImplementedError\n        self.gym_env = env if not isinstance(env, str) else gym.make(env)\n        self.gym_env.reset()\n        assert self.gym_env.state is not None\n        assert self.gym_env.observation_space is not None\n        assert self.gym_env.action_space is not None\n\n        observation_space = self.gym_env.observation_space\n        if not isinstance(observation_space, gym.spaces.box.Box) and not isinstance(observation_space, gym.spaces.discrete.Discrete):\n            raise Exception('Unknown observation space type for this environment: '+str(type(observation_space)))\n\n        self.state_low = observation_space.low if isinstance(observation_space, gym.spaces.box.Box) else [0]\n        self.state_high = observation_space.high if isinstance(observation_space, gym.spaces.box.Box) else [observation_space.n-1]\n\n        action_space = self.gym_env.action_space\n        if not isinstance(action_space, gym.spaces.box.Box) and not isinstance(action_space,\n                                                                                    gym.spaces.discrete.Discrete):\n            raise Exception('Unknown action space type for this environment: ' + str(type(action_space)))\n\n        self.action_low = action_space.low if isinstance(action_space, gym.spaces.box.Box) else [0]\n        self.action_high = action_space.high if isinstance(action_space, gym.spaces.box.Box) else [\n                action_space.n-1]\n\n    @lru_cache()\n    def is_action_space_discrete(self):\n        return isinstance(self.gym_env.action_space, gym.spaces.discrete.Discrete)\n\n    @lru_cache()\n    def state_dims(self):\n        return len(self.state_low)\n\n    @lru_cache()\n    def state_limits(self):\n        res = np.array([self.state_low, self.state_high])\n        return res\n\n    @lru_cache()\n    def action_dims(self):\n        return len(self.action_low)\n\n    @lru_cache()\n    def action_limits(self):\n        res = np.array([self.action_low, self.action_high])\n        return res\n\n    def __step(self, state, action):\n        self.gym_env.state = state\n        if isinstance(action, Iterable):\n            next_state, reward, done, _ = self.gym_env.step(tuple(action))\n        else:\n            next_state, reward, done, _ = self.gym_env.step(action)\n        return next_state, reward, done\n\n    # walks\n    def init_state(self):\n        return self.gym_env.reset()\n\n    def transition(self, state, action):\n        next_state, reward, done = self.__step(tuple(state), action)\n        return next_state\n\n    def reward(self, state, action):\n        next_state, reward, done = self.__step(state, action)\n        return reward\n\n    def is_done(self, state, action):\n        next_state, reward, done = self.__step(tuple(state), action)\n        return done\n\n    def info(self):\n        print_format = \"%(env_id)s: state_dims:%(state_dims)s state_limits:%(state_limits)s action_dims:%(action_dims)s action_limits:%(action_limits)s\"\n        env_id = str(self)\n        state_dims = self.state_dims()\n        state_limits = \"(%s %s)\" % (self.state_low, self.state_high)\n        action_dims = self.action_dims()\n        action_limits = \"(%s %s)\" % (self.action_low, self.action_high)\n        print(print_format % locals())\n\n    def random_state(self):\n        return self.gym_env.observation_space.sample()\n\n    def random_action(self):\n        return self.gym_env.action_space.sample()",
        "branches": [
            "rl.core.rl.env.EnvWrapper.__init__",
            "rl.core.rl.env.EnvWrapper.is_action_space_discrete",
            "rl.core.rl.env.EnvWrapper.state_dims",
            "rl.core.rl.env.EnvWrapper.state_limits",
            "rl.core.rl.env.EnvWrapper.action_dims",
            "rl.core.rl.env.EnvWrapper.action_limits",
            "rl.core.rl.env.EnvWrapper.__step",
            "rl.core.rl.env.EnvWrapper.init_state",
            "rl.core.rl.env.EnvWrapper.transition",
            "rl.core.rl.env.EnvWrapper.reward",
            "rl.core.rl.env.EnvWrapper.is_done",
            "rl.core.rl.env.EnvWrapper.info",
            "rl.core.rl.env.EnvWrapper.random_state",
            "rl.core.rl.env.EnvWrapper.random_action"
        ]
    },
    "rl.core.rl.env.EnvWrapper.__init__": {
        "type": "class_method",
        "code": "def __init__(self, env):\n    raise NotImplementedError\n    self.gym_env = env if not isinstance(env, str) else gym.make(env)\n    self.gym_env.reset()\n    assert self.gym_env.state is not None\n    assert self.gym_env.observation_space is not None\n    assert self.gym_env.action_space is not None\n\n    observation_space = self.gym_env.observation_space\n    if not isinstance(observation_space, gym.spaces.box.Box) and not isinstance(observation_space, gym.spaces.discrete.Discrete):\n        raise Exception('Unknown observation space type for this environment: '+str(type(observation_space)))\n\n    self.state_low = observation_space.low if isinstance(observation_space, gym.spaces.box.Box) else [0]\n    self.state_high = observation_space.high if isinstance(observation_space, gym.spaces.box.Box) else [observation_space.n-1]\n\n    action_space = self.gym_env.action_space\n    if not isinstance(action_space, gym.spaces.box.Box) and not isinstance(action_space,\n                                                                                gym.spaces.discrete.Discrete):\n        raise Exception('Unknown action space type for this environment: ' + str(type(action_space)))\n\n    self.action_low = action_space.low if isinstance(action_space, gym.spaces.box.Box) else [0]\n    self.action_high = action_space.high if isinstance(action_space, gym.spaces.box.Box) else [\n            action_space.n-1]",
        "branches": []
    },
    "rl.core.rl.env.EnvWrapper.is_action_space_discrete": {
        "type": "class_method",
        "code": "@lru_cache()\ndef is_action_space_discrete(self):\n    return isinstance(self.gym_env.action_space, gym.spaces.discrete.Discrete)",
        "branches": []
    },
    "rl.core.rl.env.EnvWrapper.state_dims": {
        "type": "class_method",
        "code": "@lru_cache()\ndef state_dims(self):\n    return len(self.state_low)",
        "branches": []
    },
    "rl.core.rl.env.EnvWrapper.state_limits": {
        "type": "class_method",
        "code": "@lru_cache()\ndef state_limits(self):\n    res = np.array([self.state_low, self.state_high])\n    return res",
        "branches": []
    },
    "rl.core.rl.env.EnvWrapper.action_dims": {
        "type": "class_method",
        "code": "@lru_cache()\ndef action_dims(self):\n    return len(self.action_low)",
        "branches": []
    },
    "rl.core.rl.env.EnvWrapper.action_limits": {
        "type": "class_method",
        "code": "@lru_cache()\ndef action_limits(self):\n    res = np.array([self.action_low, self.action_high])\n    return res",
        "branches": []
    },
    "rl.core.rl.env.EnvWrapper.__step": {
        "type": "class_method",
        "code": "def __step(self, state, action):\n    self.gym_env.state = state\n    if isinstance(action, Iterable):\n        next_state, reward, done, _ = self.gym_env.step(tuple(action))\n    else:\n        next_state, reward, done, _ = self.gym_env.step(action)\n    return next_state, reward, done",
        "branches": []
    },
    "rl.core.rl.env.EnvWrapper.init_state": {
        "type": "class_method",
        "code": "def init_state(self):\n    return self.gym_env.reset()",
        "branches": []
    },
    "rl.core.rl.env.EnvWrapper.transition": {
        "type": "class_method",
        "code": "def transition(self, state, action):\n    next_state, reward, done = self.__step(tuple(state), action)\n    return next_state",
        "branches": []
    },
    "rl.core.rl.env.EnvWrapper.reward": {
        "type": "class_method",
        "code": "def reward(self, state, action):\n    next_state, reward, done = self.__step(state, action)\n    return reward",
        "branches": []
    },
    "rl.core.rl.env.EnvWrapper.is_done": {
        "type": "class_method",
        "code": "def is_done(self, state, action):\n    next_state, reward, done = self.__step(tuple(state), action)\n    return done",
        "branches": []
    },
    "rl.core.rl.env.EnvWrapper.info": {
        "type": "class_method",
        "code": "def info(self):\n    print_format = \"%(env_id)s: state_dims:%(state_dims)s state_limits:%(state_limits)s action_dims:%(action_dims)s action_limits:%(action_limits)s\"\n    env_id = str(self)\n    state_dims = self.state_dims()\n    state_limits = \"(%s %s)\" % (self.state_low, self.state_high)\n    action_dims = self.action_dims()\n    action_limits = \"(%s %s)\" % (self.action_low, self.action_high)\n    print(print_format % locals())",
        "branches": []
    },
    "rl.core.rl.env.EnvWrapper.random_state": {
        "type": "class_method",
        "code": "def random_state(self):\n    return self.gym_env.observation_space.sample()",
        "branches": []
    },
    "rl.core.rl.env.EnvWrapper.random_action": {
        "type": "class_method",
        "code": "def random_action(self):\n    return self.gym_env.action_space.sample()",
        "branches": []
    },
    "rl.core.rl.env.gym_envs_list": {
        "type": "function",
        "code": "@lru_cache()\ndef gym_envs_list():\n    envids = sorted([spec.id for spec in gym.envs.registry.all()])\n    return envids",
        "branches": []
    },
    "rl.core.rl.env.wrappable_gym_envs": {
        "type": "function",
        "code": "@lru_cache()\ndef wrappable_gym_envs():\n    res = []\n    for env_id in gym_envs_list():\n        try:\n            EnvWrapper(env_id)\n        except Exception as e:\n            pass\n        else:\n            res.append(env_id)\n    return res",
        "branches": []
    },
    "rl.core.rl.env.wrap_env": {
        "type": "function",
        "code": "def wrap_env(env):\n    try:\n        if isinstance(env, str):\n            return EnvWrapper(gym.make(env))\n        else:\n            return EnvWrapper(env)\n    except Exception as e:\n        return None",
        "branches": []
    },
    "rl.core.rl.envs": {
        "type": "module",
        "code": "from rl.experiments.simple_2d.simple_env import SimpleEnv\n\nENVS = {\n    SimpleEnv\n}\n\n\ndef get_env(env_name):\n    for env in ENVS:\n        if env.name == env_name:\n            return env\n",
        "branches": [
            "rl.core.rl.envs.get_env"
        ]
    },
    "rl.core.rl.envs.get_env": {
        "type": "function",
        "code": "def get_env(env_name):\n    for env in ENVS:\n        if env.name == env_name:\n            return env",
        "branches": []
    },
    "rl.core.rl.__init__": {
        "type": "module",
        "code": "",
        "branches": []
    },
    "rl.core.storage": {
        "type": "directory",
        "code": null,
        "branches": [
            "rl.core.storage.db",
            "rl.core.storage.storage",
            "rl.core.storage.__init__"
        ]
    },
    "rl.core.storage.db": {
        "type": "module",
        "code": "\nimport pandas as pd\nimport sqlalchemy.exc\nfrom sqlalchemy import create_engine\n\nfrom rl.core.configs.path_confgis import EXPERIMENT_DATABASE_OBJECT_ABSPATH\n\n\ndef __get_engine():\n    db_path = EXPERIMENT_DATABASE_OBJECT_ABSPATH\n\n    engine = create_engine('sqlite:///'+db_path, echo=False)\n\n    return engine\n\n\ndef execute_query(query):\n    engine = __get_engine()\n\n    with engine.connect() as connection:\n        connection.execute(query)\n\n\ndef execute_query_and_return(query):\n    engine = __get_engine()\n\n    with engine.connect() as connection:\n        result = connection.execute(query)\n        return list(result)\n\n\ndef exp_id_already_exists(experiment_id):\n    try:\n        res = execute_query_and_return(query=f'select experiment_id from experiments where experiment_id=\"{experiment_id}\" limit 1')\n    except sqlalchemy.exc.OperationalError as e:\n        return False# no such table: experiments\n\n    return res is not None and len(res) > 0\n\n\ndef add_experiment_info(experiment_id,\n                        agent_id=None,\n                        env_id=None,\n                        total_reward=None,\n                        episodes=None,\n                        total_steps=None,\n                        start_time=None,\n                        duration_secs=None,\n                        comment=None):\n    engine = __get_engine()\n\n    args = locals()\n    del args['engine']\n    for k, v in args.items():\n        args[k] = [v]\n\n    df = pd.DataFrame.from_dict(args)\n    df.to_sql('experiments', con=engine, if_exists='append', index=False)\n\n\ndef upload_df_in_db(df, to_table):\n    engine = __get_engine()\n\n    df.to_sql(to_table, con=engine, if_exists='append', index=False)\n\n\ndef download_df_from_db(experiment_id, from_table):\n    engine = __get_engine()\n\n    df = pd.read_sql(f'select * from {from_table} where experiment_id=\\\"{experiment_id}\\\"',\n                     con=engine,\n                     coerce_float=True,\n                     index_col='episode').reset_index()\n\n    return df\n",
        "branches": [
            "rl.core.storage.db.__get_engine",
            "rl.core.storage.db.execute_query",
            "rl.core.storage.db.execute_query_and_return",
            "rl.core.storage.db.exp_id_already_exists",
            "rl.core.storage.db.add_experiment_info",
            "rl.core.storage.db.upload_df_in_db",
            "rl.core.storage.db.download_df_from_db"
        ]
    },
    "rl.core.storage.db.__get_engine": {
        "type": "function",
        "code": "def __get_engine():\n    db_path = EXPERIMENT_DATABASE_OBJECT_ABSPATH\n\n    engine = create_engine('sqlite:///'+db_path, echo=False)\n\n    return engine",
        "branches": []
    },
    "rl.core.storage.db.execute_query": {
        "type": "function",
        "code": "def execute_query(query):\n    engine = __get_engine()\n\n    with engine.connect() as connection:\n        connection.execute(query)",
        "branches": []
    },
    "rl.core.storage.db.execute_query_and_return": {
        "type": "function",
        "code": "def execute_query_and_return(query):\n    engine = __get_engine()\n\n    with engine.connect() as connection:\n        result = connection.execute(query)\n        return list(result)",
        "branches": []
    },
    "rl.core.storage.db.exp_id_already_exists": {
        "type": "function",
        "code": "def exp_id_already_exists(experiment_id):\n    try:\n        res = execute_query_and_return(query=f'select experiment_id from experiments where experiment_id=\"{experiment_id}\" limit 1')\n    except sqlalchemy.exc.OperationalError as e:\n        return False# no such table: experiments\n\n    return res is not None and len(res) > 0",
        "branches": []
    },
    "rl.core.storage.db.add_experiment_info": {
        "type": "function",
        "code": "def add_experiment_info(experiment_id,\n                        agent_id=None,\n                        env_id=None,\n                        total_reward=None,\n                        episodes=None,\n                        total_steps=None,\n                        start_time=None,\n                        duration_secs=None,\n                        comment=None):\n    engine = __get_engine()\n\n    args = locals()\n    del args['engine']\n    for k, v in args.items():\n        args[k] = [v]\n\n    df = pd.DataFrame.from_dict(args)\n    df.to_sql('experiments', con=engine, if_exists='append', index=False)",
        "branches": []
    },
    "rl.core.storage.db.upload_df_in_db": {
        "type": "function",
        "code": "def upload_df_in_db(df, to_table):\n    engine = __get_engine()\n\n    df.to_sql(to_table, con=engine, if_exists='append', index=False)",
        "branches": []
    },
    "rl.core.storage.db.download_df_from_db": {
        "type": "function",
        "code": "def download_df_from_db(experiment_id, from_table):\n    engine = __get_engine()\n\n    df = pd.read_sql(f'select * from {from_table} where experiment_id=\\\"{experiment_id}\\\"',\n                     con=engine,\n                     coerce_float=True,\n                     index_col='episode').reset_index()\n\n    return df",
        "branches": []
    },
    "rl.core.storage.storage": {
        "type": "module",
        "code": "from os.path import join, exists\n\nimport pandas as pd\n\nfrom rl.core.configs.storage_configs import DB_EXPERIMENT_TABLE_NAME_COL_EXPID, \\\n    DB_EXPERIMENT_TABLE_NAME_COL_EPISODES, \\\n    DB_EXPERIMENT_TABLE_NAME_COL_STEPS, \\\n    DB_EXPERIMENT_TABLE_NAME_COL_STATES, \\\n    DB_EXPERIMENT_TABLE_NAME_COL_ACTIONS, \\\n    DB_EXPERIMENT_TABLE_NAME_COL_REWARDS, \\\n    DB_EXPERIMENT_TABLE_NAME_COL_DONE, DB_EXPERIMENT_TABLE_NAME_COL_NEXTSTATES\n\nfrom rl.core.configs.path_confgis import EXPERIMENT_DATAFRAMES_DIRECTORY_ABSPATH\n\nfrom rl.core.utilities.timestamp import timestamp_str, timestamp_long_str\nfrom rl.core.storage.db import upload_df_in_db,\\\n                                    exp_id_already_exists,\\\n                                    add_experiment_info\n\n\ndef data_to_df(results_dict):\n    episodes = results_dict[DB_EXPERIMENT_TABLE_NAME_COL_EPISODES]\n    steps_list = results_dict[DB_EXPERIMENT_TABLE_NAME_COL_STEPS]\n    states = results_dict[DB_EXPERIMENT_TABLE_NAME_COL_STATES]\n    actions = results_dict[DB_EXPERIMENT_TABLE_NAME_COL_ACTIONS]\n    rewards = results_dict[DB_EXPERIMENT_TABLE_NAME_COL_REWARDS]\n    dones = results_dict[DB_EXPERIMENT_TABLE_NAME_COL_DONE]\n\n    to_dict = {\n        DB_EXPERIMENT_TABLE_NAME_COL_EPISODES: episodes,\n        DB_EXPERIMENT_TABLE_NAME_COL_STEPS: steps_list\n    }\n\n    states = list(map(list, zip(*states)))\n    for i, state_i in enumerate(states):\n        to_dict[f\"{DB_EXPERIMENT_TABLE_NAME_COL_STATES}_{i}\"] = state_i\n\n    actions = list(map(list, zip(*actions)))\n    for i, action_i in enumerate(actions):\n        to_dict[f\"{DB_EXPERIMENT_TABLE_NAME_COL_ACTIONS}_{i}\"] = [a if a != 'null' else None for a in action_i]\n\n    to_dict[DB_EXPERIMENT_TABLE_NAME_COL_REWARDS] = rewards\n\n    to_dict[DB_EXPERIMENT_TABLE_NAME_COL_DONE] = list(map(int, dones))\n\n    df = pd.DataFrame(to_dict)\n\n    return df\n\n\ndef compress_data_df(df):#TODO\n    dones = df[DB_EXPERIMENT_TABLE_NAME_COL_DONE]\n    del df[DB_EXPERIMENT_TABLE_NAME_COL_DONE]\n    state_cols = [col for col in df.columns if DB_EXPERIMENT_TABLE_NAME_COL_STATES in col]\n    for i, c in enumerate(state_cols):\n        df[f\"{DB_EXPERIMENT_TABLE_NAME_COL_NEXTSTATES}_{i}\"] = df[f\"{DB_EXPERIMENT_TABLE_NAME_COL_STATES}_{i}\"].shift(periods=-1)\n\n    df[DB_EXPERIMENT_TABLE_NAME_COL_DONE] = dones\n\n    df = df[df[DB_EXPERIMENT_TABLE_NAME_COL_DONE] >= 0]\n\n    return df\n\n\n# def store_results_in_database(results_dict,\n#                               to_table,\n#                               experiment_id=None,\n#                               agent_id=None,\n#                               env_id=None,\n#                               start_time=None,\n#                               duration_secs=None,\n#                               comment=None):\n#     df = data_to_df(results_dict)\n#     # df_compressed = compress_data_df(df)#TODO\n#\n#     experiment_id = _generate_valid_experiment_id(experiment_id)\n#\n#     df[DB_EXPERIMENT_TABLE_NAME_COL_EXPID] = experiment_id\n#\n#     upload_df_in_db(df, to_table)\n#\n#     add_experiment_info(\n#         experiment_id,\n#         agent_id=agent_id,\n#         env_id=env_id,\n#         total_reward=df[DB_EXPERIMENT_TABLE_NAME_COL_REWARDS].sum(),\n#         episodes=df[DB_EXPERIMENT_TABLE_NAME_COL_EPISODES].max()-len(df),\n#         total_steps=len(df),\n#         start_time=start_time,\n#         duration_secs=duration_secs,\n#         comment=comment\n#     )\n#\n#     #TODO add_experiment_info\n\n",
        "branches": [
            "rl.core.storage.storage.data_to_df",
            "rl.core.storage.storage.compress_data_df"
        ]
    },
    "rl.core.storage.storage.data_to_df": {
        "type": "function",
        "code": "def data_to_df(results_dict):\n    episodes = results_dict[DB_EXPERIMENT_TABLE_NAME_COL_EPISODES]\n    steps_list = results_dict[DB_EXPERIMENT_TABLE_NAME_COL_STEPS]\n    states = results_dict[DB_EXPERIMENT_TABLE_NAME_COL_STATES]\n    actions = results_dict[DB_EXPERIMENT_TABLE_NAME_COL_ACTIONS]\n    rewards = results_dict[DB_EXPERIMENT_TABLE_NAME_COL_REWARDS]\n    dones = results_dict[DB_EXPERIMENT_TABLE_NAME_COL_DONE]\n\n    to_dict = {\n        DB_EXPERIMENT_TABLE_NAME_COL_EPISODES: episodes,\n        DB_EXPERIMENT_TABLE_NAME_COL_STEPS: steps_list\n    }\n\n    states = list(map(list, zip(*states)))\n    for i, state_i in enumerate(states):\n        to_dict[f\"{DB_EXPERIMENT_TABLE_NAME_COL_STATES}_{i}\"] = state_i\n\n    actions = list(map(list, zip(*actions)))\n    for i, action_i in enumerate(actions):\n        to_dict[f\"{DB_EXPERIMENT_TABLE_NAME_COL_ACTIONS}_{i}\"] = [a if a != 'null' else None for a in action_i]\n\n    to_dict[DB_EXPERIMENT_TABLE_NAME_COL_REWARDS] = rewards\n\n    to_dict[DB_EXPERIMENT_TABLE_NAME_COL_DONE] = list(map(int, dones))\n\n    df = pd.DataFrame(to_dict)\n\n    return df",
        "branches": []
    },
    "rl.core.storage.storage.compress_data_df": {
        "type": "function",
        "code": "def compress_data_df(df):#TODO\n    dones = df[DB_EXPERIMENT_TABLE_NAME_COL_DONE]\n    del df[DB_EXPERIMENT_TABLE_NAME_COL_DONE]\n    state_cols = [col for col in df.columns if DB_EXPERIMENT_TABLE_NAME_COL_STATES in col]\n    for i, c in enumerate(state_cols):\n        df[f\"{DB_EXPERIMENT_TABLE_NAME_COL_NEXTSTATES}_{i}\"] = df[f\"{DB_EXPERIMENT_TABLE_NAME_COL_STATES}_{i}\"].shift(periods=-1)\n\n    df[DB_EXPERIMENT_TABLE_NAME_COL_DONE] = dones\n\n    df = df[df[DB_EXPERIMENT_TABLE_NAME_COL_DONE] >= 0]\n\n    return df",
        "branches": []
    },
    "rl.core.storage.__init__": {
        "type": "module",
        "code": "",
        "branches": []
    },
    "rl.core.utilities": {
        "type": "directory",
        "code": null,
        "branches": [
            "rl.core.utilities.file_utils",
            "rl.core.utilities.logging",
            "rl.core.utilities.profiling",
            "rl.core.utilities.timestamp",
            "rl.core.utilities.__init__"
        ]
    },
    "rl.core.utilities.file_utils": {
        "type": "module",
        "code": "from os import makedirs, listdir\nfrom os.path import exists, split, splitext, join\nimport uuid\nimport json\n\nimport pandas as pd\nfrom markdown import markdown\n\nfrom rl.core.configs.storage_configs import UNIQUE_STRING_LENGHT\nfrom rl.core.configs.path_confgis import EXPERIMENT_LOGS_DIRECTORY_ABSPATH\nfrom rl.core.configs.log_configs import LOG_CSV_DIR_PATH, LOG_HTML_DIR_PATH\n\n\n# def create_path(path):\n#     if \".\" in path:\n#         path = split(path)[0]\n#\n#     if not exists(path):\n#         makedirs(path)\n\n\ndef unique_string(length=None):\n    if length is None:\n        length = UNIQUE_STRING_LENGHT\n    return uuid.uuid4().hex[:length].upper()\n\n\ndef date_unique_string(length=None):\n    unique_s = unique_string(length)\n\n\ndef markdown_to_html(abspath):\n    with open(abspath, \"r\", encoding=\"utf-8\") as input_file:\n        text = input_file.read()\n\n    html = markdown(text)\n\n    path, filename = split(abspath)\n\n    filename, ext = splitext(filename)\n\n    html_abspath = join(path, filename+'.html')\n\n    with open(html_abspath, \"w\", encoding=\"utf-8\", errors=\"xmlcharrefreplace\") as output_file:\n        output_file.write(html)\n\n    return html_abspath\n\n\ndef generate_markdown_from_logs(tags=None):\n    logs_dir_path = join(EXPERIMENT_LOGS_DIRECTORY_ABSPATH, LOG_CSV_DIR_PATH)\n    files = listdir(logs_dir_path)\n    files_abspath = [join(logs_dir_path, file) for file in files]\n\n    dfs = [pd.read_csv(file, index_col=False) for file in files_abspath]\n\n    df = pd.concat(dfs).sort_values(by='timestamp')\n\n    df['tags'] = df['tags'].fillna('general')\n\n    df_md_conv = __convert_logs_for_markdown(df)\n\n    file_str = '   \\n'.join(df_md_conv['message'].to_list())\n\n    markdown_abspath = join(EXPERIMENT_LOGS_DIRECTORY_ABSPATH, LOG_HTML_DIR_PATH)+\"/report.md\"\n\n    with open(markdown_abspath, \"w\", encoding=\"utf-8\") as output_file:\n        output_file.write(file_str)\n\n    return markdown_abspath\n\n\ndef __convert_logs_for_markdown(df):\n    df = df.copy()\n    __apply_str_func_to_tag(df, 'markdown_image', lambda s: f\"![]({s})\")\n    __apply_str_func_to_tag(df, 'markdown_heading', lambda s: f\"### {s}\")\n\n    return df\n\n\ndef __apply_str_func_to_tag(df, tag, func):\n    indx = df['tags'].str.contains(tag)\n    n = indx.sum()\n    if n > 0:\n        t = df.loc[indx, 'message'].apply(func)\n        df.loc[indx, 'message'] = t\n    return n\n\n\ndef read_json_file(file):\n    with open(file, 'r') as f:\n        return json.load(f)\n\n\ndef store_df(df, abspath):\n    df.to_csv(abspath, index=False)\n\n\ndef read_df(abspath):\n    df = pd.read_csv(abspath, index_col=None)\n    # TODO maybe consume the file after reading it\n    return df\n\n",
        "branches": [
            "rl.core.utilities.file_utils.unique_string",
            "rl.core.utilities.file_utils.date_unique_string",
            "rl.core.utilities.file_utils.markdown_to_html",
            "rl.core.utilities.file_utils.generate_markdown_from_logs",
            "rl.core.utilities.file_utils.__convert_logs_for_markdown",
            "rl.core.utilities.file_utils.__apply_str_func_to_tag",
            "rl.core.utilities.file_utils.read_json_file",
            "rl.core.utilities.file_utils.store_df",
            "rl.core.utilities.file_utils.read_df"
        ]
    },
    "rl.core.utilities.file_utils.unique_string": {
        "type": "function",
        "code": "def unique_string(length=None):\n    if length is None:\n        length = UNIQUE_STRING_LENGHT\n    return uuid.uuid4().hex[:length].upper()",
        "branches": []
    },
    "rl.core.utilities.file_utils.date_unique_string": {
        "type": "function",
        "code": "def date_unique_string(length=None):\n    unique_s = unique_string(length)",
        "branches": []
    },
    "rl.core.utilities.file_utils.markdown_to_html": {
        "type": "function",
        "code": "def markdown_to_html(abspath):\n    with open(abspath, \"r\", encoding=\"utf-8\") as input_file:\n        text = input_file.read()\n\n    html = markdown(text)\n\n    path, filename = split(abspath)\n\n    filename, ext = splitext(filename)\n\n    html_abspath = join(path, filename+'.html')\n\n    with open(html_abspath, \"w\", encoding=\"utf-8\", errors=\"xmlcharrefreplace\") as output_file:\n        output_file.write(html)\n\n    return html_abspath",
        "branches": []
    },
    "rl.core.utilities.file_utils.generate_markdown_from_logs": {
        "type": "function",
        "code": "def generate_markdown_from_logs(tags=None):\n    logs_dir_path = join(EXPERIMENT_LOGS_DIRECTORY_ABSPATH, LOG_CSV_DIR_PATH)\n    files = listdir(logs_dir_path)\n    files_abspath = [join(logs_dir_path, file) for file in files]\n\n    dfs = [pd.read_csv(file, index_col=False) for file in files_abspath]\n\n    df = pd.concat(dfs).sort_values(by='timestamp')\n\n    df['tags'] = df['tags'].fillna('general')\n\n    df_md_conv = __convert_logs_for_markdown(df)\n\n    file_str = '   \\n'.join(df_md_conv['message'].to_list())\n\n    markdown_abspath = join(EXPERIMENT_LOGS_DIRECTORY_ABSPATH, LOG_HTML_DIR_PATH)+\"/report.md\"\n\n    with open(markdown_abspath, \"w\", encoding=\"utf-8\") as output_file:\n        output_file.write(file_str)\n\n    return markdown_abspath",
        "branches": []
    },
    "rl.core.utilities.file_utils.__convert_logs_for_markdown": {
        "type": "function",
        "code": "def __convert_logs_for_markdown(df):\n    df = df.copy()\n    __apply_str_func_to_tag(df, 'markdown_image', lambda s: f\"![]({s})\")\n    __apply_str_func_to_tag(df, 'markdown_heading', lambda s: f\"### {s}\")\n\n    return df",
        "branches": []
    },
    "rl.core.utilities.file_utils.__apply_str_func_to_tag": {
        "type": "function",
        "code": "def __apply_str_func_to_tag(df, tag, func):\n    indx = df['tags'].str.contains(tag)\n    n = indx.sum()\n    if n > 0:\n        t = df.loc[indx, 'message'].apply(func)\n        df.loc[indx, 'message'] = t\n    return n",
        "branches": []
    },
    "rl.core.utilities.file_utils.read_json_file": {
        "type": "function",
        "code": "def read_json_file(file):\n    with open(file, 'r') as f:\n        return json.load(f)",
        "branches": []
    },
    "rl.core.utilities.file_utils.store_df": {
        "type": "function",
        "code": "def store_df(df, abspath):\n    df.to_csv(abspath, index=False)",
        "branches": []
    },
    "rl.core.utilities.file_utils.read_df": {
        "type": "function",
        "code": "def read_df(abspath):\n    df = pd.read_csv(abspath, index_col=None)\n    # TODO maybe consume the file after reading it\n    return df",
        "branches": []
    },
    "rl.core.utilities.logging": {
        "type": "module",
        "code": "from functools import wraps\nimport time\nfrom os.path import join\n\nimport numpy as np\nimport pandas as pd\nimport cv2\n\nfrom rl.core.configs.log_configs import *\nfrom rl.core.configs.path_confgis import *\nfrom rl.core.utilities.file_utils import generate_markdown_from_logs, markdown_to_html\nfrom rl.core.utilities.timestamp import timestamp_long_str, timestamp_unique_str\n\n\ndef _time():\n    if GENERAL_LOG_TIMINGS_FLAG:\n        return time.time_ns()\n    else:\n        return .0\n\n\nclass Logger:\n\n    instances = []\n\n    def __init__(self, name, directory=None):\n        Logger.instances.append(self)\n\n        self.name = name\n        self.directory = directory if directory else ''\n        self.path = join(EXPERIMENT_LOGS_DIRECTORY_ABSPATH, self.directory)\n\n        create_path(self.path)\n\n        self.log_path = join(self.path, LOG_CSV_DIR_PATH)\n        create_path(self.log_path)\n\n        self.html_path = join(self.path, LOG_HTML_DIR_PATH)\n        create_path(self.html_path)\n\n        self.perf_mon_path = join(self.path, PERFORMANCE_MONITORING_DIR_PATH)\n        create_path(self.perf_mon_path)\n\n        self.imgs_path = join(self.path, LOG_IMAGES_DIR_PATH)\n        create_path(self.imgs_path)\n\n        self.__log_dict = {\n            \"timestamp\": [],\n            'message': [],\n            'tags': []\n        }\n        self.__img_dict = {\n            \"timestamp\": [],\n            'path': [],\n            'image': []\n        }\n\n    def log(self, msg, tags='general'):\n        if not GENERAL_LOG_FLAG:\n            return\n\n        tags = ['general'] if (tags is None) else tags if isinstance(tags, list) else [tags]\n\n        if GENERAL_LOG_STDOUT_FLAG:\n            print(msg, 'tags:', tags)\n\n        self.__log_dict['timestamp'].append(timestamp_long_str())\n        self.__log_dict['message'].append(msg)\n        self.__log_dict['tags'].append('|'.join(tags))\n\n    def log_image(self, img, title=None, store_directly_on_disk=False, tags=None):\n        assert isinstance(img, np.ndarray), f\"Image must be in numpy array format. Given {type(img)}\"\n\n        if not tags:\n            tags = ['log_plt']\n        else:\n            tags.append('log_plt')\n\n        if not title:\n            title = timestamp_unique_str()\n        else:\n            title = f\"{title}_{timestamp_unique_str()}\"\n\n        title += '.png'\n\n        path = join(self.imgs_path, title)\n\n        self.__img_dict['timestamp'].append(timestamp_long_str())\n        self.__img_dict['path'].append(path)\n        self.log(path, tags='markdown_image')\n\n        if store_directly_on_disk:\n            self.__save_image(img, path)\n            self.__img_dict['image'].append(None)\n            # self.log(f\"Image {title} saved as {path}\", tags=tags)\n        else:\n            self.__img_dict['image'].append(img)\n            self.log(f\"Image {title} saved temporarily in RAM\", tags=tags)\n\n    def save(self):\n        df = pd.DataFrame(self.__log_dict)\n        df.to_csv(join(self.log_path, f\"{self.name}_{CSV_FILENAME_EXTENSION_LOGS_CSV}.csv\"), index=False)\n\n        for i in range(len(self.__img_dict['image'])):\n            path = self.__img_dict['path'][i]\n            img = self.__img_dict['image'][i]\n            self.__save_image(img, path)\n\n        fpath = generate_markdown_from_logs()\n        markdown_to_html(fpath)\n\n    # https://realpython.com/primer-on-python-decorators/#decorators-with-arguments\n    def log_func_call(self, tags=None):\n        TAG = \"func_call\"\n        if not tags:\n            tags = [TAG]\n        elif isinstance(tags, list):\n            tags.append(TAG)\n        elif isinstance(tags, str):\n            tags = [tags, TAG]\n        else:\n            tags = [TAG]\n\n        def log_tags(func):\n            @wraps(func)\n            def wrapper(*args, **kwargs):\n                args_repr = [repr(a) for a in args]  # 1\n                kwargs_repr = [f\"{k}={v!r}\" for k, v in kwargs.items()]  # 2\n                signature = \", \".join(args_repr + kwargs_repr)  # 3\n                start_time = _time()\n\n                result = func(*args, **kwargs)\n                end_time = _time()\n                run_time = (end_time - start_time)/1000000#TODO config 's'|'ms'|'us'\n                result_str = repr(result).replace('\\n', '')\n                self.log(f\"Called: {func.__name__}( {signature} ) -> |{result_str!r}| in {run_time:.3f} ms\", tags=tags)\n                self.log(f\"Function:{func.__name__} Time:{run_time}\", tags=\"run_time\")  #\n\n                return result\n\n            return wrapper\n\n        return log_tags\n\n    def __save_image(self, image, path):\n        if image is None:\n            return\n\n        if image.max() <= 1.:\n            image = (255 * image).astype(int)\n        cv2.imwrite(path, image)\n        self.log(f\"Image saved as {path}\")\n\n\ndef save_loggers():\n    for _logger in Logger.instances:\n        _logger.save()\n\n\nlogger = Logger('general')\nlog = logger.log\n",
        "branches": [
            "rl.core.utilities.logging._time",
            "rl.core.utilities.logging.Logger",
            "rl.core.utilities.logging.save_loggers"
        ]
    },
    "rl.core.utilities.logging._time": {
        "type": "function",
        "code": "def _time():\n    if GENERAL_LOG_TIMINGS_FLAG:\n        return time.time_ns()\n    else:\n        return .0",
        "branches": []
    },
    "rl.core.utilities.logging.Logger": {
        "type": "class",
        "code": "class Logger:\n\n    instances = []\n\n    def __init__(self, name, directory=None):\n        Logger.instances.append(self)\n\n        self.name = name\n        self.directory = directory if directory else ''\n        self.path = join(EXPERIMENT_LOGS_DIRECTORY_ABSPATH, self.directory)\n\n        create_path(self.path)\n\n        self.log_path = join(self.path, LOG_CSV_DIR_PATH)\n        create_path(self.log_path)\n\n        self.html_path = join(self.path, LOG_HTML_DIR_PATH)\n        create_path(self.html_path)\n\n        self.perf_mon_path = join(self.path, PERFORMANCE_MONITORING_DIR_PATH)\n        create_path(self.perf_mon_path)\n\n        self.imgs_path = join(self.path, LOG_IMAGES_DIR_PATH)\n        create_path(self.imgs_path)\n\n        self.__log_dict = {\n            \"timestamp\": [],\n            'message': [],\n            'tags': []\n        }\n        self.__img_dict = {\n            \"timestamp\": [],\n            'path': [],\n            'image': []\n        }\n\n    def log(self, msg, tags='general'):\n        if not GENERAL_LOG_FLAG:\n            return\n\n        tags = ['general'] if (tags is None) else tags if isinstance(tags, list) else [tags]\n\n        if GENERAL_LOG_STDOUT_FLAG:\n            print(msg, 'tags:', tags)\n\n        self.__log_dict['timestamp'].append(timestamp_long_str())\n        self.__log_dict['message'].append(msg)\n        self.__log_dict['tags'].append('|'.join(tags))\n\n    def log_image(self, img, title=None, store_directly_on_disk=False, tags=None):\n        assert isinstance(img, np.ndarray), f\"Image must be in numpy array format. Given {type(img)}\"\n\n        if not tags:\n            tags = ['log_plt']\n        else:\n            tags.append('log_plt')\n\n        if not title:\n            title = timestamp_unique_str()\n        else:\n            title = f\"{title}_{timestamp_unique_str()}\"\n\n        title += '.png'\n\n        path = join(self.imgs_path, title)\n\n        self.__img_dict['timestamp'].append(timestamp_long_str())\n        self.__img_dict['path'].append(path)\n        self.log(path, tags='markdown_image')\n\n        if store_directly_on_disk:\n            self.__save_image(img, path)\n            self.__img_dict['image'].append(None)\n            # self.log(f\"Image {title} saved as {path}\", tags=tags)\n        else:\n            self.__img_dict['image'].append(img)\n            self.log(f\"Image {title} saved temporarily in RAM\", tags=tags)\n\n    def save(self):\n        df = pd.DataFrame(self.__log_dict)\n        df.to_csv(join(self.log_path, f\"{self.name}_{CSV_FILENAME_EXTENSION_LOGS_CSV}.csv\"), index=False)\n\n        for i in range(len(self.__img_dict['image'])):\n            path = self.__img_dict['path'][i]\n            img = self.__img_dict['image'][i]\n            self.__save_image(img, path)\n\n        fpath = generate_markdown_from_logs()\n        markdown_to_html(fpath)\n\n    # https://realpython.com/primer-on-python-decorators/#decorators-with-arguments\n    def log_func_call(self, tags=None):\n        TAG = \"func_call\"\n        if not tags:\n            tags = [TAG]\n        elif isinstance(tags, list):\n            tags.append(TAG)\n        elif isinstance(tags, str):\n            tags = [tags, TAG]\n        else:\n            tags = [TAG]\n\n        def log_tags(func):\n            @wraps(func)\n            def wrapper(*args, **kwargs):\n                args_repr = [repr(a) for a in args]  # 1\n                kwargs_repr = [f\"{k}={v!r}\" for k, v in kwargs.items()]  # 2\n                signature = \", \".join(args_repr + kwargs_repr)  # 3\n                start_time = _time()\n\n                result = func(*args, **kwargs)\n                end_time = _time()\n                run_time = (end_time - start_time)/1000000#TODO config 's'|'ms'|'us'\n                result_str = repr(result).replace('\\n', '')\n                self.log(f\"Called: {func.__name__}( {signature} ) -> |{result_str!r}| in {run_time:.3f} ms\", tags=tags)\n                self.log(f\"Function:{func.__name__} Time:{run_time}\", tags=\"run_time\")  #\n\n                return result\n\n            return wrapper\n\n        return log_tags\n\n    def __save_image(self, image, path):\n        if image is None:\n            return\n\n        if image.max() <= 1.:\n            image = (255 * image).astype(int)\n        cv2.imwrite(path, image)\n        self.log(f\"Image saved as {path}\")",
        "branches": [
            "rl.core.utilities.logging.Logger.__init__",
            "rl.core.utilities.logging.Logger.log",
            "rl.core.utilities.logging.Logger.log_image",
            "rl.core.utilities.logging.Logger.save",
            "rl.core.utilities.logging.Logger.log_func_call",
            "rl.core.utilities.logging.Logger.__save_image"
        ]
    },
    "rl.core.utilities.logging.Logger.__init__": {
        "type": "class_method",
        "code": "def __init__(self, name, directory=None):\n    Logger.instances.append(self)\n\n    self.name = name\n    self.directory = directory if directory else ''\n    self.path = join(EXPERIMENT_LOGS_DIRECTORY_ABSPATH, self.directory)\n\n    create_path(self.path)\n\n    self.log_path = join(self.path, LOG_CSV_DIR_PATH)\n    create_path(self.log_path)\n\n    self.html_path = join(self.path, LOG_HTML_DIR_PATH)\n    create_path(self.html_path)\n\n    self.perf_mon_path = join(self.path, PERFORMANCE_MONITORING_DIR_PATH)\n    create_path(self.perf_mon_path)\n\n    self.imgs_path = join(self.path, LOG_IMAGES_DIR_PATH)\n    create_path(self.imgs_path)\n\n    self.__log_dict = {\n        \"timestamp\": [],\n        'message': [],\n        'tags': []\n    }\n    self.__img_dict = {\n        \"timestamp\": [],\n        'path': [],\n        'image': []\n    }",
        "branches": []
    },
    "rl.core.utilities.logging.Logger.log": {
        "type": "class_method",
        "code": "def log(self, msg, tags='general'):\n    if not GENERAL_LOG_FLAG:\n        return\n\n    tags = ['general'] if (tags is None) else tags if isinstance(tags, list) else [tags]\n\n    if GENERAL_LOG_STDOUT_FLAG:\n        print(msg, 'tags:', tags)\n\n    self.__log_dict['timestamp'].append(timestamp_long_str())\n    self.__log_dict['message'].append(msg)\n    self.__log_dict['tags'].append('|'.join(tags))",
        "branches": []
    },
    "rl.core.utilities.logging.Logger.log_image": {
        "type": "class_method",
        "code": "def log_image(self, img, title=None, store_directly_on_disk=False, tags=None):\n    assert isinstance(img, np.ndarray), f\"Image must be in numpy array format. Given {type(img)}\"\n\n    if not tags:\n        tags = ['log_plt']\n    else:\n        tags.append('log_plt')\n\n    if not title:\n        title = timestamp_unique_str()\n    else:\n        title = f\"{title}_{timestamp_unique_str()}\"\n\n    title += '.png'\n\n    path = join(self.imgs_path, title)\n\n    self.__img_dict['timestamp'].append(timestamp_long_str())\n    self.__img_dict['path'].append(path)\n    self.log(path, tags='markdown_image')\n\n    if store_directly_on_disk:\n        self.__save_image(img, path)\n        self.__img_dict['image'].append(None)\n        # self.log(f\"Image {title} saved as {path}\", tags=tags)\n    else:\n        self.__img_dict['image'].append(img)\n        self.log(f\"Image {title} saved temporarily in RAM\", tags=tags)",
        "branches": []
    },
    "rl.core.utilities.logging.Logger.save": {
        "type": "class_method",
        "code": "def save(self):\n    df = pd.DataFrame(self.__log_dict)\n    df.to_csv(join(self.log_path, f\"{self.name}_{CSV_FILENAME_EXTENSION_LOGS_CSV}.csv\"), index=False)\n\n    for i in range(len(self.__img_dict['image'])):\n        path = self.__img_dict['path'][i]\n        img = self.__img_dict['image'][i]\n        self.__save_image(img, path)\n\n    fpath = generate_markdown_from_logs()\n    markdown_to_html(fpath)",
        "branches": []
    },
    "rl.core.utilities.logging.Logger.log_func_call": {
        "type": "class_method",
        "code": "def log_func_call(self, tags=None):\n    TAG = \"func_call\"\n    if not tags:\n        tags = [TAG]\n    elif isinstance(tags, list):\n        tags.append(TAG)\n    elif isinstance(tags, str):\n        tags = [tags, TAG]\n    else:\n        tags = [TAG]\n\n    def log_tags(func):\n        @wraps(func)\n        def wrapper(*args, **kwargs):\n            args_repr = [repr(a) for a in args]  # 1\n            kwargs_repr = [f\"{k}={v!r}\" for k, v in kwargs.items()]  # 2\n            signature = \", \".join(args_repr + kwargs_repr)  # 3\n            start_time = _time()\n\n            result = func(*args, **kwargs)\n            end_time = _time()\n            run_time = (end_time - start_time)/1000000#TODO config 's'|'ms'|'us'\n            result_str = repr(result).replace('\\n', '')\n            self.log(f\"Called: {func.__name__}( {signature} ) -> |{result_str!r}| in {run_time:.3f} ms\", tags=tags)\n            self.log(f\"Function:{func.__name__} Time:{run_time}\", tags=\"run_time\")  #\n\n            return result\n\n        return wrapper\n\n    return log_tags",
        "branches": []
    },
    "rl.core.utilities.logging.Logger.__save_image": {
        "type": "class_method",
        "code": "def __save_image(self, image, path):\n    if image is None:\n        return\n\n    if image.max() <= 1.:\n        image = (255 * image).astype(int)\n    cv2.imwrite(path, image)\n    self.log(f\"Image saved as {path}\")",
        "branches": []
    },
    "rl.core.utilities.logging.save_loggers": {
        "type": "function",
        "code": "def save_loggers():\n    for _logger in Logger.instances:\n        _logger.save()",
        "branches": []
    },
    "rl.core.utilities.profiling": {
        "type": "module",
        "code": "from functools import wraps\nimport cProfile, pstats, io\nfrom pstats import SortKey\n\nfrom rl.core.utilities.logging import log\n\nfrom rl.core.configs.path_confgis import EXPERIMENT_PERFMONITORING_DIRECTORY_ABSPATH\nfrom rl.core.configs.general_configs import CPROFILE_COMMAND_EXECUTION_FLAG\n\n\ndef cprofile(func):\n    @wraps(func)\n    def wrapper(*args, **kwargs):\n        if CPROFILE_COMMAND_EXECUTION_FLAG:\n            pr = cProfile.Profile()\n            pr.enable()\n\n        res = func(*args, **kwargs)\n\n        if CPROFILE_COMMAND_EXECUTION_FLAG:\n            pr.disable()\n            s = io.StringIO()\n            sortby = SortKey.CUMULATIVE\n            # sortby = SortKey.TIME\n            ps = pstats.Stats(pr, stream=s).sort_stats(sortby)\n\n            #snakeviz ./cprofile.prof\n            file = EXPERIMENT_PERFMONITORING_DIRECTORY_ABSPATH+\"cprofile.prof\"\n            ps.dump_stats(file)\n            # TODO change name of result file for each func\n            log(f\"CProfiled {func.__name__}, results in {file}\")\n            # with open(file, 'a') as f:\n            #     ps.print_stats()\n            #     f.write(s.getvalue())\n            #     log(f\"CProfiled {func.__name__}, results in {file}\")\n\n        return res\n\n    return wrapper\n",
        "branches": [
            "rl.core.utilities.profiling.cprofile"
        ]
    },
    "rl.core.utilities.profiling.cprofile": {
        "type": "function",
        "code": "def cprofile(func):\n    @wraps(func)\n    def wrapper(*args, **kwargs):\n        if CPROFILE_COMMAND_EXECUTION_FLAG:\n            pr = cProfile.Profile()\n            pr.enable()\n\n        res = func(*args, **kwargs)\n\n        if CPROFILE_COMMAND_EXECUTION_FLAG:\n            pr.disable()\n            s = io.StringIO()\n            sortby = SortKey.CUMULATIVE\n            # sortby = SortKey.TIME\n            ps = pstats.Stats(pr, stream=s).sort_stats(sortby)\n\n            #snakeviz ./cprofile.prof\n            file = EXPERIMENT_PERFMONITORING_DIRECTORY_ABSPATH+\"cprofile.prof\"\n            ps.dump_stats(file)\n            # TODO change name of result file for each func\n            log(f\"CProfiled {func.__name__}, results in {file}\")\n            # with open(file, 'a') as f:\n            #     ps.print_stats()\n            #     f.write(s.getvalue())\n            #     log(f\"CProfiled {func.__name__}, results in {file}\")\n\n        return res\n\n    return wrapper",
        "branches": []
    },
    "rl.core.utilities.timestamp": {
        "type": "module",
        "code": "from datetime import datetime\n\nfrom rl.core.configs.general_configs import TIMESTAMP_STRING_FORMAT,\\\n                                                TIMESTAMP_LONG_STRING_FORMAT\nfrom rl.core.utilities.file_utils import unique_string\n\n\ndef timestamp_str():\n    return datetime.now().strftime(TIMESTAMP_STRING_FORMAT)\n\n\ndef timestamp_long_str():\n    return datetime.now().strftime(TIMESTAMP_LONG_STRING_FORMAT)\n\n\ndef timestamp_unique_str():\n    return f\"{datetime.now().strftime(TIMESTAMP_STRING_FORMAT)}_{unique_string()}\"\n",
        "branches": [
            "rl.core.utilities.timestamp.timestamp_str",
            "rl.core.utilities.timestamp.timestamp_long_str",
            "rl.core.utilities.timestamp.timestamp_unique_str"
        ]
    },
    "rl.core.utilities.timestamp.timestamp_str": {
        "type": "function",
        "code": "def timestamp_str():\n    return datetime.now().strftime(TIMESTAMP_STRING_FORMAT)",
        "branches": []
    },
    "rl.core.utilities.timestamp.timestamp_long_str": {
        "type": "function",
        "code": "def timestamp_long_str():\n    return datetime.now().strftime(TIMESTAMP_LONG_STRING_FORMAT)",
        "branches": []
    },
    "rl.core.utilities.timestamp.timestamp_unique_str": {
        "type": "function",
        "code": "def timestamp_unique_str():\n    return f\"{datetime.now().strftime(TIMESTAMP_STRING_FORMAT)}_{unique_string()}\"",
        "branches": []
    },
    "rl.core.utilities.__init__": {
        "type": "module",
        "code": "",
        "branches": []
    },
    "rl.core.__init__": {
        "type": "module",
        "code": "",
        "branches": []
    },
    "rl.experiments": {
        "type": "directory",
        "code": null,
        "branches": [
            "rl.experiments.qlearning_pendulum_v0",
            "rl.experiments.simple_2d",
            "rl.experiments.test",
            "rl.experiments.__init__"
        ]
    },
    "rl.experiments.qlearning_pendulum_v0": {
        "type": "directory",
        "code": null,
        "branches": [
            "rl.experiments.qlearning_pendulum_v0.clipboard",
            "rl.experiments.qlearning_pendulum_v0.experiment",
            "rl.experiments.qlearning_pendulum_v0.misc",
            "rl.experiments.qlearning_pendulum_v0.overall_results",
            "rl.experiments.qlearning_pendulum_v0.tabularQLearning",
            "rl.experiments.qlearning_pendulum_v0.__init__"
        ]
    },
    "rl.experiments.qlearning_pendulum_v0.clipboard": {
        "type": "module",
        "code": "import multiprocessing as mp\nimport time\n\nclass AbstractCommand:\n\n    alias = None\n\n    # @logger.log_func_call() #TODO\n    def __init__(self, input_dir, output_dir):\n        # TODO instantiate on-demand file loaders\n        pass\n\n    # @logger.log_func_call()\n    def input(self):\n        print(\"Abs __input\")\n        #TODO read input files\n        pass\n\n    # @cprofile\n    # @logger.log_func_call()\n    def run(self):\n        print('ABs __run')\n        pass\n\n    # @logger.log_func_call()\n    def output(self):\n        #TODO write output to files\n        pass\n\nclass RunEpisodeCommand(AbstractCommand):\n\n    alias = 're'\n\n    def run(self):\n        print('RUN re')\n\nif __name__ == '__main__':\n    cmd = AbstractCommand(None, None)\n    print(dir(cmd))\n    re = RunEpisodeCommand(None, None)\n    print(dir(re))\n    re.input()\n    re.run()\n\n",
        "branches": [
            "rl.experiments.qlearning_pendulum_v0.clipboard.AbstractCommand",
            "rl.experiments.qlearning_pendulum_v0.clipboard.RunEpisodeCommand"
        ]
    },
    "rl.experiments.qlearning_pendulum_v0.clipboard.AbstractCommand": {
        "type": "class",
        "code": "class AbstractCommand:\n\n    alias = None\n\n    # @logger.log_func_call() #TODO\n    def __init__(self, input_dir, output_dir):\n        # TODO instantiate on-demand file loaders\n        pass\n\n    # @logger.log_func_call()\n    def input(self):\n        print(\"Abs __input\")\n        #TODO read input files\n        pass\n\n    # @cprofile\n    # @logger.log_func_call()\n    def run(self):\n        print('ABs __run')\n        pass\n\n    # @logger.log_func_call()\n    def output(self):\n        #TODO write output to files\n        pass",
        "branches": [
            "rl.experiments.qlearning_pendulum_v0.clipboard.AbstractCommand.__init__",
            "rl.experiments.qlearning_pendulum_v0.clipboard.AbstractCommand.input",
            "rl.experiments.qlearning_pendulum_v0.clipboard.AbstractCommand.run",
            "rl.experiments.qlearning_pendulum_v0.clipboard.AbstractCommand.output"
        ]
    },
    "rl.experiments.qlearning_pendulum_v0.clipboard.AbstractCommand.__init__": {
        "type": "class_method",
        "code": "def __init__(self, input_dir, output_dir):\n    # TODO instantiate on-demand file loaders\n    pass",
        "branches": []
    },
    "rl.experiments.qlearning_pendulum_v0.clipboard.AbstractCommand.input": {
        "type": "class_method",
        "code": "def input(self):\n    print(\"Abs __input\")\n    #TODO read input files\n    pass",
        "branches": []
    },
    "rl.experiments.qlearning_pendulum_v0.clipboard.AbstractCommand.run": {
        "type": "class_method",
        "code": "def run(self):\n    print('ABs __run')\n    pass",
        "branches": []
    },
    "rl.experiments.qlearning_pendulum_v0.clipboard.AbstractCommand.output": {
        "type": "class_method",
        "code": "def output(self):\n    #TODO write output to files\n    pass",
        "branches": []
    },
    "rl.experiments.qlearning_pendulum_v0.clipboard.RunEpisodeCommand": {
        "type": "class",
        "code": "class RunEpisodeCommand(AbstractCommand):\n\n    alias = 're'\n\n    def run(self):\n        print('RUN re')",
        "branches": [
            "rl.experiments.qlearning_pendulum_v0.clipboard.RunEpisodeCommand.run"
        ]
    },
    "rl.experiments.qlearning_pendulum_v0.clipboard.RunEpisodeCommand.run": {
        "type": "class_method",
        "code": "def run(self):\n    print('RUN re')",
        "branches": []
    },
    "rl.experiments.qlearning_pendulum_v0.experiment": {
        "type": "module",
        "code": "import datetime\n\nimport gym\n\nfrom experiments.qlearning_pendulum_v0 import TabularQLearningAgent\n\n\ndef experiment_args():\n    resolution = 25\n\n    time_signature = str(datetime.datetime.now()).replace('-', '').replace(':', '').replace(' ', '')\n\n    experiment_name = f'qlearning_tab{resolution}_pend_{time_signature}'\n\n    env = gym.make('Pendulum-v0')\n\n    agent = TabularQLearningAgent(resolution, epsilon=.0)\n\n    res = {\n        'env': env,\n        'agent': agent,\n        'n_episodes': 2000,\n        'experiment_name': experiment_name,\n\n    }\n\n    return res\n\n    # from overall_results import plot_db_experiment\n    # plot_db_experiment(experiment_name, 'Pendulum_v0', save_graph=False)\n",
        "branches": [
            "rl.experiments.qlearning_pendulum_v0.experiment.experiment_args"
        ]
    },
    "rl.experiments.qlearning_pendulum_v0.experiment.experiment_args": {
        "type": "function",
        "code": "def experiment_args():\n    resolution = 25\n\n    time_signature = str(datetime.datetime.now()).replace('-', '').replace(':', '').replace(' ', '')\n\n    experiment_name = f'qlearning_tab{resolution}_pend_{time_signature}'\n\n    env = gym.make('Pendulum-v0')\n\n    agent = TabularQLearningAgent(resolution, epsilon=.0)\n\n    res = {\n        'env': env,\n        'agent': agent,\n        'n_episodes': 2000,\n        'experiment_name': experiment_name,\n\n    }\n\n    return res",
        "branches": []
    },
    "rl.experiments.qlearning_pendulum_v0.misc": {
        "type": "module",
        "code": "\ndef clip(n, low, high):\n    if low > high:\n        raise AttributeError(\"Low argument has to be less or equal than high argument.\")\n    return low if n < low else high if n > high else n\n\n\nclass Map1D:\n    def __init__(self, in_low, in_high, out_a, out_b, clip=False):\n        if in_low > in_high:\n            raise AttributeError(\"in_low argument has to be less than in_high argument.\")\n\n        if in_low == in_high:\n            raise AttributeError(\"in_low argument and in_high argument cannot be equal.\")\n\n        if out_a == out_b:\n            raise AttributeError(\"out_a argument and out_b argument cannot be equal.\")\n\n        self.in_low, self.in_high = in_low, in_high\n        self.in_range = self.in_high-self.in_low\n\n        self.out_a, self.out_b, self.out_low = out_a, out_b, min(out_a, out_b)\n        self.out_range = self.out_b-self.out_a\n\n        self.f = lambda x: self.__map_clipped(x) if clip else self.__map(x)\n\n    def __map(self, x):\n        x_0 = x - self.in_low\n        x_n = x_0 / self.in_range\n        x_mapped = x_n * self.out_range + self.out_a\n        return x_mapped\n\n    def __map_clipped(self, x):\n        return self.__map(clip(x, self.in_low, self.in_high))\n\n    def map(self, x):\n        return self.f(x)\n\n    def __call__(self, x):\n        return self.map(x)\n\n\nclass MapFloatToInteger:\n    def __init__(self, low, high, n, clip_flag=True):\n        if low > high:\n            raise AttributeError(\"low argument has to be less than high argument.\")\n\n        if low == high:\n            raise AttributeError(\"low argument and high argument cannot be equal.\")\n\n        if n < 2 or not isinstance(n, int):\n            raise AttributeError(\"n argument must be an integer >= 2.\")\n\n        self.__map_in = Map1D(low, high, 0, n-1, clip=clip_flag)\n        self.__map_out = Map1D(0, n-1, low, high, clip=clip_flag)\n\n    def map(self, x):\n        return round(self.__map_in(x))\n\n    def reverse(self, n):\n        return self.__map_out(n)\n\n",
        "branches": [
            "rl.experiments.qlearning_pendulum_v0.misc.clip",
            "rl.experiments.qlearning_pendulum_v0.misc.Map1D",
            "rl.experiments.qlearning_pendulum_v0.misc.MapFloatToInteger"
        ]
    },
    "rl.experiments.qlearning_pendulum_v0.misc.clip": {
        "type": "function",
        "code": "def clip(n, low, high):\n    if low > high:\n        raise AttributeError(\"Low argument has to be less or equal than high argument.\")\n    return low if n < low else high if n > high else n",
        "branches": []
    },
    "rl.experiments.qlearning_pendulum_v0.misc.Map1D": {
        "type": "class",
        "code": "class Map1D:\n    def __init__(self, in_low, in_high, out_a, out_b, clip=False):\n        if in_low > in_high:\n            raise AttributeError(\"in_low argument has to be less than in_high argument.\")\n\n        if in_low == in_high:\n            raise AttributeError(\"in_low argument and in_high argument cannot be equal.\")\n\n        if out_a == out_b:\n            raise AttributeError(\"out_a argument and out_b argument cannot be equal.\")\n\n        self.in_low, self.in_high = in_low, in_high\n        self.in_range = self.in_high-self.in_low\n\n        self.out_a, self.out_b, self.out_low = out_a, out_b, min(out_a, out_b)\n        self.out_range = self.out_b-self.out_a\n\n        self.f = lambda x: self.__map_clipped(x) if clip else self.__map(x)\n\n    def __map(self, x):\n        x_0 = x - self.in_low\n        x_n = x_0 / self.in_range\n        x_mapped = x_n * self.out_range + self.out_a\n        return x_mapped\n\n    def __map_clipped(self, x):\n        return self.__map(clip(x, self.in_low, self.in_high))\n\n    def map(self, x):\n        return self.f(x)\n\n    def __call__(self, x):\n        return self.map(x)",
        "branches": [
            "rl.experiments.qlearning_pendulum_v0.misc.Map1D.__init__",
            "rl.experiments.qlearning_pendulum_v0.misc.Map1D.__map",
            "rl.experiments.qlearning_pendulum_v0.misc.Map1D.__map_clipped",
            "rl.experiments.qlearning_pendulum_v0.misc.Map1D.map",
            "rl.experiments.qlearning_pendulum_v0.misc.Map1D.__call__"
        ]
    },
    "rl.experiments.qlearning_pendulum_v0.misc.Map1D.__init__": {
        "type": "class_method",
        "code": "def __init__(self, in_low, in_high, out_a, out_b, clip=False):\n    if in_low > in_high:\n        raise AttributeError(\"in_low argument has to be less than in_high argument.\")\n\n    if in_low == in_high:\n        raise AttributeError(\"in_low argument and in_high argument cannot be equal.\")\n\n    if out_a == out_b:\n        raise AttributeError(\"out_a argument and out_b argument cannot be equal.\")\n\n    self.in_low, self.in_high = in_low, in_high\n    self.in_range = self.in_high-self.in_low\n\n    self.out_a, self.out_b, self.out_low = out_a, out_b, min(out_a, out_b)\n    self.out_range = self.out_b-self.out_a\n\n    self.f = lambda x: self.__map_clipped(x) if clip else self.__map(x)",
        "branches": []
    },
    "rl.experiments.qlearning_pendulum_v0.misc.Map1D.__map": {
        "type": "class_method",
        "code": "def __map(self, x):\n    x_0 = x - self.in_low\n    x_n = x_0 / self.in_range\n    x_mapped = x_n * self.out_range + self.out_a\n    return x_mapped",
        "branches": []
    },
    "rl.experiments.qlearning_pendulum_v0.misc.Map1D.__map_clipped": {
        "type": "class_method",
        "code": "def __map_clipped(self, x):\n    return self.__map(clip(x, self.in_low, self.in_high))",
        "branches": []
    },
    "rl.experiments.qlearning_pendulum_v0.misc.Map1D.map": {
        "type": "class_method",
        "code": "def map(self, x):\n    return self.f(x)",
        "branches": []
    },
    "rl.experiments.qlearning_pendulum_v0.misc.Map1D.__call__": {
        "type": "class_method",
        "code": "def __call__(self, x):\n    return self.map(x)",
        "branches": []
    },
    "rl.experiments.qlearning_pendulum_v0.misc.MapFloatToInteger": {
        "type": "class",
        "code": "class MapFloatToInteger:\n    def __init__(self, low, high, n, clip_flag=True):\n        if low > high:\n            raise AttributeError(\"low argument has to be less than high argument.\")\n\n        if low == high:\n            raise AttributeError(\"low argument and high argument cannot be equal.\")\n\n        if n < 2 or not isinstance(n, int):\n            raise AttributeError(\"n argument must be an integer >= 2.\")\n\n        self.__map_in = Map1D(low, high, 0, n-1, clip=clip_flag)\n        self.__map_out = Map1D(0, n-1, low, high, clip=clip_flag)\n\n    def map(self, x):\n        return round(self.__map_in(x))\n\n    def reverse(self, n):\n        return self.__map_out(n)",
        "branches": [
            "rl.experiments.qlearning_pendulum_v0.misc.MapFloatToInteger.__init__",
            "rl.experiments.qlearning_pendulum_v0.misc.MapFloatToInteger.map",
            "rl.experiments.qlearning_pendulum_v0.misc.MapFloatToInteger.reverse"
        ]
    },
    "rl.experiments.qlearning_pendulum_v0.misc.MapFloatToInteger.__init__": {
        "type": "class_method",
        "code": "def __init__(self, low, high, n, clip_flag=True):\n    if low > high:\n        raise AttributeError(\"low argument has to be less than high argument.\")\n\n    if low == high:\n        raise AttributeError(\"low argument and high argument cannot be equal.\")\n\n    if n < 2 or not isinstance(n, int):\n        raise AttributeError(\"n argument must be an integer >= 2.\")\n\n    self.__map_in = Map1D(low, high, 0, n-1, clip=clip_flag)\n    self.__map_out = Map1D(0, n-1, low, high, clip=clip_flag)",
        "branches": []
    },
    "rl.experiments.qlearning_pendulum_v0.misc.MapFloatToInteger.map": {
        "type": "class_method",
        "code": "def map(self, x):\n    return round(self.__map_in(x))",
        "branches": []
    },
    "rl.experiments.qlearning_pendulum_v0.misc.MapFloatToInteger.reverse": {
        "type": "class_method",
        "code": "def reverse(self, n):\n    return self.__map_out(n)",
        "branches": []
    },
    "rl.experiments.qlearning_pendulum_v0.overall_results": {
        "type": "module",
        "code": "import math\nimport time\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nplt.style.use('bmh')\nplt.rcParams.update({'xtick.labelsize': 6})\nplt.rcParams.update({'ytick.labelsize': 6})\nplt.rcParams.update({'axes.titlesize': 9})\nplt.rcParams.update({'axes.spines.top': False})\nplt.rcParams.update({'axes.spines.right': False})\nplt.rcParams.update({'axes.xmargin': 0.01})\nplt.rcParams.update({'legend.fontsize': 7})\nplt.rcParams.update({'figure.titlesize': 10})\n\nROLLING_WINDOW_SIZE = 100\n\n\ndef con_sin_to_theta(cos_th, sin_th):\n    acos_th, asin_th = math.acos(cos_th), math.asin(sin_th)\n    if asin_th > 0:\n        return acos_th\n    else:\n        return -acos_th\n\n\ndef labeling_thetas(x):\n    return 'BL' if x < -np.pi / 2 else 'UL' if x < 0 else 'UR' if x < np.pi / 2 else 'BR'\n\n\ndef detect_balance(thetas, theta_threshold=.5, thetadot_threshold=0.02, balance_cnt_threshold=10):\n    thetadots = np.diff(thetas)\n    thetadots = np.abs(np.append(thetadots, thetadots[-1]))\n    thetas_abs = thetas.abs()\n\n    cnt = 0\n    for theta, thetadot in zip(thetas_abs[::-1], thetadots[::-1]):\n        if theta > theta_threshold or thetadot > thetadot_threshold:\n            break\n        else:\n            cnt += 1\n\n    if cnt >= balance_cnt_threshold:\n        return len(thetas) - cnt\n    else:\n        return -1\n\n\ndef process_df(df_in, theta_threshold=.5, thetadot_threshold=0.02, balance_cnt_threshold=10):\n    print('Enriching dataframe.')\n    start_time = time.time()\n    df_in['acos'] = np.arccos(df_in['state_0'])\n    df_in['asin_sign'] = 1 - 2 * (np.arcsin(df_in['state_1']) < 0).astype(int)\n\n    df_in['state_th'] = df_in['acos'] * df_in['asin_sign']\n    df_in['theta_label'] = np.digitize(df_in['state_th'], bins=[-np.pi, -np.pi / 2, 0, np.pi / 2, np.pi])\n    df_in['theta_label'] = df_in['theta_label'].map({1: 'BL', 2: 'UL', 3: 'UR', 4: 'BR'})\n\n    del df_in['acos']\n    del df_in['asin_sign']\n\n    df_in['state_th2'] = df_in['state_th'].shift(1)\n    df_in['state_thdot_abs'] = (df_in['state_th2'] - df_in['state_th']).abs()\n    df_in['state_th_abs'] = df_in['state_th'].abs()\n    df_in['is_unbalanced'] = 1 - ((df_in['step'] > 0) & (df_in['state_th_abs'] <= theta_threshold) & (\n                df_in['state_thdot_abs'] <= thetadot_threshold)).astype(int)\n    df_in['unbalanced_step'] = df_in['is_unbalanced'] * df_in['step']\n\n    balancing_step_df = df_in.groupby('episode').agg({'unbalanced_step': 'max'}).reset_index().rename(\n        columns={'unbalanced_step': 'balance_step'})\n    balancing_step_df['balance_step'] = balancing_step_df['balance_step'].replace(199, -1)\n    balancing_step_df.loc[balancing_step_df['balance_step'] > (200 - balance_cnt_threshold), 'balance_step'] = -1\n\n    episode_stats_df = df_in.groupby('episode').agg(\n        {'reward': 'sum', 'state_th': 'first', 'theta_label': 'first'}).reset_index().rename(\n        columns={'state_th': 'theta_0', 'theta_label': 'theta_0_label'})\n    episode_stats_df = episode_stats_df.merge(balancing_step_df, on='episode', how='left')\n    episode_stats_df['solved'] = (episode_stats_df['balance_step'] >= 0).astype(int)\n\n    del df_in['state_th2']\n    del df_in['state_thdot_abs']\n    del df_in['state_th_abs']\n    del df_in['is_unbalanced']\n    del df_in['unbalanced_step']\n\n    print(f'Enrichment finished after {time.time()-start_time:.04f} s')\n    return df_in, episode_stats_df\n\n\ndef episode_rewards_graph(df):\n    eps = df[df['step'] == 0]\n    for l, c in zip(['UL', 'UR', 'BL', 'BR'], ['C1', 'C2', 'C0', 'C3']):\n        episodes_list = eps[eps['theta_label'] == l]['episode'].to_list()\n\n        df_res = df[df['episode'].isin(episodes_list)]\n\n        episode_rewards = df_res.groupby('episode').agg({'reward': 'sum'})\n\n        plt.plot(episode_rewards,\n                 alpha=0.3,\n                 linewidth=2,\n                 c=c)\n\n        rolling_avg = episode_rewards.rolling(ROLLING_WINDOW_SIZE,\n                                              center=True,\n                                              min_periods=ROLLING_WINDOW_SIZE // 5).mean()\n\n        plt.plot(rolling_avg, color='black', linewidth=3)\n        plt.plot(rolling_avg, color=c, label=l, linewidth=2)\n\n    plt.title(\"Score (reward) per episode\")\n    # plt.xlabel(\"Episodes\")\n    # plt.ylabel(\"Reward\")\n    plt.legend()\n\n\ndef solution_ratios_graph(episode_stats_df):\n    plt.axhline(0, c='#cccccc')\n    plt.axvline(0, c='#cccccc')\n\n    solutions = 100 * episode_stats_df['solved'].rolling(window=ROLLING_WINDOW_SIZE, center=True, min_periods=1).mean()\n    plt.plot(solutions, c='black', alpha=0.6, linewidth=4, zorder=3)\n    plt.plot(solutions, c='#dddddd', alpha=0.8, linewidth=2, label='Overall', zorder=4)\n\n    for l, c in zip(['UL', 'UR', 'BL', 'BR'], ['C1', 'C2', 'C0', 'C3']):\n        plt.plot(\n            100 * episode_stats_df[episode_stats_df['theta_0_label'] == l]['solved'].rolling(window=100, center=True,\n                                                                                             min_periods=1).mean(),\n            linewidth=0.7, c=c, label=l, alpha=1, zorder=4)\n\n    ax = plt.gca()\n    xticks = list(np.linspace(0, len(solutions), 11))  # list(ax.get_xticks())#list(range(0, len(solutions) + 1, 500))\n    for y in [50, 75, 90, 95, 99, 100]:\n        episode_numbers = np.where(solutions >= y)[0]\n        if len(episode_numbers) == 0:\n            break\n        x = episode_numbers[0]\n        plt.plot([0, x], [y, y], alpha=0.35, c='C7', zorder=1, linewidth=1)\n        plt.plot([x, x], [0, y], alpha=0.35, c='C7', zorder=1, linewidth=1)\n        # plt.scatter(x, y, marker='o', c='black')\n        plt.scatter(x, y, marker='s', c='C7', zorder=5, alpha=0.35)\n        plt.annotate(f'{y}% ({x})', xy=(x, y), c='#333333', zorder=5)\n        xticks.append(x)\n    # plt.xticks(np.sort(xticks), rotation=-90)\n\n    plt.legend()\n    # plt.xlabel('Episodes')\n    # plt.ylabel('%')\n    plt.title('% of episodes been solved')\n    plt.yticks(range(0, 101, 10));\n\n\ndef balance_steps_graph(episode_stats_df):\n    for l, c in zip(['UL', 'UR', 'BL', 'BR'], ['C1', 'C2', 'C0', 'C3']):\n        temp_df = episode_stats_df[episode_stats_df['theta_0_label'] == l]\n        temp_df = temp_df[temp_df['solved'] > 0]\n        plt.scatter(temp_df['episode'], temp_df['balance_step'], c=c, label=l, marker='s', alpha=0.25)\n        line = temp_df['balance_step'].rolling(window=max(min(ROLLING_WINDOW_SIZE, int(len(temp_df)*0.5)), 1),\n                                               center=True,\n                                               min_periods=1).mean()\n        plt.plot(line, c='black', linewidth=4)\n        plt.plot(line, c=c, linewidth=2)\n\n    plt.xticks(np.linspace(0, len(episode_stats_df), 11))\n    # plt.xticks(rotation=-90)\n    plt.xlabel('Episodes')\n    # plt.ylabel('Count')\n    plt.title('# of steps before solution')\n    plt.legend()\n\n\ndef balancing_progress_graph(df, episode_stats_df):\n    plt.axhline(0, c='#cccccc')\n    plt.axvline(0, c='#cccccc')\n\n    temp_df = df.merge(episode_stats_df[['episode', 'balance_step']], on='episode')\n    temp_df = temp_df[temp_df['balance_step'] > -1]\n    min_ep, max_ep = temp_df['episode'].min(), temp_df['episode'].max()\n    # temp_df['after_balancing'] = temp_df['balance_step']>=temp_df['step']\n    temp_df = temp_df[temp_df['balance_step'] <= temp_df['step']]\n    temp_df = temp_df.groupby('episode').agg({'state_th': 'mean'})\n\n    first_balancing_point = temp_df.index.values.min()\n    plt.axvline(first_balancing_point, alpha=0.5, color='#444444',\n                label=f'first balancing point {first_balancing_point:.00f}')\n\n    mean = temp_df['state_th'].rolling(window=ROLLING_WINDOW_SIZE, center=True, min_periods=1).mean()\n    x = mean.index.values.tolist()\n    y1 = np.zeros(mean.shape)\n\n    plt.fill_between(x=x, y1=y1, y2=mean, color='C5', alpha=0.75, label='mean', zorder=2)\n\n    mean_abs = temp_df['state_th'].abs().rolling(window=ROLLING_WINDOW_SIZE, center=True, min_periods=1).mean()\n    plt.fill_between(x=x, y1=y1, y2=mean_abs, color='C4', alpha=0.5, label='mean(abs)', zorder=1)\n    # plt.plot(mean_abs, color='C4', alpha=0.5, label='mean(abs)', zorder=1)\n\n    overall_mean = mean_abs.mean()\n    plt.axhline(overall_mean, alpha=0.5, color='#444444', label=f'overall mean {overall_mean:.02f}')\n\n    max_abs = temp_df['state_th'].abs().rolling(window=ROLLING_WINDOW_SIZE, center=True, min_periods=1).max()\n    plt.fill_between(x=x, y1=y1, y2=max_abs, color='C6', alpha=0.25, label='max(abs)', zorder=0)\n\n    plt.title('Avg theta (rads) averages after balancing')\n    plt.xlabel('Episodes')\n    # plt.ylabel('Theta (rads)')\n    y_ticks = np.append(np.linspace(-.5, .5, 11), overall_mean)\n    plt.yticks(y_ticks)\n    plt.xticks(np.append(np.linspace(0, df['episode'].max() + 1, 11), first_balancing_point))\n    plt.legend()\n\n\ndef state_uniformity_graph(episode_stats_df):\n    WINDOW_SIZE = len(episode_stats_df) // 10\n    STEP = WINDOW_SIZE // 2\n    res = {\n        'UR': [],\n        'BR': [],\n        'UL': [],\n        'BL': [],\n    }\n    x = list(range(0, len(episode_stats_df) - WINDOW_SIZE + 1, STEP))\n    for step in x:\n        temp_df = episode_stats_df[\n            (episode_stats_df['episode'] >= step) & (episode_stats_df['episode'] < step + WINDOW_SIZE)]\n        hist = temp_df['theta_0_label'].value_counts().to_dict()\n        for k, v in hist.items():\n            res[k].append(100 * v / WINDOW_SIZE)\n\n    sum_line = 100 * np.ones(len(res['UR']))\n    y2 = np.zeros(len(res['UR']))\n    for l, c in zip(['UL', 'UR', 'BL', 'BR'], ['C1', 'C2', 'C0', 'C3']):\n        plt.fill_between(x=x, y1=sum_line, y2=y2, color=c, label=l)\n        plt.plot(x, sum_line, color='#444444', linewidth=1)\n        sum_line -= res[l]\n\n    plt.yticks([0, 25, 50, 75, 100])\n    plt.xticks(range(0, max(x) + 1, STEP))\n    plt.legend()\n    plt.title(f'Uniformity of initial thetas,(window:{WINDOW_SIZE})')\n\n\ndef plot(df, save_graph=None):\n    exp_id = df['experiment_id'][0]\n\n    enriched_df, episode_stats_df = process_df(df)\n\n    fig = plt.figure(figsize=(15, 10))\n    fig.suptitle(f\"Experiment: {exp_id}, Total reward: {df['reward'].sum():.0f}\")\n    fig.patch.set_facecolor('xkcd:light grey')\n\n    plt.subplot(2, 2, 1)\n    episode_rewards_graph(enriched_df)\n\n    plt.subplot(2, 2, 2)\n    solution_ratios_graph(episode_stats_df)\n\n    plt.subplot(2, 2, 3)\n    balance_steps_graph(episode_stats_df)\n\n    plt.subplot(2, 2, 4)\n    balancing_progress_graph(enriched_df, episode_stats_df)\n\n    # plt.subplot(4, 2, 8)\n    # state_uniformity(episode_stats_df)\n\n    plt.tight_layout()\n\n    if save_graph is not None:\n        path = None\n        if isinstance(save_graph, str):\n            path = save_graph\n        elif isinstance(save_graph, bool):\n            path = f\"graphs/overall/{exp_id}.png\"\n            plt.savefig(path)\n\n        try:\n            plt.savefig(path)\n        except Exception as e:\n            print(e)\n            print('Due to the exception above, graph was not saved.')\n\n    plt.show()\n\n\ndef plot_db_experiment(experiment_name, from_table, save_graph=True):\n    from rl.src import download_df_from_db\n    import os\n\n    df = download_df_from_db(experiment_name, from_table)\n\n    df_filename = f'{experiment_name}.csv'\n    df.to_csv(df_filename)\n    plot(df, save_graph=save_graph)\n    try:\n        os.remove(df_filename)\n    except Exception as e:\n        print(f\"Unable to delete dataframe file ({df_filename}) after plotting.\")\n",
        "branches": [
            "rl.experiments.qlearning_pendulum_v0.overall_results.con_sin_to_theta",
            "rl.experiments.qlearning_pendulum_v0.overall_results.labeling_thetas",
            "rl.experiments.qlearning_pendulum_v0.overall_results.detect_balance",
            "rl.experiments.qlearning_pendulum_v0.overall_results.process_df",
            "rl.experiments.qlearning_pendulum_v0.overall_results.episode_rewards_graph",
            "rl.experiments.qlearning_pendulum_v0.overall_results.solution_ratios_graph",
            "rl.experiments.qlearning_pendulum_v0.overall_results.balance_steps_graph",
            "rl.experiments.qlearning_pendulum_v0.overall_results.balancing_progress_graph",
            "rl.experiments.qlearning_pendulum_v0.overall_results.state_uniformity_graph",
            "rl.experiments.qlearning_pendulum_v0.overall_results.plot",
            "rl.experiments.qlearning_pendulum_v0.overall_results.plot_db_experiment"
        ]
    },
    "rl.experiments.qlearning_pendulum_v0.overall_results.con_sin_to_theta": {
        "type": "function",
        "code": "def con_sin_to_theta(cos_th, sin_th):\n    acos_th, asin_th = math.acos(cos_th), math.asin(sin_th)\n    if asin_th > 0:\n        return acos_th\n    else:\n        return -acos_th",
        "branches": []
    },
    "rl.experiments.qlearning_pendulum_v0.overall_results.labeling_thetas": {
        "type": "function",
        "code": "def labeling_thetas(x):\n    return 'BL' if x < -np.pi / 2 else 'UL' if x < 0 else 'UR' if x < np.pi / 2 else 'BR'",
        "branches": []
    },
    "rl.experiments.qlearning_pendulum_v0.overall_results.detect_balance": {
        "type": "function",
        "code": "def detect_balance(thetas, theta_threshold=.5, thetadot_threshold=0.02, balance_cnt_threshold=10):\n    thetadots = np.diff(thetas)\n    thetadots = np.abs(np.append(thetadots, thetadots[-1]))\n    thetas_abs = thetas.abs()\n\n    cnt = 0\n    for theta, thetadot in zip(thetas_abs[::-1], thetadots[::-1]):\n        if theta > theta_threshold or thetadot > thetadot_threshold:\n            break\n        else:\n            cnt += 1\n\n    if cnt >= balance_cnt_threshold:\n        return len(thetas) - cnt\n    else:\n        return -1",
        "branches": []
    },
    "rl.experiments.qlearning_pendulum_v0.overall_results.process_df": {
        "type": "function",
        "code": "def process_df(df_in, theta_threshold=.5, thetadot_threshold=0.02, balance_cnt_threshold=10):\n    print('Enriching dataframe.')\n    start_time = time.time()\n    df_in['acos'] = np.arccos(df_in['state_0'])\n    df_in['asin_sign'] = 1 - 2 * (np.arcsin(df_in['state_1']) < 0).astype(int)\n\n    df_in['state_th'] = df_in['acos'] * df_in['asin_sign']\n    df_in['theta_label'] = np.digitize(df_in['state_th'], bins=[-np.pi, -np.pi / 2, 0, np.pi / 2, np.pi])\n    df_in['theta_label'] = df_in['theta_label'].map({1: 'BL', 2: 'UL', 3: 'UR', 4: 'BR'})\n\n    del df_in['acos']\n    del df_in['asin_sign']\n\n    df_in['state_th2'] = df_in['state_th'].shift(1)\n    df_in['state_thdot_abs'] = (df_in['state_th2'] - df_in['state_th']).abs()\n    df_in['state_th_abs'] = df_in['state_th'].abs()\n    df_in['is_unbalanced'] = 1 - ((df_in['step'] > 0) & (df_in['state_th_abs'] <= theta_threshold) & (\n                df_in['state_thdot_abs'] <= thetadot_threshold)).astype(int)\n    df_in['unbalanced_step'] = df_in['is_unbalanced'] * df_in['step']\n\n    balancing_step_df = df_in.groupby('episode').agg({'unbalanced_step': 'max'}).reset_index().rename(\n        columns={'unbalanced_step': 'balance_step'})\n    balancing_step_df['balance_step'] = balancing_step_df['balance_step'].replace(199, -1)\n    balancing_step_df.loc[balancing_step_df['balance_step'] > (200 - balance_cnt_threshold), 'balance_step'] = -1\n\n    episode_stats_df = df_in.groupby('episode').agg(\n        {'reward': 'sum', 'state_th': 'first', 'theta_label': 'first'}).reset_index().rename(\n        columns={'state_th': 'theta_0', 'theta_label': 'theta_0_label'})\n    episode_stats_df = episode_stats_df.merge(balancing_step_df, on='episode', how='left')\n    episode_stats_df['solved'] = (episode_stats_df['balance_step'] >= 0).astype(int)\n\n    del df_in['state_th2']\n    del df_in['state_thdot_abs']\n    del df_in['state_th_abs']\n    del df_in['is_unbalanced']\n    del df_in['unbalanced_step']\n\n    print(f'Enrichment finished after {time.time()-start_time:.04f} s')\n    return df_in, episode_stats_df",
        "branches": []
    },
    "rl.experiments.qlearning_pendulum_v0.overall_results.episode_rewards_graph": {
        "type": "function",
        "code": "def episode_rewards_graph(df):\n    eps = df[df['step'] == 0]\n    for l, c in zip(['UL', 'UR', 'BL', 'BR'], ['C1', 'C2', 'C0', 'C3']):\n        episodes_list = eps[eps['theta_label'] == l]['episode'].to_list()\n\n        df_res = df[df['episode'].isin(episodes_list)]\n\n        episode_rewards = df_res.groupby('episode').agg({'reward': 'sum'})\n\n        plt.plot(episode_rewards,\n                 alpha=0.3,\n                 linewidth=2,\n                 c=c)\n\n        rolling_avg = episode_rewards.rolling(ROLLING_WINDOW_SIZE,\n                                              center=True,\n                                              min_periods=ROLLING_WINDOW_SIZE // 5).mean()\n\n        plt.plot(rolling_avg, color='black', linewidth=3)\n        plt.plot(rolling_avg, color=c, label=l, linewidth=2)\n\n    plt.title(\"Score (reward) per episode\")\n    # plt.xlabel(\"Episodes\")\n    # plt.ylabel(\"Reward\")\n    plt.legend()",
        "branches": []
    },
    "rl.experiments.qlearning_pendulum_v0.overall_results.solution_ratios_graph": {
        "type": "function",
        "code": "def solution_ratios_graph(episode_stats_df):\n    plt.axhline(0, c='#cccccc')\n    plt.axvline(0, c='#cccccc')\n\n    solutions = 100 * episode_stats_df['solved'].rolling(window=ROLLING_WINDOW_SIZE, center=True, min_periods=1).mean()\n    plt.plot(solutions, c='black', alpha=0.6, linewidth=4, zorder=3)\n    plt.plot(solutions, c='#dddddd', alpha=0.8, linewidth=2, label='Overall', zorder=4)\n\n    for l, c in zip(['UL', 'UR', 'BL', 'BR'], ['C1', 'C2', 'C0', 'C3']):\n        plt.plot(\n            100 * episode_stats_df[episode_stats_df['theta_0_label'] == l]['solved'].rolling(window=100, center=True,\n                                                                                             min_periods=1).mean(),\n            linewidth=0.7, c=c, label=l, alpha=1, zorder=4)\n\n    ax = plt.gca()\n    xticks = list(np.linspace(0, len(solutions), 11))  # list(ax.get_xticks())#list(range(0, len(solutions) + 1, 500))\n    for y in [50, 75, 90, 95, 99, 100]:\n        episode_numbers = np.where(solutions >= y)[0]\n        if len(episode_numbers) == 0:\n            break\n        x = episode_numbers[0]\n        plt.plot([0, x], [y, y], alpha=0.35, c='C7', zorder=1, linewidth=1)\n        plt.plot([x, x], [0, y], alpha=0.35, c='C7', zorder=1, linewidth=1)\n        # plt.scatter(x, y, marker='o', c='black')\n        plt.scatter(x, y, marker='s', c='C7', zorder=5, alpha=0.35)\n        plt.annotate(f'{y}% ({x})', xy=(x, y), c='#333333', zorder=5)\n        xticks.append(x)\n    # plt.xticks(np.sort(xticks), rotation=-90)\n\n    plt.legend()\n    # plt.xlabel('Episodes')\n    # plt.ylabel('%')\n    plt.title('% of episodes been solved')\n    plt.yticks(range(0, 101, 10));",
        "branches": []
    },
    "rl.experiments.qlearning_pendulum_v0.overall_results.balance_steps_graph": {
        "type": "function",
        "code": "def balance_steps_graph(episode_stats_df):\n    for l, c in zip(['UL', 'UR', 'BL', 'BR'], ['C1', 'C2', 'C0', 'C3']):\n        temp_df = episode_stats_df[episode_stats_df['theta_0_label'] == l]\n        temp_df = temp_df[temp_df['solved'] > 0]\n        plt.scatter(temp_df['episode'], temp_df['balance_step'], c=c, label=l, marker='s', alpha=0.25)\n        line = temp_df['balance_step'].rolling(window=max(min(ROLLING_WINDOW_SIZE, int(len(temp_df)*0.5)), 1),\n                                               center=True,\n                                               min_periods=1).mean()\n        plt.plot(line, c='black', linewidth=4)\n        plt.plot(line, c=c, linewidth=2)\n\n    plt.xticks(np.linspace(0, len(episode_stats_df), 11))\n    # plt.xticks(rotation=-90)\n    plt.xlabel('Episodes')\n    # plt.ylabel('Count')\n    plt.title('# of steps before solution')\n    plt.legend()",
        "branches": []
    },
    "rl.experiments.qlearning_pendulum_v0.overall_results.balancing_progress_graph": {
        "type": "function",
        "code": "def balancing_progress_graph(df, episode_stats_df):\n    plt.axhline(0, c='#cccccc')\n    plt.axvline(0, c='#cccccc')\n\n    temp_df = df.merge(episode_stats_df[['episode', 'balance_step']], on='episode')\n    temp_df = temp_df[temp_df['balance_step'] > -1]\n    min_ep, max_ep = temp_df['episode'].min(), temp_df['episode'].max()\n    # temp_df['after_balancing'] = temp_df['balance_step']>=temp_df['step']\n    temp_df = temp_df[temp_df['balance_step'] <= temp_df['step']]\n    temp_df = temp_df.groupby('episode').agg({'state_th': 'mean'})\n\n    first_balancing_point = temp_df.index.values.min()\n    plt.axvline(first_balancing_point, alpha=0.5, color='#444444',\n                label=f'first balancing point {first_balancing_point:.00f}')\n\n    mean = temp_df['state_th'].rolling(window=ROLLING_WINDOW_SIZE, center=True, min_periods=1).mean()\n    x = mean.index.values.tolist()\n    y1 = np.zeros(mean.shape)\n\n    plt.fill_between(x=x, y1=y1, y2=mean, color='C5', alpha=0.75, label='mean', zorder=2)\n\n    mean_abs = temp_df['state_th'].abs().rolling(window=ROLLING_WINDOW_SIZE, center=True, min_periods=1).mean()\n    plt.fill_between(x=x, y1=y1, y2=mean_abs, color='C4', alpha=0.5, label='mean(abs)', zorder=1)\n    # plt.plot(mean_abs, color='C4', alpha=0.5, label='mean(abs)', zorder=1)\n\n    overall_mean = mean_abs.mean()\n    plt.axhline(overall_mean, alpha=0.5, color='#444444', label=f'overall mean {overall_mean:.02f}')\n\n    max_abs = temp_df['state_th'].abs().rolling(window=ROLLING_WINDOW_SIZE, center=True, min_periods=1).max()\n    plt.fill_between(x=x, y1=y1, y2=max_abs, color='C6', alpha=0.25, label='max(abs)', zorder=0)\n\n    plt.title('Avg theta (rads) averages after balancing')\n    plt.xlabel('Episodes')\n    # plt.ylabel('Theta (rads)')\n    y_ticks = np.append(np.linspace(-.5, .5, 11), overall_mean)\n    plt.yticks(y_ticks)\n    plt.xticks(np.append(np.linspace(0, df['episode'].max() + 1, 11), first_balancing_point))\n    plt.legend()",
        "branches": []
    },
    "rl.experiments.qlearning_pendulum_v0.overall_results.state_uniformity_graph": {
        "type": "function",
        "code": "def state_uniformity_graph(episode_stats_df):\n    WINDOW_SIZE = len(episode_stats_df) // 10\n    STEP = WINDOW_SIZE // 2\n    res = {\n        'UR': [],\n        'BR': [],\n        'UL': [],\n        'BL': [],\n    }\n    x = list(range(0, len(episode_stats_df) - WINDOW_SIZE + 1, STEP))\n    for step in x:\n        temp_df = episode_stats_df[\n            (episode_stats_df['episode'] >= step) & (episode_stats_df['episode'] < step + WINDOW_SIZE)]\n        hist = temp_df['theta_0_label'].value_counts().to_dict()\n        for k, v in hist.items():\n            res[k].append(100 * v / WINDOW_SIZE)\n\n    sum_line = 100 * np.ones(len(res['UR']))\n    y2 = np.zeros(len(res['UR']))\n    for l, c in zip(['UL', 'UR', 'BL', 'BR'], ['C1', 'C2', 'C0', 'C3']):\n        plt.fill_between(x=x, y1=sum_line, y2=y2, color=c, label=l)\n        plt.plot(x, sum_line, color='#444444', linewidth=1)\n        sum_line -= res[l]\n\n    plt.yticks([0, 25, 50, 75, 100])\n    plt.xticks(range(0, max(x) + 1, STEP))\n    plt.legend()\n    plt.title(f'Uniformity of initial thetas,(window:{WINDOW_SIZE})')",
        "branches": []
    },
    "rl.experiments.qlearning_pendulum_v0.overall_results.plot": {
        "type": "function",
        "code": "def plot(df, save_graph=None):\n    exp_id = df['experiment_id'][0]\n\n    enriched_df, episode_stats_df = process_df(df)\n\n    fig = plt.figure(figsize=(15, 10))\n    fig.suptitle(f\"Experiment: {exp_id}, Total reward: {df['reward'].sum():.0f}\")\n    fig.patch.set_facecolor('xkcd:light grey')\n\n    plt.subplot(2, 2, 1)\n    episode_rewards_graph(enriched_df)\n\n    plt.subplot(2, 2, 2)\n    solution_ratios_graph(episode_stats_df)\n\n    plt.subplot(2, 2, 3)\n    balance_steps_graph(episode_stats_df)\n\n    plt.subplot(2, 2, 4)\n    balancing_progress_graph(enriched_df, episode_stats_df)\n\n    # plt.subplot(4, 2, 8)\n    # state_uniformity(episode_stats_df)\n\n    plt.tight_layout()\n\n    if save_graph is not None:\n        path = None\n        if isinstance(save_graph, str):\n            path = save_graph\n        elif isinstance(save_graph, bool):\n            path = f\"graphs/overall/{exp_id}.png\"\n            plt.savefig(path)\n\n        try:\n            plt.savefig(path)\n        except Exception as e:\n            print(e)\n            print('Due to the exception above, graph was not saved.')\n\n    plt.show()",
        "branches": []
    },
    "rl.experiments.qlearning_pendulum_v0.overall_results.plot_db_experiment": {
        "type": "function",
        "code": "def plot_db_experiment(experiment_name, from_table, save_graph=True):\n    from rl.src import download_df_from_db\n    import os\n\n    df = download_df_from_db(experiment_name, from_table)\n\n    df_filename = f'{experiment_name}.csv'\n    df.to_csv(df_filename)\n    plot(df, save_graph=save_graph)\n    try:\n        os.remove(df_filename)\n    except Exception as e:\n        print(f\"Unable to delete dataframe file ({df_filename}) after plotting.\")",
        "branches": []
    },
    "rl.experiments.qlearning_pendulum_v0.tabularQLearning": {
        "type": "module",
        "code": "import numpy as np\n\nfrom rl.src import Agent\nfrom rl.src import wrap_env\n\nfrom experiments.qlearning_pendulum_v0 import MapFloatToInteger\n\n\nclass TabularQLearningAgent(Agent):\n    def __init__(self, points_per_dim, a=0.9, gamma=0.9, epsilon=.2):\n        assert points_per_dim >= 2\n        self.points_per_dim = points_per_dim\n        self.q_table = None\n\n        self.learning_rate = a\n        self.discount_rate = gamma\n        self.epsilon = epsilon\n\n        self.wrapped_env = None\n        self.state_low, self.state_high = None, None\n        self.state_diff = None\n        self.state_dims = None\n        self.is_action_space_discrete = None\n        self.action_mapper, self.actions_mapping_f = None, None\n        self.states_mapping_f_list = None\n\n    def set_env(self, env):\n        print(env)\n        self.wrapped_env = wrap_env(env)\n        print(self.wrapped_env.info())\n        self.state_low, self.state_high = self.wrapped_env.state_low, self.wrapped_env.state_high\n        self.state_diff = self.state_high-self.state_low\n\n        self.state_dims = self.wrapped_env.state_dims()\n        print(self.state_low, self.state_high, self.state_diff, self.state_dims)\n\n        self.is_action_space_discrete = self.wrapped_env.is_action_space_discrete()\n\n        assert self.wrapped_env.action_dims() == 1, 'QLearningAgent is not working for environments with action space with more than one dimensions.'\n        self.action_mapper = None\n        if self.is_action_space_discrete:\n            n_action_points = self.wrapped_env.action_high[0]-self.wrapped_env.action_low[0]+1\n            self.actions_mapping_f = lambda x: x\n        else:\n            n_action_points = self.points_per_dim\n            self.action_mapper = MapFloatToInteger(low=self.wrapped_env.action_low[0],\n                                       high=self.wrapped_env.action_high[0],\n                                       n=n_action_points)\n            self.actions_mapping_f = lambda x: self.action_mapper.map(x)\n\n        print(n_action_points)\n        self.q_table = np.ones(shape=[self.points_per_dim]*self.state_dims+[n_action_points])*2\n\n        self.states_mapping_f_list = []\n        for dim_range in self.wrapped_env.state_limits().T:\n            low, high = dim_range\n            self.states_mapping_f_list.append(MapFloatToInteger(low, high, self.points_per_dim))\n\n    def __state_index(self, state):\n        return tuple([self.states_mapping_f_list[i].map(s) for i, s in enumerate(state)])\n\n    def __q_of_state(self, state):\n        index = self.__state_index(state)\n        res = self.q_table[index]\n        return res\n\n    def act(self, state):\n        if np.random.uniform() < self.epsilon:\n            action = self.wrapped_env.random_action()\n            return action\n        else:\n            action_i = np.argmax(self.__q_of_state(state))\n\n        if self.is_action_space_discrete:\n            action = action_i\n\n        else:\n            action = [self.action_mapper.reverse(action_i)]\n\n        return action\n\n    def observe(self, state, action, reward, next_state, done):\n        if not self.is_action_space_discrete:\n            action_i = int(self.action_mapper.map(action[0]))\n        else:\n            action_i = action\n\n        if done:\n            new_Q_s_a = self.discount_rate * reward\n        else:\n            Q_s = self.__q_of_state(state)\n            current_Q_s_a = Q_s[action_i]\n            Q_sn = self.__q_of_state(next_state)\n            max_Q_sn = np.max(Q_sn)\n            td = reward+self.discount_rate*max_Q_sn-current_Q_s_a\n            new_Q_s_a = current_Q_s_a + self.learning_rate * td\n\n        index = self.__state_index(state)\n        self.q_table[index][action_i] = new_Q_s_a\n\n        return\n\n    def name(self):\n        return f'Q_learning_tabular({self.points_per_dim})_v1'\n",
        "branches": [
            "rl.experiments.qlearning_pendulum_v0.tabularQLearning.TabularQLearningAgent"
        ]
    },
    "rl.experiments.qlearning_pendulum_v0.tabularQLearning.TabularQLearningAgent": {
        "type": "class",
        "code": "class TabularQLearningAgent(Agent):\n    def __init__(self, points_per_dim, a=0.9, gamma=0.9, epsilon=.2):\n        assert points_per_dim >= 2\n        self.points_per_dim = points_per_dim\n        self.q_table = None\n\n        self.learning_rate = a\n        self.discount_rate = gamma\n        self.epsilon = epsilon\n\n        self.wrapped_env = None\n        self.state_low, self.state_high = None, None\n        self.state_diff = None\n        self.state_dims = None\n        self.is_action_space_discrete = None\n        self.action_mapper, self.actions_mapping_f = None, None\n        self.states_mapping_f_list = None\n\n    def set_env(self, env):\n        print(env)\n        self.wrapped_env = wrap_env(env)\n        print(self.wrapped_env.info())\n        self.state_low, self.state_high = self.wrapped_env.state_low, self.wrapped_env.state_high\n        self.state_diff = self.state_high-self.state_low\n\n        self.state_dims = self.wrapped_env.state_dims()\n        print(self.state_low, self.state_high, self.state_diff, self.state_dims)\n\n        self.is_action_space_discrete = self.wrapped_env.is_action_space_discrete()\n\n        assert self.wrapped_env.action_dims() == 1, 'QLearningAgent is not working for environments with action space with more than one dimensions.'\n        self.action_mapper = None\n        if self.is_action_space_discrete:\n            n_action_points = self.wrapped_env.action_high[0]-self.wrapped_env.action_low[0]+1\n            self.actions_mapping_f = lambda x: x\n        else:\n            n_action_points = self.points_per_dim\n            self.action_mapper = MapFloatToInteger(low=self.wrapped_env.action_low[0],\n                                       high=self.wrapped_env.action_high[0],\n                                       n=n_action_points)\n            self.actions_mapping_f = lambda x: self.action_mapper.map(x)\n\n        print(n_action_points)\n        self.q_table = np.ones(shape=[self.points_per_dim]*self.state_dims+[n_action_points])*2\n\n        self.states_mapping_f_list = []\n        for dim_range in self.wrapped_env.state_limits().T:\n            low, high = dim_range\n            self.states_mapping_f_list.append(MapFloatToInteger(low, high, self.points_per_dim))\n\n    def __state_index(self, state):\n        return tuple([self.states_mapping_f_list[i].map(s) for i, s in enumerate(state)])\n\n    def __q_of_state(self, state):\n        index = self.__state_index(state)\n        res = self.q_table[index]\n        return res\n\n    def act(self, state):\n        if np.random.uniform() < self.epsilon:\n            action = self.wrapped_env.random_action()\n            return action\n        else:\n            action_i = np.argmax(self.__q_of_state(state))\n\n        if self.is_action_space_discrete:\n            action = action_i\n\n        else:\n            action = [self.action_mapper.reverse(action_i)]\n\n        return action\n\n    def observe(self, state, action, reward, next_state, done):\n        if not self.is_action_space_discrete:\n            action_i = int(self.action_mapper.map(action[0]))\n        else:\n            action_i = action\n\n        if done:\n            new_Q_s_a = self.discount_rate * reward\n        else:\n            Q_s = self.__q_of_state(state)\n            current_Q_s_a = Q_s[action_i]\n            Q_sn = self.__q_of_state(next_state)\n            max_Q_sn = np.max(Q_sn)\n            td = reward+self.discount_rate*max_Q_sn-current_Q_s_a\n            new_Q_s_a = current_Q_s_a + self.learning_rate * td\n\n        index = self.__state_index(state)\n        self.q_table[index][action_i] = new_Q_s_a\n\n        return\n\n    def name(self):\n        return f'Q_learning_tabular({self.points_per_dim})_v1'",
        "branches": [
            "rl.experiments.qlearning_pendulum_v0.tabularQLearning.TabularQLearningAgent.__init__",
            "rl.experiments.qlearning_pendulum_v0.tabularQLearning.TabularQLearningAgent.set_env",
            "rl.experiments.qlearning_pendulum_v0.tabularQLearning.TabularQLearningAgent.__state_index",
            "rl.experiments.qlearning_pendulum_v0.tabularQLearning.TabularQLearningAgent.__q_of_state",
            "rl.experiments.qlearning_pendulum_v0.tabularQLearning.TabularQLearningAgent.act",
            "rl.experiments.qlearning_pendulum_v0.tabularQLearning.TabularQLearningAgent.observe",
            "rl.experiments.qlearning_pendulum_v0.tabularQLearning.TabularQLearningAgent.name"
        ]
    },
    "rl.experiments.qlearning_pendulum_v0.tabularQLearning.TabularQLearningAgent.__init__": {
        "type": "class_method",
        "code": "def __init__(self, points_per_dim, a=0.9, gamma=0.9, epsilon=.2):\n    assert points_per_dim >= 2\n    self.points_per_dim = points_per_dim\n    self.q_table = None\n\n    self.learning_rate = a\n    self.discount_rate = gamma\n    self.epsilon = epsilon\n\n    self.wrapped_env = None\n    self.state_low, self.state_high = None, None\n    self.state_diff = None\n    self.state_dims = None\n    self.is_action_space_discrete = None\n    self.action_mapper, self.actions_mapping_f = None, None\n    self.states_mapping_f_list = None",
        "branches": []
    },
    "rl.experiments.qlearning_pendulum_v0.tabularQLearning.TabularQLearningAgent.set_env": {
        "type": "class_method",
        "code": "def set_env(self, env):\n    print(env)\n    self.wrapped_env = wrap_env(env)\n    print(self.wrapped_env.info())\n    self.state_low, self.state_high = self.wrapped_env.state_low, self.wrapped_env.state_high\n    self.state_diff = self.state_high-self.state_low\n\n    self.state_dims = self.wrapped_env.state_dims()\n    print(self.state_low, self.state_high, self.state_diff, self.state_dims)\n\n    self.is_action_space_discrete = self.wrapped_env.is_action_space_discrete()\n\n    assert self.wrapped_env.action_dims() == 1, 'QLearningAgent is not working for environments with action space with more than one dimensions.'\n    self.action_mapper = None\n    if self.is_action_space_discrete:\n        n_action_points = self.wrapped_env.action_high[0]-self.wrapped_env.action_low[0]+1\n        self.actions_mapping_f = lambda x: x\n    else:\n        n_action_points = self.points_per_dim\n        self.action_mapper = MapFloatToInteger(low=self.wrapped_env.action_low[0],\n                                   high=self.wrapped_env.action_high[0],\n                                   n=n_action_points)\n        self.actions_mapping_f = lambda x: self.action_mapper.map(x)\n\n    print(n_action_points)\n    self.q_table = np.ones(shape=[self.points_per_dim]*self.state_dims+[n_action_points])*2\n\n    self.states_mapping_f_list = []\n    for dim_range in self.wrapped_env.state_limits().T:\n        low, high = dim_range\n        self.states_mapping_f_list.append(MapFloatToInteger(low, high, self.points_per_dim))",
        "branches": []
    },
    "rl.experiments.qlearning_pendulum_v0.tabularQLearning.TabularQLearningAgent.__state_index": {
        "type": "class_method",
        "code": "def __state_index(self, state):\n    return tuple([self.states_mapping_f_list[i].map(s) for i, s in enumerate(state)])",
        "branches": []
    },
    "rl.experiments.qlearning_pendulum_v0.tabularQLearning.TabularQLearningAgent.__q_of_state": {
        "type": "class_method",
        "code": "def __q_of_state(self, state):\n    index = self.__state_index(state)\n    res = self.q_table[index]\n    return res",
        "branches": []
    },
    "rl.experiments.qlearning_pendulum_v0.tabularQLearning.TabularQLearningAgent.act": {
        "type": "class_method",
        "code": "def act(self, state):\n    if np.random.uniform() < self.epsilon:\n        action = self.wrapped_env.random_action()\n        return action\n    else:\n        action_i = np.argmax(self.__q_of_state(state))\n\n    if self.is_action_space_discrete:\n        action = action_i\n\n    else:\n        action = [self.action_mapper.reverse(action_i)]\n\n    return action",
        "branches": []
    },
    "rl.experiments.qlearning_pendulum_v0.tabularQLearning.TabularQLearningAgent.observe": {
        "type": "class_method",
        "code": "def observe(self, state, action, reward, next_state, done):\n    if not self.is_action_space_discrete:\n        action_i = int(self.action_mapper.map(action[0]))\n    else:\n        action_i = action\n\n    if done:\n        new_Q_s_a = self.discount_rate * reward\n    else:\n        Q_s = self.__q_of_state(state)\n        current_Q_s_a = Q_s[action_i]\n        Q_sn = self.__q_of_state(next_state)\n        max_Q_sn = np.max(Q_sn)\n        td = reward+self.discount_rate*max_Q_sn-current_Q_s_a\n        new_Q_s_a = current_Q_s_a + self.learning_rate * td\n\n    index = self.__state_index(state)\n    self.q_table[index][action_i] = new_Q_s_a\n\n    return",
        "branches": []
    },
    "rl.experiments.qlearning_pendulum_v0.tabularQLearning.TabularQLearningAgent.name": {
        "type": "class_method",
        "code": "def name(self):\n    return f'Q_learning_tabular({self.points_per_dim})_v1'",
        "branches": []
    },
    "rl.experiments.qlearning_pendulum_v0.__init__": {
        "type": "module",
        "code": "",
        "branches": []
    },
    "rl.experiments.simple_2d": {
        "type": "directory",
        "code": null,
        "branches": [
            "rl.experiments.simple_2d.alphastar",
            "rl.experiments.simple_2d.my_agent",
            "rl.experiments.simple_2d.np_cache",
            "rl.experiments.simple_2d.random_walk_agent",
            "rl.experiments.simple_2d.run_experiment",
            "rl.experiments.simple_2d.simple_env",
            "rl.experiments.simple_2d.__init__"
        ]
    },
    "rl.experiments.simple_2d.alphastar": {
        "type": "module",
        "code": "# https://github.com/jimkon/alphastar-search/blob/master/alphastar.py\nimport numpy as np\n\nNODE_STATE_INDEX = 0\nNODE_G_INDEX = 1\nNODE_H_INDEX = 2\nNODE_PARENT_INDEX = 3\nNODE_ACTION_INDEX = 4\n\n\nclass __OpenSet:\n\n    def __init__(self, threshold):\n        self.diff_threshold = np.array(threshold)\n        self.points = np.empty([0, threshold.shape[0]])\n        self.nodes = []\n\n    def append(self, node):\n        self.nodes.append(node)\n        self.points = np.append(self.points, np.atleast_2d(node[NODE_STATE_INDEX]), axis=0)\n\n    def contains(self, node):\n        diffs = np.abs(self.points - node[NODE_STATE_INDEX])\n        comps = diffs <= self.diff_threshold\n        if np.any(np.logical_and.reduce(comps, axis=1)):\n            return True\n        return False\n\n    def pop(self, i):\n        node = self.nodes.pop(i)\n        np.delete(self.points, i)\n        return node\n\n    def __len__(self):\n        return len(self.nodes)\n\n\nclass __CloseSet:\n\n    def __init__(self, threshold):\n        self.diff_threshold = np.array(threshold)\n\n        self.points = np.empty([0, threshold.shape[0]])\n        self.nodes = []\n\n    def append(self, node):\n        self.nodes.append(node)\n        self.points = np.append(self.points, np.atleast_2d(node[NODE_STATE_INDEX]), axis=0)\n\n    def contains(self, node):\n        diffs = np.abs(self.points - node[NODE_STATE_INDEX])\n        comps = diffs <= self.diff_threshold\n        if np.any(np.logical_and.reduce(comps, axis=1)):\n            return True\n        return False\n\n\ndef solve(start_state, goal_state, h_func, next_actions_func, state_similarity=None, g_func=None, is_end_state_func=None,\n          next_states_func=None, max_iters=1000): #TODO add is_end_state_func, next_states_func to readme\n\n    def node(state, parent, action=None):  # !\n        # node tuple: state, g, h, parent, action\n        if parent is not None:\n            p_state, p_g, p_h, p_parent, p_action = parent\n        else:\n            p_state, p_g, p_h, p_parent = state, 0., 0., None\n        return state, g_func(state, p_state) + p_g, h_func(state, goal_state), parent, action\n\n    start_state = np.array(start_state)\n    goal_state = np.array(goal_state)\n\n    if g_func is None:\n        g_func = lambda x, y: np.linalg.norm(x - y)\n\n    if state_similarity is None:\n        state_similarity = np.zeros(start_state.shape)\n\n    if is_end_state_func is None:\n        is_end_state_func = lambda x: np.all(np.abs(x - goal_state) <= state_similarity)\n\n    if next_states_func is None:\n        next_states_func = lambda s, a: s + a\n\n    state_len = len(start_state)\n\n    assert state_len == len(goal_state)\n\n    assert state_similarity is not None\n    if isinstance(state_similarity, float) or isinstance(state_similarity, int):\n        state_similarity = np.ones(state_len) * state_similarity\n\n    start_node = node(start_state, None)\n\n    open_set = __OpenSet(state_similarity)\n    close_set = __CloseSet(state_similarity)\n\n    open_set.append(start_node)\n\n    iters = 0\n\n    num_nodes_searched = 0\n\n    while len(open_set) > 0:\n        fs = [node[NODE_G_INDEX] + node[NODE_H_INDEX] for node in open_set.nodes]  # !\n        lowest_f_index = np.argmin(fs)\n\n        current = open_set.pop(lowest_f_index)\n        num_nodes_searched += 1\n\n        close_set.append(current)\n        if is_end_state_func(current[NODE_STATE_INDEX]):\n            return current, num_nodes_searched, open_set.nodes, close_set.nodes\n\n        next_actions = next_actions_func(current[NODE_STATE_INDEX])\n        next_states = next_states_func(current[NODE_STATE_INDEX], next_actions)\n        for state, action in zip(next_states, next_actions):\n\n            successor = node(state, current, action)\n            if close_set.contains(successor):\n                continue\n\n            if not open_set.contains(successor):  # !\n                open_set.append(successor)\n\n        if 0 >= max_iters > iters:\n            break\n        iters += 1\n\n    return start_node, num_nodes_searched, open_set.nodes, close_set.nodes\n\n\ndef get_path(start_node):#TODO update readme\n    states, actions = [], []\n    current = start_node\n    while current is not None:\n        states.append(current[NODE_STATE_INDEX])\n        actions.append(current[NODE_ACTION_INDEX])\n\n        current = current[NODE_PARENT_INDEX]\n\n    states.reverse()\n    actions.reverse()\n    return states, actions\n",
        "branches": [
            "rl.experiments.simple_2d.alphastar.__OpenSet",
            "rl.experiments.simple_2d.alphastar.__CloseSet",
            "rl.experiments.simple_2d.alphastar.solve",
            "rl.experiments.simple_2d.alphastar.get_path"
        ]
    },
    "rl.experiments.simple_2d.alphastar.__OpenSet": {
        "type": "class",
        "code": "class __OpenSet:\n\n    def __init__(self, threshold):\n        self.diff_threshold = np.array(threshold)\n        self.points = np.empty([0, threshold.shape[0]])\n        self.nodes = []\n\n    def append(self, node):\n        self.nodes.append(node)\n        self.points = np.append(self.points, np.atleast_2d(node[NODE_STATE_INDEX]), axis=0)\n\n    def contains(self, node):\n        diffs = np.abs(self.points - node[NODE_STATE_INDEX])\n        comps = diffs <= self.diff_threshold\n        if np.any(np.logical_and.reduce(comps, axis=1)):\n            return True\n        return False\n\n    def pop(self, i):\n        node = self.nodes.pop(i)\n        np.delete(self.points, i)\n        return node\n\n    def __len__(self):\n        return len(self.nodes)",
        "branches": [
            "rl.experiments.simple_2d.alphastar.__OpenSet.__init__",
            "rl.experiments.simple_2d.alphastar.__OpenSet.append",
            "rl.experiments.simple_2d.alphastar.__OpenSet.contains",
            "rl.experiments.simple_2d.alphastar.__OpenSet.pop",
            "rl.experiments.simple_2d.alphastar.__OpenSet.__len__"
        ]
    },
    "rl.experiments.simple_2d.alphastar.__OpenSet.__init__": {
        "type": "class_method",
        "code": "def __init__(self, threshold):\n    self.diff_threshold = np.array(threshold)\n    self.points = np.empty([0, threshold.shape[0]])\n    self.nodes = []",
        "branches": []
    },
    "rl.experiments.simple_2d.alphastar.__OpenSet.append": {
        "type": "class_method",
        "code": "def append(self, node):\n    self.nodes.append(node)\n    self.points = np.append(self.points, np.atleast_2d(node[NODE_STATE_INDEX]), axis=0)",
        "branches": []
    },
    "rl.experiments.simple_2d.alphastar.__OpenSet.contains": {
        "type": "class_method",
        "code": "def contains(self, node):\n    diffs = np.abs(self.points - node[NODE_STATE_INDEX])\n    comps = diffs <= self.diff_threshold\n    if np.any(np.logical_and.reduce(comps, axis=1)):\n        return True\n    return False",
        "branches": []
    },
    "rl.experiments.simple_2d.alphastar.__OpenSet.pop": {
        "type": "class_method",
        "code": "def pop(self, i):\n    node = self.nodes.pop(i)\n    np.delete(self.points, i)\n    return node",
        "branches": []
    },
    "rl.experiments.simple_2d.alphastar.__OpenSet.__len__": {
        "type": "class_method",
        "code": "def __len__(self):\n    return len(self.nodes)",
        "branches": []
    },
    "rl.experiments.simple_2d.alphastar.__CloseSet": {
        "type": "class",
        "code": "class __CloseSet:\n\n    def __init__(self, threshold):\n        self.diff_threshold = np.array(threshold)\n\n        self.points = np.empty([0, threshold.shape[0]])\n        self.nodes = []\n\n    def append(self, node):\n        self.nodes.append(node)\n        self.points = np.append(self.points, np.atleast_2d(node[NODE_STATE_INDEX]), axis=0)\n\n    def contains(self, node):\n        diffs = np.abs(self.points - node[NODE_STATE_INDEX])\n        comps = diffs <= self.diff_threshold\n        if np.any(np.logical_and.reduce(comps, axis=1)):\n            return True\n        return False",
        "branches": [
            "rl.experiments.simple_2d.alphastar.__CloseSet.__init__",
            "rl.experiments.simple_2d.alphastar.__CloseSet.append",
            "rl.experiments.simple_2d.alphastar.__CloseSet.contains"
        ]
    },
    "rl.experiments.simple_2d.alphastar.__CloseSet.__init__": {
        "type": "class_method",
        "code": "def __init__(self, threshold):\n    self.diff_threshold = np.array(threshold)\n\n    self.points = np.empty([0, threshold.shape[0]])\n    self.nodes = []",
        "branches": []
    },
    "rl.experiments.simple_2d.alphastar.__CloseSet.append": {
        "type": "class_method",
        "code": "def append(self, node):\n    self.nodes.append(node)\n    self.points = np.append(self.points, np.atleast_2d(node[NODE_STATE_INDEX]), axis=0)",
        "branches": []
    },
    "rl.experiments.simple_2d.alphastar.__CloseSet.contains": {
        "type": "class_method",
        "code": "def contains(self, node):\n    diffs = np.abs(self.points - node[NODE_STATE_INDEX])\n    comps = diffs <= self.diff_threshold\n    if np.any(np.logical_and.reduce(comps, axis=1)):\n        return True\n    return False",
        "branches": []
    },
    "rl.experiments.simple_2d.alphastar.solve": {
        "type": "function",
        "code": "def solve(start_state, goal_state, h_func, next_actions_func, state_similarity=None, g_func=None, is_end_state_func=None,\n          next_states_func=None, max_iters=1000): #TODO add is_end_state_func, next_states_func to readme\n\n    def node(state, parent, action=None):  # !\n        # node tuple: state, g, h, parent, action\n        if parent is not None:\n            p_state, p_g, p_h, p_parent, p_action = parent\n        else:\n            p_state, p_g, p_h, p_parent = state, 0., 0., None\n        return state, g_func(state, p_state) + p_g, h_func(state, goal_state), parent, action\n\n    start_state = np.array(start_state)\n    goal_state = np.array(goal_state)\n\n    if g_func is None:\n        g_func = lambda x, y: np.linalg.norm(x - y)\n\n    if state_similarity is None:\n        state_similarity = np.zeros(start_state.shape)\n\n    if is_end_state_func is None:\n        is_end_state_func = lambda x: np.all(np.abs(x - goal_state) <= state_similarity)\n\n    if next_states_func is None:\n        next_states_func = lambda s, a: s + a\n\n    state_len = len(start_state)\n\n    assert state_len == len(goal_state)\n\n    assert state_similarity is not None\n    if isinstance(state_similarity, float) or isinstance(state_similarity, int):\n        state_similarity = np.ones(state_len) * state_similarity\n\n    start_node = node(start_state, None)\n\n    open_set = __OpenSet(state_similarity)\n    close_set = __CloseSet(state_similarity)\n\n    open_set.append(start_node)\n\n    iters = 0\n\n    num_nodes_searched = 0\n\n    while len(open_set) > 0:\n        fs = [node[NODE_G_INDEX] + node[NODE_H_INDEX] for node in open_set.nodes]  # !\n        lowest_f_index = np.argmin(fs)\n\n        current = open_set.pop(lowest_f_index)\n        num_nodes_searched += 1\n\n        close_set.append(current)\n        if is_end_state_func(current[NODE_STATE_INDEX]):\n            return current, num_nodes_searched, open_set.nodes, close_set.nodes\n\n        next_actions = next_actions_func(current[NODE_STATE_INDEX])\n        next_states = next_states_func(current[NODE_STATE_INDEX], next_actions)\n        for state, action in zip(next_states, next_actions):\n\n            successor = node(state, current, action)\n            if close_set.contains(successor):\n                continue\n\n            if not open_set.contains(successor):  # !\n                open_set.append(successor)\n\n        if 0 >= max_iters > iters:\n            break\n        iters += 1\n\n    return start_node, num_nodes_searched, open_set.nodes, close_set.nodes",
        "branches": []
    },
    "rl.experiments.simple_2d.alphastar.get_path": {
        "type": "function",
        "code": "def get_path(start_node):#TODO update readme\n    states, actions = [], []\n    current = start_node\n    while current is not None:\n        states.append(current[NODE_STATE_INDEX])\n        actions.append(current[NODE_ACTION_INDEX])\n\n        current = current[NODE_PARENT_INDEX]\n\n    states.reverse()\n    actions.reverse()\n    return states, actions",
        "branches": []
    },
    "rl.experiments.simple_2d.my_agent": {
        "type": "module",
        "code": "from rl.core.rl.agent import AbstractAgent\nfrom rl.experiments.simple_2d.simple_env import *\nfrom rl.experiments.simple_2d.alphastar import solve, get_path\nfrom rl.core.utilities.logging import Logger\n\nlogger = Logger('my_agent')\n\n\ndef create_path_plot(path, plot=False):\n    img = DEFAULT_MAP.copy()\n    img = np.dstack([img, img, img])\n\n    cv2.polylines(img, [path], False, (0., 1., 1.))\n\n    (x1, y1), (x2, y2) = path[0], path[-1]\n    img[y1, x1] = np.array([1, 0.2, 0.2])\n    img[y2, x2] = np.array([0.2, 0.2, 1])\n\n    img = cv2.resize(img, (500, 500), interpolation=cv2.INTER_NEAREST)  # INTER_AREA\n\n    if plot:\n        cv2.imshow('plotting the path', img)\n        cv2.waitKey(0)\n\n    return img\n\n\ndef state_plus_actions(state, actions):\n    new_state = state\n    path = [new_state]\n    for action in actions:\n        new_state = new_state+action\n        path.append(new_state)\n\n    return np.array(path)\n\n\ndef find_state_in_path(state, path):\n    if state is None or path is None:\n        return -1\n\n    for i, s in enumerate(path):\n        if np.array_equal(state, s):\n            return i\n    return -1\n\n\ndef states_equal(state_1, state_2):\n    if state_1 is None or state_2 is None:\n        return False\n    return np.array_equal(state_1, state_2)\n\n\ndef max_reward_state():\n    map = DEFAULT_MAP\n    max_1d = np.max(map, axis=0)\n    argmax_x = np.argmax(max_1d)\n    argmax_y = np.argmax(map[:, argmax_x])\n    return np.array([argmax_x, argmax_y])\n\n\nclass MyAgent_Abstract_SE_POC(AbstractAgent):\n\n    def all_possible_actions(self):\n        range_1d = list(range(MIN_ACTION, MAX_ACTION+1))\n\n        actions = np.array([[a1, a2] for a1 in range_1d for a2 in range_1d])\n\n        return actions\n\n    def transition(self, state, action):\n        next_state = np.clip(state + action,\n                             a_min=[0, 0],\n                             a_max=[DEFAULT_MAP_WIDTH - 1, DEFAULT_MAP_HEIGHT - 1])\n\n        return next_state\n\n    def transitions(self, state):\n        actions = self.all_possible_actions()\n        next_states = np.array([self.transition(state, a) for a in actions])\n        return next_states, actions\n\n    def reward(self, state):\n        state_x, state_y = state\n        reward = DEFAULT_MAP[state_x][state_y]\n        return reward\n\n    def step_distance(self, state_a, state_b):\n        return np.max(np.abs(state_a-state_b))\n\n    def euclidean_distance(self, state_a, state_b):\n        return np.sqrt(np.sum(np.square(state_b-state_a)))\n\n\nclass MyAgent_Greedy_SE_POC(MyAgent_Abstract_SE_POC):\n\n    name = 'my_greedy_POC_agent'\n\n    def act(self, state):\n        next_states, actions = self.transitions(state)\n        rewards = [self.reward(next_state) for next_state in next_states]\n        index = np.argmax(rewards)\n        res_action = actions[index]\n        return res_action\n\n\nclass MyAgent_Abstract_PathPlanning_POC(MyAgent_Abstract_SE_POC):\n\n    def __init__(self):\n        self.__path = None\n        self.__actions = None\n\n    def act(self, state):\n        action = self.follow_the_path(state)\n        return action\n\n    def follow_the_path(self, current_state):\n        target_state = self.target_state()\n        if self.__path is None or self.__actions is None:\n            logger.log(f\"Calculating path from {current_state} to {target_state}. Reason: no path found.\")\n            self.__path, self.__actions = self.calculate_path_and_actions(current_state, target_state)\n\n        path_last_state, path_first_state = self.__path[-1], self.__path[0]\n\n        if not states_equal(target_state, path_last_state) or not states_equal(current_state, path_first_state):\n            logger.log(f\"Calculating path from {current_state} to {target_state}. Reason: Path failed. new target state:{not states_equal(target_state, path_last_state)}, current state missed:{not states_equal(current_state, path_first_state)}\")\n            self.__path, self.__actions = self.calculate_path_and_actions(current_state, target_state)\n\n        result_action = self.__actions[0]\n        logger.log(f\"Picking the last action {result_action}\")\n        if len(self.__actions) > 1:\n            self.__path, self.__actions = self.__path[1:], self.__actions[1:]\n        return result_action\n\n    @logger.log_func_call('agent')\n    def calculate_path_and_actions(self, state_a, state_b):\n        raise NotImplementedError\n\n    @logger.log_func_call('agent')\n    def target_state(self):\n        raise NotImplementedError\n\n\nclass MyAgent_ShortestPath_SE_POC(MyAgent_Abstract_PathPlanning_POC):\n\n    def target_state(self):\n        return max_reward_state()\n\n    def calculate_path_and_actions(self, state_a, state_b):\n        path, actions = self.shortest_path_actions(state_a, state_b)\n        if len(path) > 2:\n            create_path_plot(path)\n        return path, actions\n\n    def shortest_path_actions(self, state_a, state_b):\n        if states_equal(state_a, state_b):\n            return np.array([state_a, state_b]), np.array([[0, 0]])\n\n        delta = state_b - state_a\n        cur_state = np.array(state_a)\n        states = [cur_state]\n        actions = []\n        for i in range(self.step_distance(state_a, state_b)):\n            action = np.clip(delta,\n                             a_min=MIN_ACTION,\n                             a_max=MAX_ACTION)\n            delta -= action\n            actions.append(action)\n\n            cur_state = cur_state + action\n            states.append(cur_state)\n        logger.log(f\"Shortest path from {state_a} to {state_b} is {states} with actions {actions}\")\n        return np.array(states), np.array(actions)\n\n    def name(self):\n        return 'my_shortest_path_to_max_reward_state_POC_agent'\n\n\nclass MyAgent_BestPath_SE_POC(MyAgent_Abstract_PathPlanning_POC):\n\n    def target_state(self):\n        return max_reward_state()\n\n    def calculate_path_and_actions(self, state_a, state_b):\n        path, actions = self.best_path_actions(state_a, state_b)\n        if len(path) > 2:\n            img = create_path_plot(path, plot=False)\n            logger.log_image(img)\n        return path, actions\n\n    def best_path_actions(self, state_a, state_b):\n        if states_equal(state_a, state_b):\n            return np.array([state_a, state_b]), np.array([[0, 0]])\n        start_node, n, open_set, close_set = solve(start_state=state_a,\n                                                   goal_state=state_b,\n                                                   h_func=lambda s_a, s_b: self.step_distance(s_a, s_b),\n                                                   next_actions_func=lambda s: self.all_possible_actions(),\n                                                   next_states_func=lambda s, a: self.transitions(s)[0],\n                                                   g_func=lambda s1, s2: -self.reward(s1))\n        logger.log(f\"Best path calculation: {n} nodes searched.\")\n        states, actions = get_path(start_node)\n\n        if len(actions) > 1:\n            actions = actions[1:]\n\n        logger.log(f\"Best path from {state_a} to {state_b} is {states} with actions {actions}\")\n\n        return np.array(states), np.array(actions)\n\n\n    def name(self):\n        return 'my_best_path_to_max_reward_state_POC_agent'",
        "branches": [
            "rl.experiments.simple_2d.my_agent.create_path_plot",
            "rl.experiments.simple_2d.my_agent.state_plus_actions",
            "rl.experiments.simple_2d.my_agent.find_state_in_path",
            "rl.experiments.simple_2d.my_agent.states_equal",
            "rl.experiments.simple_2d.my_agent.max_reward_state",
            "rl.experiments.simple_2d.my_agent.MyAgent_Abstract_SE_POC",
            "rl.experiments.simple_2d.my_agent.MyAgent_Greedy_SE_POC",
            "rl.experiments.simple_2d.my_agent.MyAgent_Abstract_PathPlanning_POC",
            "rl.experiments.simple_2d.my_agent.MyAgent_ShortestPath_SE_POC",
            "rl.experiments.simple_2d.my_agent.MyAgent_BestPath_SE_POC"
        ]
    },
    "rl.experiments.simple_2d.my_agent.create_path_plot": {
        "type": "function",
        "code": "def create_path_plot(path, plot=False):\n    img = DEFAULT_MAP.copy()\n    img = np.dstack([img, img, img])\n\n    cv2.polylines(img, [path], False, (0., 1., 1.))\n\n    (x1, y1), (x2, y2) = path[0], path[-1]\n    img[y1, x1] = np.array([1, 0.2, 0.2])\n    img[y2, x2] = np.array([0.2, 0.2, 1])\n\n    img = cv2.resize(img, (500, 500), interpolation=cv2.INTER_NEAREST)  # INTER_AREA\n\n    if plot:\n        cv2.imshow('plotting the path', img)\n        cv2.waitKey(0)\n\n    return img",
        "branches": []
    },
    "rl.experiments.simple_2d.my_agent.state_plus_actions": {
        "type": "function",
        "code": "def state_plus_actions(state, actions):\n    new_state = state\n    path = [new_state]\n    for action in actions:\n        new_state = new_state+action\n        path.append(new_state)\n\n    return np.array(path)",
        "branches": []
    },
    "rl.experiments.simple_2d.my_agent.find_state_in_path": {
        "type": "function",
        "code": "def find_state_in_path(state, path):\n    if state is None or path is None:\n        return -1\n\n    for i, s in enumerate(path):\n        if np.array_equal(state, s):\n            return i\n    return -1",
        "branches": []
    },
    "rl.experiments.simple_2d.my_agent.states_equal": {
        "type": "function",
        "code": "def states_equal(state_1, state_2):\n    if state_1 is None or state_2 is None:\n        return False\n    return np.array_equal(state_1, state_2)",
        "branches": []
    },
    "rl.experiments.simple_2d.my_agent.max_reward_state": {
        "type": "function",
        "code": "def max_reward_state():\n    map = DEFAULT_MAP\n    max_1d = np.max(map, axis=0)\n    argmax_x = np.argmax(max_1d)\n    argmax_y = np.argmax(map[:, argmax_x])\n    return np.array([argmax_x, argmax_y])",
        "branches": []
    },
    "rl.experiments.simple_2d.my_agent.MyAgent_Abstract_SE_POC": {
        "type": "class",
        "code": "class MyAgent_Abstract_SE_POC(AbstractAgent):\n\n    def all_possible_actions(self):\n        range_1d = list(range(MIN_ACTION, MAX_ACTION+1))\n\n        actions = np.array([[a1, a2] for a1 in range_1d for a2 in range_1d])\n\n        return actions\n\n    def transition(self, state, action):\n        next_state = np.clip(state + action,\n                             a_min=[0, 0],\n                             a_max=[DEFAULT_MAP_WIDTH - 1, DEFAULT_MAP_HEIGHT - 1])\n\n        return next_state\n\n    def transitions(self, state):\n        actions = self.all_possible_actions()\n        next_states = np.array([self.transition(state, a) for a in actions])\n        return next_states, actions\n\n    def reward(self, state):\n        state_x, state_y = state\n        reward = DEFAULT_MAP[state_x][state_y]\n        return reward\n\n    def step_distance(self, state_a, state_b):\n        return np.max(np.abs(state_a-state_b))\n\n    def euclidean_distance(self, state_a, state_b):\n        return np.sqrt(np.sum(np.square(state_b-state_a)))",
        "branches": [
            "rl.experiments.simple_2d.my_agent.MyAgent_Abstract_SE_POC.all_possible_actions",
            "rl.experiments.simple_2d.my_agent.MyAgent_Abstract_SE_POC.transition",
            "rl.experiments.simple_2d.my_agent.MyAgent_Abstract_SE_POC.transitions",
            "rl.experiments.simple_2d.my_agent.MyAgent_Abstract_SE_POC.reward",
            "rl.experiments.simple_2d.my_agent.MyAgent_Abstract_SE_POC.step_distance",
            "rl.experiments.simple_2d.my_agent.MyAgent_Abstract_SE_POC.euclidean_distance"
        ]
    },
    "rl.experiments.simple_2d.my_agent.MyAgent_Abstract_SE_POC.all_possible_actions": {
        "type": "class_method",
        "code": "def all_possible_actions(self):\n    range_1d = list(range(MIN_ACTION, MAX_ACTION+1))\n\n    actions = np.array([[a1, a2] for a1 in range_1d for a2 in range_1d])\n\n    return actions",
        "branches": []
    },
    "rl.experiments.simple_2d.my_agent.MyAgent_Abstract_SE_POC.transition": {
        "type": "class_method",
        "code": "def transition(self, state, action):\n    next_state = np.clip(state + action,\n                         a_min=[0, 0],\n                         a_max=[DEFAULT_MAP_WIDTH - 1, DEFAULT_MAP_HEIGHT - 1])\n\n    return next_state",
        "branches": []
    },
    "rl.experiments.simple_2d.my_agent.MyAgent_Abstract_SE_POC.transitions": {
        "type": "class_method",
        "code": "def transitions(self, state):\n    actions = self.all_possible_actions()\n    next_states = np.array([self.transition(state, a) for a in actions])\n    return next_states, actions",
        "branches": []
    },
    "rl.experiments.simple_2d.my_agent.MyAgent_Abstract_SE_POC.reward": {
        "type": "class_method",
        "code": "def reward(self, state):\n    state_x, state_y = state\n    reward = DEFAULT_MAP[state_x][state_y]\n    return reward",
        "branches": []
    },
    "rl.experiments.simple_2d.my_agent.MyAgent_Abstract_SE_POC.step_distance": {
        "type": "class_method",
        "code": "def step_distance(self, state_a, state_b):\n    return np.max(np.abs(state_a-state_b))",
        "branches": []
    },
    "rl.experiments.simple_2d.my_agent.MyAgent_Abstract_SE_POC.euclidean_distance": {
        "type": "class_method",
        "code": "def euclidean_distance(self, state_a, state_b):\n    return np.sqrt(np.sum(np.square(state_b-state_a)))",
        "branches": []
    },
    "rl.experiments.simple_2d.my_agent.MyAgent_Greedy_SE_POC": {
        "type": "class",
        "code": "class MyAgent_Greedy_SE_POC(MyAgent_Abstract_SE_POC):\n\n    name = 'my_greedy_POC_agent'\n\n    def act(self, state):\n        next_states, actions = self.transitions(state)\n        rewards = [self.reward(next_state) for next_state in next_states]\n        index = np.argmax(rewards)\n        res_action = actions[index]\n        return res_action",
        "branches": [
            "rl.experiments.simple_2d.my_agent.MyAgent_Greedy_SE_POC.act"
        ]
    },
    "rl.experiments.simple_2d.my_agent.MyAgent_Greedy_SE_POC.act": {
        "type": "class_method",
        "code": "def act(self, state):\n    next_states, actions = self.transitions(state)\n    rewards = [self.reward(next_state) for next_state in next_states]\n    index = np.argmax(rewards)\n    res_action = actions[index]\n    return res_action",
        "branches": []
    },
    "rl.experiments.simple_2d.my_agent.MyAgent_Abstract_PathPlanning_POC": {
        "type": "class",
        "code": "class MyAgent_Abstract_PathPlanning_POC(MyAgent_Abstract_SE_POC):\n\n    def __init__(self):\n        self.__path = None\n        self.__actions = None\n\n    def act(self, state):\n        action = self.follow_the_path(state)\n        return action\n\n    def follow_the_path(self, current_state):\n        target_state = self.target_state()\n        if self.__path is None or self.__actions is None:\n            logger.log(f\"Calculating path from {current_state} to {target_state}. Reason: no path found.\")\n            self.__path, self.__actions = self.calculate_path_and_actions(current_state, target_state)\n\n        path_last_state, path_first_state = self.__path[-1], self.__path[0]\n\n        if not states_equal(target_state, path_last_state) or not states_equal(current_state, path_first_state):\n            logger.log(f\"Calculating path from {current_state} to {target_state}. Reason: Path failed. new target state:{not states_equal(target_state, path_last_state)}, current state missed:{not states_equal(current_state, path_first_state)}\")\n            self.__path, self.__actions = self.calculate_path_and_actions(current_state, target_state)\n\n        result_action = self.__actions[0]\n        logger.log(f\"Picking the last action {result_action}\")\n        if len(self.__actions) > 1:\n            self.__path, self.__actions = self.__path[1:], self.__actions[1:]\n        return result_action\n\n    @logger.log_func_call('agent')\n    def calculate_path_and_actions(self, state_a, state_b):\n        raise NotImplementedError\n\n    @logger.log_func_call('agent')\n    def target_state(self):\n        raise NotImplementedError",
        "branches": [
            "rl.experiments.simple_2d.my_agent.MyAgent_Abstract_PathPlanning_POC.__init__",
            "rl.experiments.simple_2d.my_agent.MyAgent_Abstract_PathPlanning_POC.act",
            "rl.experiments.simple_2d.my_agent.MyAgent_Abstract_PathPlanning_POC.follow_the_path",
            "rl.experiments.simple_2d.my_agent.MyAgent_Abstract_PathPlanning_POC.calculate_path_and_actions",
            "rl.experiments.simple_2d.my_agent.MyAgent_Abstract_PathPlanning_POC.target_state"
        ]
    },
    "rl.experiments.simple_2d.my_agent.MyAgent_Abstract_PathPlanning_POC.__init__": {
        "type": "class_method",
        "code": "def __init__(self):\n    self.__path = None\n    self.__actions = None",
        "branches": []
    },
    "rl.experiments.simple_2d.my_agent.MyAgent_Abstract_PathPlanning_POC.act": {
        "type": "class_method",
        "code": "def act(self, state):\n    action = self.follow_the_path(state)\n    return action",
        "branches": []
    },
    "rl.experiments.simple_2d.my_agent.MyAgent_Abstract_PathPlanning_POC.follow_the_path": {
        "type": "class_method",
        "code": "def follow_the_path(self, current_state):\n    target_state = self.target_state()\n    if self.__path is None or self.__actions is None:\n        logger.log(f\"Calculating path from {current_state} to {target_state}. Reason: no path found.\")\n        self.__path, self.__actions = self.calculate_path_and_actions(current_state, target_state)\n\n    path_last_state, path_first_state = self.__path[-1], self.__path[0]\n\n    if not states_equal(target_state, path_last_state) or not states_equal(current_state, path_first_state):\n        logger.log(f\"Calculating path from {current_state} to {target_state}. Reason: Path failed. new target state:{not states_equal(target_state, path_last_state)}, current state missed:{not states_equal(current_state, path_first_state)}\")\n        self.__path, self.__actions = self.calculate_path_and_actions(current_state, target_state)\n\n    result_action = self.__actions[0]\n    logger.log(f\"Picking the last action {result_action}\")\n    if len(self.__actions) > 1:\n        self.__path, self.__actions = self.__path[1:], self.__actions[1:]\n    return result_action",
        "branches": []
    },
    "rl.experiments.simple_2d.my_agent.MyAgent_Abstract_PathPlanning_POC.calculate_path_and_actions": {
        "type": "class_method",
        "code": "@logger.log_func_call('agent')\ndef calculate_path_and_actions(self, state_a, state_b):\n    raise NotImplementedError",
        "branches": []
    },
    "rl.experiments.simple_2d.my_agent.MyAgent_Abstract_PathPlanning_POC.target_state": {
        "type": "class_method",
        "code": "@logger.log_func_call('agent')\ndef target_state(self):\n    raise NotImplementedError",
        "branches": []
    },
    "rl.experiments.simple_2d.my_agent.MyAgent_ShortestPath_SE_POC": {
        "type": "class",
        "code": "class MyAgent_ShortestPath_SE_POC(MyAgent_Abstract_PathPlanning_POC):\n\n    def target_state(self):\n        return max_reward_state()\n\n    def calculate_path_and_actions(self, state_a, state_b):\n        path, actions = self.shortest_path_actions(state_a, state_b)\n        if len(path) > 2:\n            create_path_plot(path)\n        return path, actions\n\n    def shortest_path_actions(self, state_a, state_b):\n        if states_equal(state_a, state_b):\n            return np.array([state_a, state_b]), np.array([[0, 0]])\n\n        delta = state_b - state_a\n        cur_state = np.array(state_a)\n        states = [cur_state]\n        actions = []\n        for i in range(self.step_distance(state_a, state_b)):\n            action = np.clip(delta,\n                             a_min=MIN_ACTION,\n                             a_max=MAX_ACTION)\n            delta -= action\n            actions.append(action)\n\n            cur_state = cur_state + action\n            states.append(cur_state)\n        logger.log(f\"Shortest path from {state_a} to {state_b} is {states} with actions {actions}\")\n        return np.array(states), np.array(actions)\n\n    def name(self):\n        return 'my_shortest_path_to_max_reward_state_POC_agent'",
        "branches": [
            "rl.experiments.simple_2d.my_agent.MyAgent_ShortestPath_SE_POC.target_state",
            "rl.experiments.simple_2d.my_agent.MyAgent_ShortestPath_SE_POC.calculate_path_and_actions",
            "rl.experiments.simple_2d.my_agent.MyAgent_ShortestPath_SE_POC.shortest_path_actions",
            "rl.experiments.simple_2d.my_agent.MyAgent_ShortestPath_SE_POC.name"
        ]
    },
    "rl.experiments.simple_2d.my_agent.MyAgent_ShortestPath_SE_POC.target_state": {
        "type": "class_method",
        "code": "def target_state(self):\n    return max_reward_state()",
        "branches": []
    },
    "rl.experiments.simple_2d.my_agent.MyAgent_ShortestPath_SE_POC.calculate_path_and_actions": {
        "type": "class_method",
        "code": "def calculate_path_and_actions(self, state_a, state_b):\n    path, actions = self.shortest_path_actions(state_a, state_b)\n    if len(path) > 2:\n        create_path_plot(path)\n    return path, actions",
        "branches": []
    },
    "rl.experiments.simple_2d.my_agent.MyAgent_ShortestPath_SE_POC.shortest_path_actions": {
        "type": "class_method",
        "code": "def shortest_path_actions(self, state_a, state_b):\n    if states_equal(state_a, state_b):\n        return np.array([state_a, state_b]), np.array([[0, 0]])\n\n    delta = state_b - state_a\n    cur_state = np.array(state_a)\n    states = [cur_state]\n    actions = []\n    for i in range(self.step_distance(state_a, state_b)):\n        action = np.clip(delta,\n                         a_min=MIN_ACTION,\n                         a_max=MAX_ACTION)\n        delta -= action\n        actions.append(action)\n\n        cur_state = cur_state + action\n        states.append(cur_state)\n    logger.log(f\"Shortest path from {state_a} to {state_b} is {states} with actions {actions}\")\n    return np.array(states), np.array(actions)",
        "branches": []
    },
    "rl.experiments.simple_2d.my_agent.MyAgent_ShortestPath_SE_POC.name": {
        "type": "class_method",
        "code": "def name(self):\n    return 'my_shortest_path_to_max_reward_state_POC_agent'",
        "branches": []
    },
    "rl.experiments.simple_2d.my_agent.MyAgent_BestPath_SE_POC": {
        "type": "class",
        "code": "class MyAgent_BestPath_SE_POC(MyAgent_Abstract_PathPlanning_POC):\n\n    def target_state(self):\n        return max_reward_state()\n\n    def calculate_path_and_actions(self, state_a, state_b):\n        path, actions = self.best_path_actions(state_a, state_b)\n        if len(path) > 2:\n            img = create_path_plot(path, plot=False)\n            logger.log_image(img)\n        return path, actions\n\n    def best_path_actions(self, state_a, state_b):\n        if states_equal(state_a, state_b):\n            return np.array([state_a, state_b]), np.array([[0, 0]])\n        start_node, n, open_set, close_set = solve(start_state=state_a,\n                                                   goal_state=state_b,\n                                                   h_func=lambda s_a, s_b: self.step_distance(s_a, s_b),\n                                                   next_actions_func=lambda s: self.all_possible_actions(),\n                                                   next_states_func=lambda s, a: self.transitions(s)[0],\n                                                   g_func=lambda s1, s2: -self.reward(s1))\n        logger.log(f\"Best path calculation: {n} nodes searched.\")\n        states, actions = get_path(start_node)\n\n        if len(actions) > 1:\n            actions = actions[1:]\n\n        logger.log(f\"Best path from {state_a} to {state_b} is {states} with actions {actions}\")\n\n        return np.array(states), np.array(actions)\n\n\n    def name(self):\n        return 'my_best_path_to_max_reward_state_POC_agent'",
        "branches": [
            "rl.experiments.simple_2d.my_agent.MyAgent_BestPath_SE_POC.target_state",
            "rl.experiments.simple_2d.my_agent.MyAgent_BestPath_SE_POC.calculate_path_and_actions",
            "rl.experiments.simple_2d.my_agent.MyAgent_BestPath_SE_POC.best_path_actions",
            "rl.experiments.simple_2d.my_agent.MyAgent_BestPath_SE_POC.name"
        ]
    },
    "rl.experiments.simple_2d.my_agent.MyAgent_BestPath_SE_POC.target_state": {
        "type": "class_method",
        "code": "def target_state(self):\n    return max_reward_state()",
        "branches": []
    },
    "rl.experiments.simple_2d.my_agent.MyAgent_BestPath_SE_POC.calculate_path_and_actions": {
        "type": "class_method",
        "code": "def calculate_path_and_actions(self, state_a, state_b):\n    path, actions = self.best_path_actions(state_a, state_b)\n    if len(path) > 2:\n        img = create_path_plot(path, plot=False)\n        logger.log_image(img)\n    return path, actions",
        "branches": []
    },
    "rl.experiments.simple_2d.my_agent.MyAgent_BestPath_SE_POC.best_path_actions": {
        "type": "class_method",
        "code": "def best_path_actions(self, state_a, state_b):\n    if states_equal(state_a, state_b):\n        return np.array([state_a, state_b]), np.array([[0, 0]])\n    start_node, n, open_set, close_set = solve(start_state=state_a,\n                                               goal_state=state_b,\n                                               h_func=lambda s_a, s_b: self.step_distance(s_a, s_b),\n                                               next_actions_func=lambda s: self.all_possible_actions(),\n                                               next_states_func=lambda s, a: self.transitions(s)[0],\n                                               g_func=lambda s1, s2: -self.reward(s1))\n    logger.log(f\"Best path calculation: {n} nodes searched.\")\n    states, actions = get_path(start_node)\n\n    if len(actions) > 1:\n        actions = actions[1:]\n\n    logger.log(f\"Best path from {state_a} to {state_b} is {states} with actions {actions}\")\n\n    return np.array(states), np.array(actions)",
        "branches": []
    },
    "rl.experiments.simple_2d.my_agent.MyAgent_BestPath_SE_POC.name": {
        "type": "class_method",
        "code": "def name(self):\n    return 'my_best_path_to_max_reward_state_POC_agent'",
        "branches": []
    },
    "rl.experiments.simple_2d.np_cache": {
        "type": "module",
        "code": "# https://gist.github.com/Susensio/61f4fee01150caaac1e10fc5f005eb75\n\nfrom functools import lru_cache, wraps\nimport numpy as np\n\n\ndef np_cache(*args, **kwargs):\n    \"\"\"LRU cache implementation for functions whose FIRST parameter is a numpy array\n    >>> array = np.array([[1, 2, 3], [4, 5, 6]])\n    >>> @np_cache(maxsize=256)\n    ... def multiply(array, factor):\n    ...     print(\"Calculating...\")\n    ...     return factor*array\n    >>> multiply(array, 2)\n    Calculating...\n    array([[ 2,  4,  6],\n           [ 8, 10, 12]])\n    >>> multiply(array, 2)\n    array([[ 2,  4,  6],\n           [ 8, 10, 12]])\n    >>> multiply.cache_info()\n    CacheInfo(hits=1, misses=1, maxsize=256, currsize=1)\n\n    \"\"\"\n\n    def decorator(function):\n        @wraps(function)\n        def wrapper(np_array, *args, **kwargs):\n            hashable_array = array_to_tuple(np_array)\n            return cached_wrapper(hashable_array, *args, **kwargs)\n\n        @lru_cache(*args, **kwargs)\n        def cached_wrapper(hashable_array, *args, **kwargs):\n            array = np.array(hashable_array)\n            return function(array, *args, **kwargs)\n\n        def array_to_tuple(np_array):\n            \"\"\"Iterates recursivelly.\"\"\"\n            try:\n                return tuple(array_to_tuple(_) for _ in np_array)\n            except TypeError:\n                return np_array\n\n        # copy lru_cache attributes over too\n        wrapper.cache_info = cached_wrapper.cache_info\n        wrapper.cache_clear = cached_wrapper.cache_clear\n\n        return wrapper\n\n    return decorator",
        "branches": [
            "rl.experiments.simple_2d.np_cache.np_cache"
        ]
    },
    "rl.experiments.simple_2d.np_cache.np_cache": {
        "type": "function",
        "code": "def np_cache(*args, **kwargs):\n    \"\"\"LRU cache implementation for functions whose FIRST parameter is a numpy array\n    >>> array = np.array([[1, 2, 3], [4, 5, 6]])\n    >>> @np_cache(maxsize=256)\n    ... def multiply(array, factor):\n    ...     print(\"Calculating...\")\n    ...     return factor*array\n    >>> multiply(array, 2)\n    Calculating...\n    array([[ 2,  4,  6],\n           [ 8, 10, 12]])\n    >>> multiply(array, 2)\n    array([[ 2,  4,  6],\n           [ 8, 10, 12]])\n    >>> multiply.cache_info()\n    CacheInfo(hits=1, misses=1, maxsize=256, currsize=1)\n\n    \"\"\"\n\n    def decorator(function):\n        @wraps(function)\n        def wrapper(np_array, *args, **kwargs):\n            hashable_array = array_to_tuple(np_array)\n            return cached_wrapper(hashable_array, *args, **kwargs)\n\n        @lru_cache(*args, **kwargs)\n        def cached_wrapper(hashable_array, *args, **kwargs):\n            array = np.array(hashable_array)\n            return function(array, *args, **kwargs)\n\n        def array_to_tuple(np_array):\n            \"\"\"Iterates recursivelly.\"\"\"\n            try:\n                return tuple(array_to_tuple(_) for _ in np_array)\n            except TypeError:\n                return np_array\n\n        # copy lru_cache attributes over too\n        wrapper.cache_info = cached_wrapper.cache_info\n        wrapper.cache_clear = cached_wrapper.cache_clear\n\n        return wrapper\n\n    return decorator",
        "branches": []
    },
    "rl.experiments.simple_2d.random_walk_agent": {
        "type": "module",
        "code": "import random\n\nfrom rl.core.rl.agent import AbstractAgent\nfrom rl.core.utilities.logging import Logger\n\nDIR = [\n        [-1, 1],\n        [-1, 0],\n        [-1, -1],\n        [0, -1],\n        [0, 0],\n        [0, 1],\n        [1, -1],\n        [1, 0],\n        [1, 1],\n    ]\n\nlogger = Logger('random_walk_agent')\n\n\nclass RandomWalkAgent(AbstractAgent):\n\n    def __init__(self):\n        self.step_count = -1\n        self.direction_duration = 0\n        self.direction = 0\n\n    def act(self, state):\n        self.step_count += 1\n\n        if self.step_count == self.direction_duration:\n            self.step_count = 0\n            self.direction_duration = random.randint(3, 10)\n            self.direction = random.randint(0, len(DIR)-1)\n\n        return DIR[self.direction]\n\n    def name(self):\n        return 'Random Walk Agent'\n\n\n\n",
        "branches": [
            "rl.experiments.simple_2d.random_walk_agent.RandomWalkAgent"
        ]
    },
    "rl.experiments.simple_2d.random_walk_agent.RandomWalkAgent": {
        "type": "class",
        "code": "class RandomWalkAgent(AbstractAgent):\n\n    def __init__(self):\n        self.step_count = -1\n        self.direction_duration = 0\n        self.direction = 0\n\n    def act(self, state):\n        self.step_count += 1\n\n        if self.step_count == self.direction_duration:\n            self.step_count = 0\n            self.direction_duration = random.randint(3, 10)\n            self.direction = random.randint(0, len(DIR)-1)\n\n        return DIR[self.direction]\n\n    def name(self):\n        return 'Random Walk Agent'",
        "branches": [
            "rl.experiments.simple_2d.random_walk_agent.RandomWalkAgent.__init__",
            "rl.experiments.simple_2d.random_walk_agent.RandomWalkAgent.act",
            "rl.experiments.simple_2d.random_walk_agent.RandomWalkAgent.name"
        ]
    },
    "rl.experiments.simple_2d.random_walk_agent.RandomWalkAgent.__init__": {
        "type": "class_method",
        "code": "def __init__(self):\n    self.step_count = -1\n    self.direction_duration = 0\n    self.direction = 0",
        "branches": []
    },
    "rl.experiments.simple_2d.random_walk_agent.RandomWalkAgent.act": {
        "type": "class_method",
        "code": "def act(self, state):\n    self.step_count += 1\n\n    if self.step_count == self.direction_duration:\n        self.step_count = 0\n        self.direction_duration = random.randint(3, 10)\n        self.direction = random.randint(0, len(DIR)-1)\n\n    return DIR[self.direction]",
        "branches": []
    },
    "rl.experiments.simple_2d.random_walk_agent.RandomWalkAgent.name": {
        "type": "class_method",
        "code": "def name(self):\n    return 'Random Walk Agent'",
        "branches": []
    },
    "rl.experiments.simple_2d.run_experiment": {
        "type": "module",
        "code": "from rl.core.rl.engine import run_episodes\nfrom rl.core.utilities.logging import save_loggers\n\nfrom rl.experiments.simple_2d.my_agent import *\n\n\ndef run_experiment():\n    # agent = RandomWalkAgent()\n    # agent = MyAgent_Greedy_SE_POC()\n    # agent = MyAgent_ShortestPath_SE_POC()\n    agent = MyAgent_BestPath_SE_POC()\n\n    env = SimpleEnv()\n\n    experiment_name = 'simple_experiment_test'\n    # logger(experiment_name)\n    # run_and_store_episodes(env, agent, 3, experiment_name, store_results_func='database', verbosity='step')\n    run_episodes(env, agent, 1, verbosity='step', render=True)\n    # logger.save()\n    save_loggers()\n\nif __name__ == \"__main__\":\n\n    run_experiment()\n",
        "branches": [
            "rl.experiments.simple_2d.run_experiment.run_experiment"
        ]
    },
    "rl.experiments.simple_2d.run_experiment.run_experiment": {
        "type": "function",
        "code": "def run_experiment():\n    # agent = RandomWalkAgent()\n    # agent = MyAgent_Greedy_SE_POC()\n    # agent = MyAgent_ShortestPath_SE_POC()\n    agent = MyAgent_BestPath_SE_POC()\n\n    env = SimpleEnv()\n\n    experiment_name = 'simple_experiment_test'\n    # logger(experiment_name)\n    # run_and_store_episodes(env, agent, 3, experiment_name, store_results_func='database', verbosity='step')\n    run_episodes(env, agent, 1, verbosity='step', render=True)\n    # logger.save()\n    save_loggers()",
        "branches": []
    },
    "rl.experiments.simple_2d.simple_env": {
        "type": "module",
        "code": "import numpy as np\nimport cv2\nfrom perlin_noise import PerlinNoise\n\nfrom rl.core.rl.env import AbstractEnv\nfrom rl.core.utilities.logging import Logger\n\n\ndef perlin_map(width=100, height=100, octaves=3, seed=6):\n    noise = PerlinNoise(octaves=octaves, seed=seed)\n    xpix, ypix = width, height\n    pic = np.array([[noise([i / xpix, j / ypix]) for j in range(xpix)] for i in range(ypix)])\n    pic -= pic.min()\n    pic /= pic.max()\n\n    return np.array(pic)\n\n\nDEFAULT_MAP_WIDTH = 100\nDEFAULT_MAP_HEIGHT = 100\nDEFAULT_MAP = perlin_map(width=DEFAULT_MAP_WIDTH, height=DEFAULT_MAP_HEIGHT)\n\nMIN_ACTION = -1\nMAX_ACTION = 1\nSTEPS_PER_EPISODE = 100\nSTARTING_X = 50\nSTARTING_Y = 50\n\nlogger = Logger('simple_env')\n\n\nclass SimpleEnv(AbstractEnv):\n\n    name = 'SimpleEnv'\n\n    def __init__(self):\n        self.width, self.height = DEFAULT_MAP_WIDTH, DEFAULT_MAP_HEIGHT\n        self.map = DEFAULT_MAP\n        self.min_action, self.max_action = MIN_ACTION, MAX_ACTION\n        self.steps_per_episode = STEPS_PER_EPISODE\n\n        self.x, self.y = -1, -1\n        self.step_count = -1\n\n        self.track_x, self.track_y = [], []\n\n        max_1d = np.max(self.map, axis=0)\n        self.argmax_x = np.argmax(max_1d)\n        self.argmax_y = np.argmax(self.map[:, self.argmax_x])\n\n    def reset(self):\n        self.x, self.y = STARTING_X, STARTING_Y\n        self.step_count = -1\n\n        self.track_x, self.track_y = [self.x], [self.y]\n\n        state = np.array([self.x, self.y])\n        return state\n\n    def step(self, action):\n        action = np.clip(np.round(action),\n                         a_min=self.min_action,\n                         a_max=self.max_action)\n\n        state = np.array([self.x, self.y])\n\n        next_state = np.clip(state+action,\n                             a_min=[0, 0],\n                             a_max=[self.width-1, self.height-1])\n        self.x, self.y = next_state\n        self.track_x.append(self.x), self.track_y.append(self.y)\n\n        reward = self.map[self.y][self.x]\n\n        self.step_count += 1\n\n        done = self.step_count == self.steps_per_episode-1\n        return next_state, reward, done, None\n\n    @logger.log_func_call('run_time')\n    def render(self, width=500, height=500, fps=30):\n        approx_wait_time = 1000//fps\n\n        # m = self.map\n        # m = np.square(self.map)\n        m = np.round(self.map, 1)\n\n        r_img = np.zeros(m.shape)\n        g_img = m\n        b_img = np.zeros(m.shape)\n\n        for i, (x, y) in enumerate(zip(self.track_x, self.track_y)):\n            i_ratio = i/100.\n            r_img[y][x] = i_ratio*0.5\n            g_img[y][x] = 1-i_ratio\n            b_img[y][x] = 1\n\n        r_img[STARTING_Y][STARTING_X] = 1\n        g_img[STARTING_Y][STARTING_X] = 0\n        b_img[STARTING_Y][STARTING_X] = 1\n\n        r_img[self.y][self.x] = 1\n        g_img[self.y][self.x] = 0\n        b_img[self.y][self.x] = 0\n\n        r_img[self.argmax_y][self.argmax_x] = 0\n        g_img[self.argmax_y][self.argmax_x] = 0\n        b_img[self.argmax_y][self.argmax_x] = 1\n\n        p_img = np.dstack([b_img, g_img, r_img])\n        p_img = cv2.resize(p_img, (height, width), interpolation=cv2.INTER_NEAREST)#INTER_AREA\n        cv2.imshow('simple_env', p_img)\n        cv2.waitKey(approx_wait_time)\n\n    def __repr__(self):\n        return self.__class__\n\n\n",
        "branches": [
            "rl.experiments.simple_2d.simple_env.perlin_map",
            "rl.experiments.simple_2d.simple_env.SimpleEnv"
        ]
    },
    "rl.experiments.simple_2d.simple_env.perlin_map": {
        "type": "function",
        "code": "def perlin_map(width=100, height=100, octaves=3, seed=6):\n    noise = PerlinNoise(octaves=octaves, seed=seed)\n    xpix, ypix = width, height\n    pic = np.array([[noise([i / xpix, j / ypix]) for j in range(xpix)] for i in range(ypix)])\n    pic -= pic.min()\n    pic /= pic.max()\n\n    return np.array(pic)",
        "branches": []
    },
    "rl.experiments.simple_2d.simple_env.SimpleEnv": {
        "type": "class",
        "code": "class SimpleEnv(AbstractEnv):\n\n    name = 'SimpleEnv'\n\n    def __init__(self):\n        self.width, self.height = DEFAULT_MAP_WIDTH, DEFAULT_MAP_HEIGHT\n        self.map = DEFAULT_MAP\n        self.min_action, self.max_action = MIN_ACTION, MAX_ACTION\n        self.steps_per_episode = STEPS_PER_EPISODE\n\n        self.x, self.y = -1, -1\n        self.step_count = -1\n\n        self.track_x, self.track_y = [], []\n\n        max_1d = np.max(self.map, axis=0)\n        self.argmax_x = np.argmax(max_1d)\n        self.argmax_y = np.argmax(self.map[:, self.argmax_x])\n\n    def reset(self):\n        self.x, self.y = STARTING_X, STARTING_Y\n        self.step_count = -1\n\n        self.track_x, self.track_y = [self.x], [self.y]\n\n        state = np.array([self.x, self.y])\n        return state\n\n    def step(self, action):\n        action = np.clip(np.round(action),\n                         a_min=self.min_action,\n                         a_max=self.max_action)\n\n        state = np.array([self.x, self.y])\n\n        next_state = np.clip(state+action,\n                             a_min=[0, 0],\n                             a_max=[self.width-1, self.height-1])\n        self.x, self.y = next_state\n        self.track_x.append(self.x), self.track_y.append(self.y)\n\n        reward = self.map[self.y][self.x]\n\n        self.step_count += 1\n\n        done = self.step_count == self.steps_per_episode-1\n        return next_state, reward, done, None\n\n    @logger.log_func_call('run_time')\n    def render(self, width=500, height=500, fps=30):\n        approx_wait_time = 1000//fps\n\n        # m = self.map\n        # m = np.square(self.map)\n        m = np.round(self.map, 1)\n\n        r_img = np.zeros(m.shape)\n        g_img = m\n        b_img = np.zeros(m.shape)\n\n        for i, (x, y) in enumerate(zip(self.track_x, self.track_y)):\n            i_ratio = i/100.\n            r_img[y][x] = i_ratio*0.5\n            g_img[y][x] = 1-i_ratio\n            b_img[y][x] = 1\n\n        r_img[STARTING_Y][STARTING_X] = 1\n        g_img[STARTING_Y][STARTING_X] = 0\n        b_img[STARTING_Y][STARTING_X] = 1\n\n        r_img[self.y][self.x] = 1\n        g_img[self.y][self.x] = 0\n        b_img[self.y][self.x] = 0\n\n        r_img[self.argmax_y][self.argmax_x] = 0\n        g_img[self.argmax_y][self.argmax_x] = 0\n        b_img[self.argmax_y][self.argmax_x] = 1\n\n        p_img = np.dstack([b_img, g_img, r_img])\n        p_img = cv2.resize(p_img, (height, width), interpolation=cv2.INTER_NEAREST)#INTER_AREA\n        cv2.imshow('simple_env', p_img)\n        cv2.waitKey(approx_wait_time)\n\n    def __repr__(self):\n        return self.__class__",
        "branches": [
            "rl.experiments.simple_2d.simple_env.SimpleEnv.__init__",
            "rl.experiments.simple_2d.simple_env.SimpleEnv.reset",
            "rl.experiments.simple_2d.simple_env.SimpleEnv.step",
            "rl.experiments.simple_2d.simple_env.SimpleEnv.render",
            "rl.experiments.simple_2d.simple_env.SimpleEnv.__repr__"
        ]
    },
    "rl.experiments.simple_2d.simple_env.SimpleEnv.__init__": {
        "type": "class_method",
        "code": "def __init__(self):\n    self.width, self.height = DEFAULT_MAP_WIDTH, DEFAULT_MAP_HEIGHT\n    self.map = DEFAULT_MAP\n    self.min_action, self.max_action = MIN_ACTION, MAX_ACTION\n    self.steps_per_episode = STEPS_PER_EPISODE\n\n    self.x, self.y = -1, -1\n    self.step_count = -1\n\n    self.track_x, self.track_y = [], []\n\n    max_1d = np.max(self.map, axis=0)\n    self.argmax_x = np.argmax(max_1d)\n    self.argmax_y = np.argmax(self.map[:, self.argmax_x])",
        "branches": []
    },
    "rl.experiments.simple_2d.simple_env.SimpleEnv.reset": {
        "type": "class_method",
        "code": "def reset(self):\n    self.x, self.y = STARTING_X, STARTING_Y\n    self.step_count = -1\n\n    self.track_x, self.track_y = [self.x], [self.y]\n\n    state = np.array([self.x, self.y])\n    return state",
        "branches": []
    },
    "rl.experiments.simple_2d.simple_env.SimpleEnv.step": {
        "type": "class_method",
        "code": "def step(self, action):\n    action = np.clip(np.round(action),\n                     a_min=self.min_action,\n                     a_max=self.max_action)\n\n    state = np.array([self.x, self.y])\n\n    next_state = np.clip(state+action,\n                         a_min=[0, 0],\n                         a_max=[self.width-1, self.height-1])\n    self.x, self.y = next_state\n    self.track_x.append(self.x), self.track_y.append(self.y)\n\n    reward = self.map[self.y][self.x]\n\n    self.step_count += 1\n\n    done = self.step_count == self.steps_per_episode-1\n    return next_state, reward, done, None",
        "branches": []
    },
    "rl.experiments.simple_2d.simple_env.SimpleEnv.render": {
        "type": "class_method",
        "code": "@logger.log_func_call('run_time')\ndef render(self, width=500, height=500, fps=30):\n    approx_wait_time = 1000//fps\n\n    # m = self.map\n    # m = np.square(self.map)\n    m = np.round(self.map, 1)\n\n    r_img = np.zeros(m.shape)\n    g_img = m\n    b_img = np.zeros(m.shape)\n\n    for i, (x, y) in enumerate(zip(self.track_x, self.track_y)):\n        i_ratio = i/100.\n        r_img[y][x] = i_ratio*0.5\n        g_img[y][x] = 1-i_ratio\n        b_img[y][x] = 1\n\n    r_img[STARTING_Y][STARTING_X] = 1\n    g_img[STARTING_Y][STARTING_X] = 0\n    b_img[STARTING_Y][STARTING_X] = 1\n\n    r_img[self.y][self.x] = 1\n    g_img[self.y][self.x] = 0\n    b_img[self.y][self.x] = 0\n\n    r_img[self.argmax_y][self.argmax_x] = 0\n    g_img[self.argmax_y][self.argmax_x] = 0\n    b_img[self.argmax_y][self.argmax_x] = 1\n\n    p_img = np.dstack([b_img, g_img, r_img])\n    p_img = cv2.resize(p_img, (height, width), interpolation=cv2.INTER_NEAREST)#INTER_AREA\n    cv2.imshow('simple_env', p_img)\n    cv2.waitKey(approx_wait_time)",
        "branches": []
    },
    "rl.experiments.simple_2d.simple_env.SimpleEnv.__repr__": {
        "type": "class_method",
        "code": "def __repr__(self):\n    return self.__class__",
        "branches": []
    },
    "rl.experiments.simple_2d.__init__": {
        "type": "module",
        "code": "",
        "branches": []
    },
    "rl.experiments.test": {
        "type": "directory",
        "code": null,
        "branches": [
            "rl.experiments.test.run_experiment",
            "rl.experiments.test.__init__"
        ]
    },
    "rl.experiments.test.run_experiment": {
        "type": "module",
        "code": "from rl.core.rl.engine import run_and_store_episodes\n\nfrom rl.core.rl.env import AbstractEnv\n\n\ndef cnt():\n    i = 0\n    while True:\n        yield i\n        i += 1\n\n\ncnt_gen = cnt()\n\n\nclass TestEnv(AbstractEnv):\n    def __init__(self):\n        self.ep = -1\n        self.n_step = 0\n\n    def reset(self):\n        self.ep += 1\n        self.n_step = 0\n        res = [next(cnt_gen), next(cnt_gen)]\n        # row_num_obj[0] += 1\n        return res\n\n    def step(self, action):\n        self.n_step += 1\n        # print('step', row_num_obj[0])\n        res = [next(cnt_gen), next(cnt_gen)]\n        done = self.n_step >= 3**self.ep\n        reward = next(cnt_gen)\n        # row_num_obj[0] += 1\n        return res, reward, done, None\n\n    def __repr__(self):\n        return 'test_env'\n\n\nclass TestAgent:\n    def __init__(self):\n        pass\n        # self.df = df\n        # self.row_num = 0\n\n    def act(self, state):\n        res = next(cnt_gen)\n        # self.row_num += 1\n        return res\n\n    def set_env(self, env):\n        pass\n\n    def observe(self, *args):\n        pass\n\n    def name(self):\n        return 'test_agent'\n\n\ndef experiment_args():\n    agent = TestAgent()\n    env = TestEnv()\n    experiment_name = 'test_experiment'\n    res = {\n        'env': env,\n        'agent': agent,\n        'n_episodes': 3,\n        'experiment_name': experiment_name,\n        'verbosity': 'episode_step'\n    }\n\n    return res\n\n\ndef run_experiment():\n    agent = TestAgent()\n    env = TestEnv()\n    experiment_name = 'test_experiment'\n\n    run_and_store_episodes(env, agent, 3, experiment_name, store_results_func='database', verbosity='step')\n\n\nif __name__ == \"__main__\":\n    run_experiment()\n",
        "branches": [
            "rl.experiments.test.run_experiment.cnt",
            "rl.experiments.test.run_experiment.TestEnv",
            "rl.experiments.test.run_experiment.TestAgent",
            "rl.experiments.test.run_experiment.experiment_args",
            "rl.experiments.test.run_experiment.run_experiment"
        ]
    },
    "rl.experiments.test.run_experiment.cnt": {
        "type": "function",
        "code": "def cnt():\n    i = 0\n    while True:\n        yield i\n        i += 1",
        "branches": []
    },
    "rl.experiments.test.run_experiment.TestEnv": {
        "type": "class",
        "code": "class TestEnv(AbstractEnv):\n    def __init__(self):\n        self.ep = -1\n        self.n_step = 0\n\n    def reset(self):\n        self.ep += 1\n        self.n_step = 0\n        res = [next(cnt_gen), next(cnt_gen)]\n        # row_num_obj[0] += 1\n        return res\n\n    def step(self, action):\n        self.n_step += 1\n        # print('step', row_num_obj[0])\n        res = [next(cnt_gen), next(cnt_gen)]\n        done = self.n_step >= 3**self.ep\n        reward = next(cnt_gen)\n        # row_num_obj[0] += 1\n        return res, reward, done, None\n\n    def __repr__(self):\n        return 'test_env'",
        "branches": [
            "rl.experiments.test.run_experiment.TestEnv.__init__",
            "rl.experiments.test.run_experiment.TestEnv.reset",
            "rl.experiments.test.run_experiment.TestEnv.step",
            "rl.experiments.test.run_experiment.TestEnv.__repr__"
        ]
    },
    "rl.experiments.test.run_experiment.TestEnv.__init__": {
        "type": "class_method",
        "code": "def __init__(self):\n    self.ep = -1\n    self.n_step = 0",
        "branches": []
    },
    "rl.experiments.test.run_experiment.TestEnv.reset": {
        "type": "class_method",
        "code": "def reset(self):\n    self.ep += 1\n    self.n_step = 0\n    res = [next(cnt_gen), next(cnt_gen)]\n    # row_num_obj[0] += 1\n    return res",
        "branches": []
    },
    "rl.experiments.test.run_experiment.TestEnv.step": {
        "type": "class_method",
        "code": "def step(self, action):\n    self.n_step += 1\n    # print('step', row_num_obj[0])\n    res = [next(cnt_gen), next(cnt_gen)]\n    done = self.n_step >= 3**self.ep\n    reward = next(cnt_gen)\n    # row_num_obj[0] += 1\n    return res, reward, done, None",
        "branches": []
    },
    "rl.experiments.test.run_experiment.TestEnv.__repr__": {
        "type": "class_method",
        "code": "def __repr__(self):\n    return 'test_env'",
        "branches": []
    },
    "rl.experiments.test.run_experiment.TestAgent": {
        "type": "class",
        "code": "class TestAgent:\n    def __init__(self):\n        pass\n        # self.df = df\n        # self.row_num = 0\n\n    def act(self, state):\n        res = next(cnt_gen)\n        # self.row_num += 1\n        return res\n\n    def set_env(self, env):\n        pass\n\n    def observe(self, *args):\n        pass\n\n    def name(self):\n        return 'test_agent'",
        "branches": [
            "rl.experiments.test.run_experiment.TestAgent.__init__",
            "rl.experiments.test.run_experiment.TestAgent.act",
            "rl.experiments.test.run_experiment.TestAgent.set_env",
            "rl.experiments.test.run_experiment.TestAgent.observe",
            "rl.experiments.test.run_experiment.TestAgent.name"
        ]
    },
    "rl.experiments.test.run_experiment.TestAgent.__init__": {
        "type": "class_method",
        "code": "def __init__(self):\n    pass",
        "branches": []
    },
    "rl.experiments.test.run_experiment.TestAgent.act": {
        "type": "class_method",
        "code": "def act(self, state):\n    res = next(cnt_gen)\n    # self.row_num += 1\n    return res",
        "branches": []
    },
    "rl.experiments.test.run_experiment.TestAgent.set_env": {
        "type": "class_method",
        "code": "def set_env(self, env):\n    pass",
        "branches": []
    },
    "rl.experiments.test.run_experiment.TestAgent.observe": {
        "type": "class_method",
        "code": "def observe(self, *args):\n    pass",
        "branches": []
    },
    "rl.experiments.test.run_experiment.TestAgent.name": {
        "type": "class_method",
        "code": "def name(self):\n    return 'test_agent'",
        "branches": []
    },
    "rl.experiments.test.run_experiment.experiment_args": {
        "type": "function",
        "code": "def experiment_args():\n    agent = TestAgent()\n    env = TestEnv()\n    experiment_name = 'test_experiment'\n    res = {\n        'env': env,\n        'agent': agent,\n        'n_episodes': 3,\n        'experiment_name': experiment_name,\n        'verbosity': 'episode_step'\n    }\n\n    return res",
        "branches": []
    },
    "rl.experiments.test.run_experiment.run_experiment": {
        "type": "function",
        "code": "def run_experiment():\n    agent = TestAgent()\n    env = TestEnv()\n    experiment_name = 'test_experiment'\n\n    run_and_store_episodes(env, agent, 3, experiment_name, store_results_func='database', verbosity='step')",
        "branches": []
    },
    "rl.experiments.test.__init__": {
        "type": "module",
        "code": "",
        "branches": []
    },
    "rl.experiments.__init__": {
        "type": "module",
        "code": "",
        "branches": []
    },
    "rl.tests": {
        "type": "directory",
        "code": null,
        "branches": [
            "rl.tests.test_engine",
            "rl.tests.test_envs",
            "rl.tests.test_files",
            "rl.tests.test_misc",
            "rl.tests.test_storage",
            "rl.tests.__init__"
        ]
    },
    "rl.tests.test_engine": {
        "type": "module",
        "code": "import unittest\n\nimport pandas as pd\n\nfrom rl.src import engine\nfrom rl.src import download_df_from_db, execute_query, execute_query_and_return\n\n\nclass TestEngineMisc(unittest.TestCase):\n    def test_env_name_to_table(self):\n        self.assertEqual(engine.__env_name_to_table('<a<b<table-name>>>'), 'table_name')\n        self.assertEqual(engine.__env_name_to_table('table-name'), 'table_name')\n\n\nclass TestEngine(unittest.TestCase):\n\n    def setUp(self):\n        row_num_obj = [-1]\n        df = pd.DataFrame({\n            'episode': [0, 0, 0, 1, 1, 1, 1, 1, 2, 2],\n            'step': [0, 1, 2, 0, 1, 2, 3, 4, 0, 1],\n            'state_0': [1, 2, 3, 4, 5, 6, 7, 8, 9, 0],\n            'state_1': [11, 12, 13, 14, 15, 16, 17, 18, 19, 10],\n            'action_0': [100, 101, 102, 103, 104, 105, 106, 107, 108, 109],\n            'reward': [110, 111, 112, 113, 114, 115, 116, 117, 118, 119],\n            'done': [0, 1, -1, 0, 0, 0, 1, -1, 1, -1],\n        })\n\n        class TestAgent:\n            def __init__(self):\n                self.df = df\n                # self.row_num = 0\n\n            def act(self, state):\n                res = self.df['action_0'].tolist()[row_num_obj[0]]\n                # self.row_num += 1\n                return res\n\n            def set_env(self, env):\n                pass\n\n            def observe(self, *args):\n                pass\n\n            def name(self):\n                return 'test_agent'\n\n        class TestEnv:\n            def __init__(self):\n                self.df = df\n                # self.row_num = 0\n\n            def reset(self):\n                # print('reset', row_num_obj[0])\n                res = [self.df['state_0'].tolist()[row_num_obj[0]], self.df['state_1'].tolist()[row_num_obj[0]]]\n                row_num_obj[0] += 1\n                return res\n\n            def step(self, action):\n                # print('step', row_num_obj[0])\n                res = [self.df['state_0'].tolist()[row_num_obj[0]], self.df['state_1'].tolist()[row_num_obj[0]]]\n                done = self.df['done'].tolist()[row_num_obj[0]]\n                reward = 110+row_num_obj[0]\n                row_num_obj[0] += 1\n                return res, reward, done, None\n\n            def __repr__(self):\n                return 'test_env'\n\n        self.agent = TestAgent()\n        self.env = TestEnv()\n\n    def tearDown(self):\n        pass\n\n    def test_run_episodes_dataframe(self):\n        expected_res_df = pd.DataFrame({\n            'episode': [0, 0, 1, 1, 1, 1, 2],\n            'step': [0, 1, 0, 1, 2, 3, 0],\n            'state_0': [0, 1, 3, 4, 5, 6, 8],\n            'state_1': [10, 11, 13, 14, 15, 16, 18],\n            'action_0': [100, 101, 103, 104, 105, 106, 108],\n            'reward': [110, 111, 113, 114, 115, 116, 118],\n            'next_state_0': [1, 2, 4, 5, 6, 7, 9],\n            'next_state_1': [11, 12, 14, 15, 16, 17, 19],\n            'done': [0, 1, 0, 0, 0, 1, 1],\n            'experiment_id': ['test']*7\n        })\n        engine.run_episodes(self.env,\n                            self.agent,\n                            3,\n                            experiment_name='test',\n                            store_results='database')\n\n        res_df = download_df_from_db('test', 'test_env')\n        # res_df = pd.read_csv(\"../files/results/dataframes/test.csv\")\n        self.assertTrue((expected_res_df.reset_index(drop=True) == res_df.reset_index(drop=True)).all().all())#self.assertTrue(df.equals(res_df))\n\n        print(execute_query_and_return(\"select * from experiments where experiment_id='test'\"))\n        # self.assertTrue()\n        execute_query(\"delete from experiments where experiment_id='test'\")\n        execute_query(\"drop table test_env\")\n\n\nif __name__ == '__main__':\n    unittest.main()\n\n\n",
        "branches": [
            "rl.tests.test_engine.TestEngineMisc",
            "rl.tests.test_engine.TestEngine"
        ]
    },
    "rl.tests.test_engine.TestEngineMisc": {
        "type": "class",
        "code": "class TestEngineMisc(unittest.TestCase):\n    def test_env_name_to_table(self):\n        self.assertEqual(engine.__env_name_to_table('<a<b<table-name>>>'), 'table_name')\n        self.assertEqual(engine.__env_name_to_table('table-name'), 'table_name')",
        "branches": [
            "rl.tests.test_engine.TestEngineMisc.test_env_name_to_table"
        ]
    },
    "rl.tests.test_engine.TestEngineMisc.test_env_name_to_table": {
        "type": "class_method",
        "code": "def test_env_name_to_table(self):\n    self.assertEqual(engine.__env_name_to_table('<a<b<table-name>>>'), 'table_name')\n    self.assertEqual(engine.__env_name_to_table('table-name'), 'table_name')",
        "branches": []
    },
    "rl.tests.test_engine.TestEngine": {
        "type": "class",
        "code": "class TestEngine(unittest.TestCase):\n\n    def setUp(self):\n        row_num_obj = [-1]\n        df = pd.DataFrame({\n            'episode': [0, 0, 0, 1, 1, 1, 1, 1, 2, 2],\n            'step': [0, 1, 2, 0, 1, 2, 3, 4, 0, 1],\n            'state_0': [1, 2, 3, 4, 5, 6, 7, 8, 9, 0],\n            'state_1': [11, 12, 13, 14, 15, 16, 17, 18, 19, 10],\n            'action_0': [100, 101, 102, 103, 104, 105, 106, 107, 108, 109],\n            'reward': [110, 111, 112, 113, 114, 115, 116, 117, 118, 119],\n            'done': [0, 1, -1, 0, 0, 0, 1, -1, 1, -1],\n        })\n\n        class TestAgent:\n            def __init__(self):\n                self.df = df\n                # self.row_num = 0\n\n            def act(self, state):\n                res = self.df['action_0'].tolist()[row_num_obj[0]]\n                # self.row_num += 1\n                return res\n\n            def set_env(self, env):\n                pass\n\n            def observe(self, *args):\n                pass\n\n            def name(self):\n                return 'test_agent'\n\n        class TestEnv:\n            def __init__(self):\n                self.df = df\n                # self.row_num = 0\n\n            def reset(self):\n                # print('reset', row_num_obj[0])\n                res = [self.df['state_0'].tolist()[row_num_obj[0]], self.df['state_1'].tolist()[row_num_obj[0]]]\n                row_num_obj[0] += 1\n                return res\n\n            def step(self, action):\n                # print('step', row_num_obj[0])\n                res = [self.df['state_0'].tolist()[row_num_obj[0]], self.df['state_1'].tolist()[row_num_obj[0]]]\n                done = self.df['done'].tolist()[row_num_obj[0]]\n                reward = 110+row_num_obj[0]\n                row_num_obj[0] += 1\n                return res, reward, done, None\n\n            def __repr__(self):\n                return 'test_env'\n\n        self.agent = TestAgent()\n        self.env = TestEnv()\n\n    def tearDown(self):\n        pass\n\n    def test_run_episodes_dataframe(self):\n        expected_res_df = pd.DataFrame({\n            'episode': [0, 0, 1, 1, 1, 1, 2],\n            'step': [0, 1, 0, 1, 2, 3, 0],\n            'state_0': [0, 1, 3, 4, 5, 6, 8],\n            'state_1': [10, 11, 13, 14, 15, 16, 18],\n            'action_0': [100, 101, 103, 104, 105, 106, 108],\n            'reward': [110, 111, 113, 114, 115, 116, 118],\n            'next_state_0': [1, 2, 4, 5, 6, 7, 9],\n            'next_state_1': [11, 12, 14, 15, 16, 17, 19],\n            'done': [0, 1, 0, 0, 0, 1, 1],\n            'experiment_id': ['test']*7\n        })\n        engine.run_episodes(self.env,\n                            self.agent,\n                            3,\n                            experiment_name='test',\n                            store_results='database')\n\n        res_df = download_df_from_db('test', 'test_env')\n        # res_df = pd.read_csv(\"../files/results/dataframes/test.csv\")\n        self.assertTrue((expected_res_df.reset_index(drop=True) == res_df.reset_index(drop=True)).all().all())#self.assertTrue(df.equals(res_df))\n\n        print(execute_query_and_return(\"select * from experiments where experiment_id='test'\"))\n        # self.assertTrue()\n        execute_query(\"delete from experiments where experiment_id='test'\")\n        execute_query(\"drop table test_env\")",
        "branches": [
            "rl.tests.test_engine.TestEngine.setUp",
            "rl.tests.test_engine.TestEngine.tearDown",
            "rl.tests.test_engine.TestEngine.test_run_episodes_dataframe"
        ]
    },
    "rl.tests.test_engine.TestEngine.setUp": {
        "type": "class_method",
        "code": "def setUp(self):\n    row_num_obj = [-1]\n    df = pd.DataFrame({\n        'episode': [0, 0, 0, 1, 1, 1, 1, 1, 2, 2],\n        'step': [0, 1, 2, 0, 1, 2, 3, 4, 0, 1],\n        'state_0': [1, 2, 3, 4, 5, 6, 7, 8, 9, 0],\n        'state_1': [11, 12, 13, 14, 15, 16, 17, 18, 19, 10],\n        'action_0': [100, 101, 102, 103, 104, 105, 106, 107, 108, 109],\n        'reward': [110, 111, 112, 113, 114, 115, 116, 117, 118, 119],\n        'done': [0, 1, -1, 0, 0, 0, 1, -1, 1, -1],\n    })\n\n    class TestAgent:\n        def __init__(self):\n            self.df = df\n            # self.row_num = 0\n\n        def act(self, state):\n            res = self.df['action_0'].tolist()[row_num_obj[0]]\n            # self.row_num += 1\n            return res\n\n        def set_env(self, env):\n            pass\n\n        def observe(self, *args):\n            pass\n\n        def name(self):\n            return 'test_agent'\n\n    class TestEnv:\n        def __init__(self):\n            self.df = df\n            # self.row_num = 0\n\n        def reset(self):\n            # print('reset', row_num_obj[0])\n            res = [self.df['state_0'].tolist()[row_num_obj[0]], self.df['state_1'].tolist()[row_num_obj[0]]]\n            row_num_obj[0] += 1\n            return res\n\n        def step(self, action):\n            # print('step', row_num_obj[0])\n            res = [self.df['state_0'].tolist()[row_num_obj[0]], self.df['state_1'].tolist()[row_num_obj[0]]]\n            done = self.df['done'].tolist()[row_num_obj[0]]\n            reward = 110+row_num_obj[0]\n            row_num_obj[0] += 1\n            return res, reward, done, None\n\n        def __repr__(self):\n            return 'test_env'\n\n    self.agent = TestAgent()\n    self.env = TestEnv()",
        "branches": []
    },
    "rl.tests.test_engine.TestEngine.tearDown": {
        "type": "class_method",
        "code": "def tearDown(self):\n    pass",
        "branches": []
    },
    "rl.tests.test_engine.TestEngine.test_run_episodes_dataframe": {
        "type": "class_method",
        "code": "def test_run_episodes_dataframe(self):\n    expected_res_df = pd.DataFrame({\n        'episode': [0, 0, 1, 1, 1, 1, 2],\n        'step': [0, 1, 0, 1, 2, 3, 0],\n        'state_0': [0, 1, 3, 4, 5, 6, 8],\n        'state_1': [10, 11, 13, 14, 15, 16, 18],\n        'action_0': [100, 101, 103, 104, 105, 106, 108],\n        'reward': [110, 111, 113, 114, 115, 116, 118],\n        'next_state_0': [1, 2, 4, 5, 6, 7, 9],\n        'next_state_1': [11, 12, 14, 15, 16, 17, 19],\n        'done': [0, 1, 0, 0, 0, 1, 1],\n        'experiment_id': ['test']*7\n    })\n    engine.run_episodes(self.env,\n                        self.agent,\n                        3,\n                        experiment_name='test',\n                        store_results='database')\n\n    res_df = download_df_from_db('test', 'test_env')\n    # res_df = pd.read_csv(\"../files/results/dataframes/test.csv\")\n    self.assertTrue((expected_res_df.reset_index(drop=True) == res_df.reset_index(drop=True)).all().all())#self.assertTrue(df.equals(res_df))\n\n    print(execute_query_and_return(\"select * from experiments where experiment_id='test'\"))\n    # self.assertTrue()\n    execute_query(\"delete from experiments where experiment_id='test'\")\n    execute_query(\"drop table test_env\")",
        "branches": []
    },
    "rl.tests.test_envs": {
        "type": "module",
        "code": "import unittest\n\nfrom rl.src import envs\n\n\nclass TestEnvs(unittest.TestCase):\n\n    def test_gym_envs_list(self):\n        gym_envs = envs.gym_envs_list()\n        self.assertIsNotNone(gym_envs)\n        self.assertTrue(len(gym_envs) > 0)\n\n    def test_wrappable_envs(self):\n        gym_envs = envs.gym_envs_list()\n        wrapped_envs = envs.wrappable_gym_envs()\n\n        self.assertIsNotNone(wrapped_envs)\n        self.assertTrue(len(wrapped_envs) > 0)\n        self.assertTrue(len(wrapped_envs) <= len(gym_envs))\n\n    def test_wrap_env(self):\n        self.assertIsNotNone(envs.wrap_env('MountainCar-v0'))\n        self.assertIsNone(envs.wrap_env('random-text'))\n        self.assertIsNone(envs.wrap_env(None))\n\n    def test_is_action_space_discrete(self):\n        self.assertTrue(envs.wrap_env('MountainCar-v0').is_action_space_discrete())\n        self.assertTrue(not envs.wrap_env('MountainCarContinuous-v0').is_action_space_discrete())\n\n    def test_state_dims(self):\n        wrappable_envs = envs.wrappable_gym_envs()\n\n        for env_id in wrappable_envs:\n            env = envs.EnvWrapper(env_id)\n            self.assertTrue(env.state_dims() > 0)\n\n    def test_state_limits(self):\n        wrappable_envs = envs.wrappable_gym_envs()\n\n        for env_id in wrappable_envs:\n            env = envs.EnvWrapper(env_id)\n            state_lims = env.state_limits()\n            state_low, state_high = state_lims[0], state_lims[1]\n            diff = state_high-state_low\n            self.assertTrue(all(diff >= 0))\n\n    def test_action_dims(self):\n        wrappable_envs = envs.wrappable_gym_envs()\n\n        for env_id in wrappable_envs:\n            env = envs.EnvWrapper(env_id)\n            self.assertTrue(env.action_dims() > 0)\n            if env.is_action_space_discrete():\n                self.assertTrue(env.action_dims() == 1)\n\n    def test_action_limits(self):\n        wrappable_envs = envs.wrappable_gym_envs()\n\n        for env_id in wrappable_envs:\n            env = envs.EnvWrapper(env_id)\n            action_lims = env.action_limits()\n            action_low, action_high = action_lims[0], action_lims[1]\n            diff = action_high-action_low\n            self.assertTrue(all(diff >= 0))\n\n\nif __name__ == '__main__':\n    unittest.main()\n",
        "branches": [
            "rl.tests.test_envs.TestEnvs"
        ]
    },
    "rl.tests.test_envs.TestEnvs": {
        "type": "class",
        "code": "class TestEnvs(unittest.TestCase):\n\n    def test_gym_envs_list(self):\n        gym_envs = envs.gym_envs_list()\n        self.assertIsNotNone(gym_envs)\n        self.assertTrue(len(gym_envs) > 0)\n\n    def test_wrappable_envs(self):\n        gym_envs = envs.gym_envs_list()\n        wrapped_envs = envs.wrappable_gym_envs()\n\n        self.assertIsNotNone(wrapped_envs)\n        self.assertTrue(len(wrapped_envs) > 0)\n        self.assertTrue(len(wrapped_envs) <= len(gym_envs))\n\n    def test_wrap_env(self):\n        self.assertIsNotNone(envs.wrap_env('MountainCar-v0'))\n        self.assertIsNone(envs.wrap_env('random-text'))\n        self.assertIsNone(envs.wrap_env(None))\n\n    def test_is_action_space_discrete(self):\n        self.assertTrue(envs.wrap_env('MountainCar-v0').is_action_space_discrete())\n        self.assertTrue(not envs.wrap_env('MountainCarContinuous-v0').is_action_space_discrete())\n\n    def test_state_dims(self):\n        wrappable_envs = envs.wrappable_gym_envs()\n\n        for env_id in wrappable_envs:\n            env = envs.EnvWrapper(env_id)\n            self.assertTrue(env.state_dims() > 0)\n\n    def test_state_limits(self):\n        wrappable_envs = envs.wrappable_gym_envs()\n\n        for env_id in wrappable_envs:\n            env = envs.EnvWrapper(env_id)\n            state_lims = env.state_limits()\n            state_low, state_high = state_lims[0], state_lims[1]\n            diff = state_high-state_low\n            self.assertTrue(all(diff >= 0))\n\n    def test_action_dims(self):\n        wrappable_envs = envs.wrappable_gym_envs()\n\n        for env_id in wrappable_envs:\n            env = envs.EnvWrapper(env_id)\n            self.assertTrue(env.action_dims() > 0)\n            if env.is_action_space_discrete():\n                self.assertTrue(env.action_dims() == 1)\n\n    def test_action_limits(self):\n        wrappable_envs = envs.wrappable_gym_envs()\n\n        for env_id in wrappable_envs:\n            env = envs.EnvWrapper(env_id)\n            action_lims = env.action_limits()\n            action_low, action_high = action_lims[0], action_lims[1]\n            diff = action_high-action_low\n            self.assertTrue(all(diff >= 0))",
        "branches": [
            "rl.tests.test_envs.TestEnvs.test_gym_envs_list",
            "rl.tests.test_envs.TestEnvs.test_wrappable_envs",
            "rl.tests.test_envs.TestEnvs.test_wrap_env",
            "rl.tests.test_envs.TestEnvs.test_is_action_space_discrete",
            "rl.tests.test_envs.TestEnvs.test_state_dims",
            "rl.tests.test_envs.TestEnvs.test_state_limits",
            "rl.tests.test_envs.TestEnvs.test_action_dims",
            "rl.tests.test_envs.TestEnvs.test_action_limits"
        ]
    },
    "rl.tests.test_envs.TestEnvs.test_gym_envs_list": {
        "type": "class_method",
        "code": "def test_gym_envs_list(self):\n    gym_envs = envs.gym_envs_list()\n    self.assertIsNotNone(gym_envs)\n    self.assertTrue(len(gym_envs) > 0)",
        "branches": []
    },
    "rl.tests.test_envs.TestEnvs.test_wrappable_envs": {
        "type": "class_method",
        "code": "def test_wrappable_envs(self):\n    gym_envs = envs.gym_envs_list()\n    wrapped_envs = envs.wrappable_gym_envs()\n\n    self.assertIsNotNone(wrapped_envs)\n    self.assertTrue(len(wrapped_envs) > 0)\n    self.assertTrue(len(wrapped_envs) <= len(gym_envs))",
        "branches": []
    },
    "rl.tests.test_envs.TestEnvs.test_wrap_env": {
        "type": "class_method",
        "code": "def test_wrap_env(self):\n    self.assertIsNotNone(envs.wrap_env('MountainCar-v0'))\n    self.assertIsNone(envs.wrap_env('random-text'))\n    self.assertIsNone(envs.wrap_env(None))",
        "branches": []
    },
    "rl.tests.test_envs.TestEnvs.test_is_action_space_discrete": {
        "type": "class_method",
        "code": "def test_is_action_space_discrete(self):\n    self.assertTrue(envs.wrap_env('MountainCar-v0').is_action_space_discrete())\n    self.assertTrue(not envs.wrap_env('MountainCarContinuous-v0').is_action_space_discrete())",
        "branches": []
    },
    "rl.tests.test_envs.TestEnvs.test_state_dims": {
        "type": "class_method",
        "code": "def test_state_dims(self):\n    wrappable_envs = envs.wrappable_gym_envs()\n\n    for env_id in wrappable_envs:\n        env = envs.EnvWrapper(env_id)\n        self.assertTrue(env.state_dims() > 0)",
        "branches": []
    },
    "rl.tests.test_envs.TestEnvs.test_state_limits": {
        "type": "class_method",
        "code": "def test_state_limits(self):\n    wrappable_envs = envs.wrappable_gym_envs()\n\n    for env_id in wrappable_envs:\n        env = envs.EnvWrapper(env_id)\n        state_lims = env.state_limits()\n        state_low, state_high = state_lims[0], state_lims[1]\n        diff = state_high-state_low\n        self.assertTrue(all(diff >= 0))",
        "branches": []
    },
    "rl.tests.test_envs.TestEnvs.test_action_dims": {
        "type": "class_method",
        "code": "def test_action_dims(self):\n    wrappable_envs = envs.wrappable_gym_envs()\n\n    for env_id in wrappable_envs:\n        env = envs.EnvWrapper(env_id)\n        self.assertTrue(env.action_dims() > 0)\n        if env.is_action_space_discrete():\n            self.assertTrue(env.action_dims() == 1)",
        "branches": []
    },
    "rl.tests.test_envs.TestEnvs.test_action_limits": {
        "type": "class_method",
        "code": "def test_action_limits(self):\n    wrappable_envs = envs.wrappable_gym_envs()\n\n    for env_id in wrappable_envs:\n        env = envs.EnvWrapper(env_id)\n        action_lims = env.action_limits()\n        action_low, action_high = action_lims[0], action_lims[1]\n        diff = action_high-action_low\n        self.assertTrue(all(diff >= 0))",
        "branches": []
    },
    "rl.tests.test_files": {
        "type": "module",
        "code": "import unittest\n\nimport pandas as pd\npd.set_option('display.max_columns', None)\nimport numpy as np\n\nfrom sqlalchemy.exc import OperationalError\n\nfrom rl.src import files\n\n\nclass TestFiles(unittest.TestCase):\n    def test_db(self):\n        self.assertIsNotNone(files.__get_engine())\n\n        files.execute_query(\"drop table if exists test_table\")\n        self.assertRaises(OperationalError, files.execute_query, \"select * from test_table\")\n\n        files.execute_query(\"create table test_table(test_col text)\")\n        self.assertEqual(len(files.execute_query_and_return(\"select * from test_table\")), 0)\n\n        files.execute_query(\"drop table test_table\")\n\n    def test_experiments_table(self):\n        files.execute_query(\"delete from experiments where experiment_id='test_experiment_id'\")\n        self.assertFalse(files.exp_id_already_exists('test_experiment_id'))\n\n        files.execute_query(\"insert into experiments(experiment_id) values ('test_experiment_id')\")\n        self.assertTrue(files.exp_id_already_exists('test_experiment_id'))\n\n        files.execute_query(\"delete from experiments where experiment_id='test_experiment_id'\")\n        files.__add_experiment_info('test_experiment_id')\n        self.assertTrue(files.exp_id_already_exists('test_experiment_id'))\n\n        files.execute_query(\"delete from experiments where experiment_id='test_experiment_id'\")\n\n    def test_upload_and_download_dfs_on_db(self):\n        tablename = 'test_table'\n        N = 10\n\n        df = pd.DataFrame({\n            'episode': np.arange(N),\n            'col_0': np.random.random(size=N),\n            'col_1': np.random.random(size=N),\n            'col_2': np.random.random(size=N),\n            'col_3': np.random.random(size=N),\n        })\n        df['experiment_id'] = 'test_experiment_id'\n\n        files.execute_query(f\"drop table if exists {tablename}\")\n        files.upload_df_in_db(df, to_table=tablename)\n        res_df = files.download_df_from_db('test_experiment_id', from_table=tablename)\n\n        self.assertTrue((df == res_df).all().all())#self.assertTrue(df.equals(res_df))\n        files.execute_query(f\"drop table {tablename}\")\n\n    def test_data_to_df(self):\n        df = pd.DataFrame({\n            'episode': [0, 0, 0, 1, 1, 1, 1, 1, 2, 2],\n            'step': [0, 1, 2, 0, 1, 2, 3, 4, 0, 1],\n            'state_0': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9],\n            'state_1': [10, 11, 12, 13, 14, 15, 16, 17, 18, 19],\n            'action_0': [100, 101, 102, 103, 104, 105, 106, 107, 108, 109],\n            'reward': [110, 111, 112, 113, 114, 115, 116, 117, 118, 119],\n            'done': [0, 1, -1, 0, 0, 0, 1, -1, 1, -1],\n        })\n\n        episodes = df['episode'].tolist()\n        steps_list = df['step'].tolist()\n        states = list(zip(df['state_0'].tolist(), df['state_1'].tolist()))\n        actions = [[a] for a in df['action_0'].tolist()]\n        rewards = df['reward'].tolist()\n        dones = df['done'].tolist()\n\n        res_df = files.data_to_df(episodes, steps_list, states, actions, rewards, dones, compressed=True)\n        self.assertTrue((df == res_df).all().all())#self.assertTrue(df.equals(res_df))\n\n        df2 = pd.DataFrame({\n            'episode': [0, 0, 1, 1, 1, 1, 2],\n            'step': [0, 1, 0, 1, 2, 3, 0],\n            'state_0': [0, 1, 3, 4, 5, 6, 8],\n            'state_1': [10, 11, 13, 14, 15, 16, 18],\n            'action_0': [100, 101, 103, 104, 105, 106, 108],\n            'reward': [110, 111, 113, 114, 115, 116, 118],\n            'next_state_0': [1, 2, 4, 5, 6, 7, 9],\n            'next_state_1': [11, 12, 14, 15, 16, 17, 19],\n            'done': [0, 1, 0, 0, 0, 1, 1],\n        })\n        res_df = files.data_to_df(episodes, steps_list, states, actions, rewards, dones, compressed=False)\n        self.assertTrue((df2.reset_index(drop=True) == res_df.reset_index(drop=True)).all().all())#self.assertTrue(df.equals(res_df))\n\n\nif __name__ == '__main__':\n    unittest.main()\n",
        "branches": [
            "rl.tests.test_files.TestFiles"
        ]
    },
    "rl.tests.test_files.TestFiles": {
        "type": "class",
        "code": "class TestFiles(unittest.TestCase):\n    def test_db(self):\n        self.assertIsNotNone(files.__get_engine())\n\n        files.execute_query(\"drop table if exists test_table\")\n        self.assertRaises(OperationalError, files.execute_query, \"select * from test_table\")\n\n        files.execute_query(\"create table test_table(test_col text)\")\n        self.assertEqual(len(files.execute_query_and_return(\"select * from test_table\")), 0)\n\n        files.execute_query(\"drop table test_table\")\n\n    def test_experiments_table(self):\n        files.execute_query(\"delete from experiments where experiment_id='test_experiment_id'\")\n        self.assertFalse(files.exp_id_already_exists('test_experiment_id'))\n\n        files.execute_query(\"insert into experiments(experiment_id) values ('test_experiment_id')\")\n        self.assertTrue(files.exp_id_already_exists('test_experiment_id'))\n\n        files.execute_query(\"delete from experiments where experiment_id='test_experiment_id'\")\n        files.__add_experiment_info('test_experiment_id')\n        self.assertTrue(files.exp_id_already_exists('test_experiment_id'))\n\n        files.execute_query(\"delete from experiments where experiment_id='test_experiment_id'\")\n\n    def test_upload_and_download_dfs_on_db(self):\n        tablename = 'test_table'\n        N = 10\n\n        df = pd.DataFrame({\n            'episode': np.arange(N),\n            'col_0': np.random.random(size=N),\n            'col_1': np.random.random(size=N),\n            'col_2': np.random.random(size=N),\n            'col_3': np.random.random(size=N),\n        })\n        df['experiment_id'] = 'test_experiment_id'\n\n        files.execute_query(f\"drop table if exists {tablename}\")\n        files.upload_df_in_db(df, to_table=tablename)\n        res_df = files.download_df_from_db('test_experiment_id', from_table=tablename)\n\n        self.assertTrue((df == res_df).all().all())#self.assertTrue(df.equals(res_df))\n        files.execute_query(f\"drop table {tablename}\")\n\n    def test_data_to_df(self):\n        df = pd.DataFrame({\n            'episode': [0, 0, 0, 1, 1, 1, 1, 1, 2, 2],\n            'step': [0, 1, 2, 0, 1, 2, 3, 4, 0, 1],\n            'state_0': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9],\n            'state_1': [10, 11, 12, 13, 14, 15, 16, 17, 18, 19],\n            'action_0': [100, 101, 102, 103, 104, 105, 106, 107, 108, 109],\n            'reward': [110, 111, 112, 113, 114, 115, 116, 117, 118, 119],\n            'done': [0, 1, -1, 0, 0, 0, 1, -1, 1, -1],\n        })\n\n        episodes = df['episode'].tolist()\n        steps_list = df['step'].tolist()\n        states = list(zip(df['state_0'].tolist(), df['state_1'].tolist()))\n        actions = [[a] for a in df['action_0'].tolist()]\n        rewards = df['reward'].tolist()\n        dones = df['done'].tolist()\n\n        res_df = files.data_to_df(episodes, steps_list, states, actions, rewards, dones, compressed=True)\n        self.assertTrue((df == res_df).all().all())#self.assertTrue(df.equals(res_df))\n\n        df2 = pd.DataFrame({\n            'episode': [0, 0, 1, 1, 1, 1, 2],\n            'step': [0, 1, 0, 1, 2, 3, 0],\n            'state_0': [0, 1, 3, 4, 5, 6, 8],\n            'state_1': [10, 11, 13, 14, 15, 16, 18],\n            'action_0': [100, 101, 103, 104, 105, 106, 108],\n            'reward': [110, 111, 113, 114, 115, 116, 118],\n            'next_state_0': [1, 2, 4, 5, 6, 7, 9],\n            'next_state_1': [11, 12, 14, 15, 16, 17, 19],\n            'done': [0, 1, 0, 0, 0, 1, 1],\n        })\n        res_df = files.data_to_df(episodes, steps_list, states, actions, rewards, dones, compressed=False)\n        self.assertTrue((df2.reset_index(drop=True) == res_df.reset_index(drop=True)).all().all())#self.assertTrue(df.equals(res_df))",
        "branches": [
            "rl.tests.test_files.TestFiles.test_db",
            "rl.tests.test_files.TestFiles.test_experiments_table",
            "rl.tests.test_files.TestFiles.test_upload_and_download_dfs_on_db",
            "rl.tests.test_files.TestFiles.test_data_to_df"
        ]
    },
    "rl.tests.test_files.TestFiles.test_db": {
        "type": "class_method",
        "code": "def test_db(self):\n    self.assertIsNotNone(files.__get_engine())\n\n    files.execute_query(\"drop table if exists test_table\")\n    self.assertRaises(OperationalError, files.execute_query, \"select * from test_table\")\n\n    files.execute_query(\"create table test_table(test_col text)\")\n    self.assertEqual(len(files.execute_query_and_return(\"select * from test_table\")), 0)\n\n    files.execute_query(\"drop table test_table\")",
        "branches": []
    },
    "rl.tests.test_files.TestFiles.test_experiments_table": {
        "type": "class_method",
        "code": "def test_experiments_table(self):\n    files.execute_query(\"delete from experiments where experiment_id='test_experiment_id'\")\n    self.assertFalse(files.exp_id_already_exists('test_experiment_id'))\n\n    files.execute_query(\"insert into experiments(experiment_id) values ('test_experiment_id')\")\n    self.assertTrue(files.exp_id_already_exists('test_experiment_id'))\n\n    files.execute_query(\"delete from experiments where experiment_id='test_experiment_id'\")\n    files.__add_experiment_info('test_experiment_id')\n    self.assertTrue(files.exp_id_already_exists('test_experiment_id'))\n\n    files.execute_query(\"delete from experiments where experiment_id='test_experiment_id'\")",
        "branches": []
    },
    "rl.tests.test_files.TestFiles.test_upload_and_download_dfs_on_db": {
        "type": "class_method",
        "code": "def test_upload_and_download_dfs_on_db(self):\n    tablename = 'test_table'\n    N = 10\n\n    df = pd.DataFrame({\n        'episode': np.arange(N),\n        'col_0': np.random.random(size=N),\n        'col_1': np.random.random(size=N),\n        'col_2': np.random.random(size=N),\n        'col_3': np.random.random(size=N),\n    })\n    df['experiment_id'] = 'test_experiment_id'\n\n    files.execute_query(f\"drop table if exists {tablename}\")\n    files.upload_df_in_db(df, to_table=tablename)\n    res_df = files.download_df_from_db('test_experiment_id', from_table=tablename)\n\n    self.assertTrue((df == res_df).all().all())#self.assertTrue(df.equals(res_df))\n    files.execute_query(f\"drop table {tablename}\")",
        "branches": []
    },
    "rl.tests.test_files.TestFiles.test_data_to_df": {
        "type": "class_method",
        "code": "def test_data_to_df(self):\n    df = pd.DataFrame({\n        'episode': [0, 0, 0, 1, 1, 1, 1, 1, 2, 2],\n        'step': [0, 1, 2, 0, 1, 2, 3, 4, 0, 1],\n        'state_0': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9],\n        'state_1': [10, 11, 12, 13, 14, 15, 16, 17, 18, 19],\n        'action_0': [100, 101, 102, 103, 104, 105, 106, 107, 108, 109],\n        'reward': [110, 111, 112, 113, 114, 115, 116, 117, 118, 119],\n        'done': [0, 1, -1, 0, 0, 0, 1, -1, 1, -1],\n    })\n\n    episodes = df['episode'].tolist()\n    steps_list = df['step'].tolist()\n    states = list(zip(df['state_0'].tolist(), df['state_1'].tolist()))\n    actions = [[a] for a in df['action_0'].tolist()]\n    rewards = df['reward'].tolist()\n    dones = df['done'].tolist()\n\n    res_df = files.data_to_df(episodes, steps_list, states, actions, rewards, dones, compressed=True)\n    self.assertTrue((df == res_df).all().all())#self.assertTrue(df.equals(res_df))\n\n    df2 = pd.DataFrame({\n        'episode': [0, 0, 1, 1, 1, 1, 2],\n        'step': [0, 1, 0, 1, 2, 3, 0],\n        'state_0': [0, 1, 3, 4, 5, 6, 8],\n        'state_1': [10, 11, 13, 14, 15, 16, 18],\n        'action_0': [100, 101, 103, 104, 105, 106, 108],\n        'reward': [110, 111, 113, 114, 115, 116, 118],\n        'next_state_0': [1, 2, 4, 5, 6, 7, 9],\n        'next_state_1': [11, 12, 14, 15, 16, 17, 19],\n        'done': [0, 1, 0, 0, 0, 1, 1],\n    })\n    res_df = files.data_to_df(episodes, steps_list, states, actions, rewards, dones, compressed=False)\n    self.assertTrue((df2.reset_index(drop=True) == res_df.reset_index(drop=True)).all().all())#self.assertTrue(df.equals(res_df))",
        "branches": []
    },
    "rl.tests.test_misc": {
        "type": "module",
        "code": "import unittest\n\n\nclass TestClip(unittest.TestCase):\n    def test_clip(self):\n        self.assertEqual(clip(0, 1, 4), 1)\n        self.assertEqual(clip(1, 1, 4), 1)\n        self.assertEqual(clip(.999, 1, 4), 1)\n        self.assertEqual(clip(2, 1, 4), 2)\n        self.assertEqual(clip(3, 1, 4), 3)\n        self.assertEqual(clip(4, 1, 4), 4)\n        self.assertEqual(clip(4.001, 1, 4), 4)\n        self.assertEqual(clip(5, 1, 4), 4)\n\n        self.assertRaises(AttributeError, clip, 3, 3, 1)\n\n\nclass TestMap1D(unittest.TestCase):\n    def test_map_init(self):\n        self.assertIsNotNone(Map1D(0, 1, 0, 10))\n        self.assertRaises(AttributeError, Map1D, 1, 0, 0, 10)\n        self.assertRaises(AttributeError, Map1D, 0, 0, 0, 10)\n        self.assertRaises(AttributeError, Map1D, 0, 1, 10, 10)\n\n    def test_map_scale_unclipped(self):\n        map_unclipped = Map1D(4, 5, 0, 10, clip=False)\n        self.assertEqual(map_unclipped(2), -20)\n        self.assertEqual(map_unclipped(3), -10)\n        self.assertEqual(map_unclipped(3.5), -5)\n        self.assertEqual(map_unclipped(4), 0)\n        self.assertEqual(map_unclipped(4.5), 5)\n        self.assertEqual(map_unclipped(5), 10)\n        self.assertEqual(map_unclipped(6), 20)\n\n    def test_map_scale_clipped(self):\n        map_clipped = Map1D(4, 5, 0, 10, clip=True)\n        self.assertEqual(map_clipped(2), 0)\n        self.assertEqual(map_clipped(3), 0)\n        self.assertEqual(map_clipped(3.5), 0)\n        self.assertEqual(map_clipped(4), 0)\n        self.assertEqual(map_clipped(4.5), 5)\n        self.assertEqual(map_clipped(5), 10)\n        self.assertEqual(map_clipped(6), 10)\n\n    def test_map_inv_scale_unclipped(self):\n        map_unclipped = Map1D(4, 5, 10, 0, clip=False)\n        self.assertEqual(map_unclipped(2), 30)\n        self.assertEqual(map_unclipped(3), 20)\n        self.assertEqual(map_unclipped(3.5), 15)\n        self.assertEqual(map_unclipped(4), 10)\n        self.assertEqual(map_unclipped(4.5), 5)\n        self.assertEqual(map_unclipped(5), 0)\n        self.assertEqual(map_unclipped(6), -10)\n\n    def test_map_inv_scale_clipped(self):\n        map_clipped = Map1D(4, 5, 10, 0, clip=True)\n        self.assertEqual(map_clipped(2), 10)\n        self.assertEqual(map_clipped(3), 10)\n        self.assertEqual(map_clipped(3.5), 10)\n        self.assertEqual(map_clipped(4), 10)\n        self.assertEqual(map_clipped(4.5), 5)\n        self.assertEqual(map_clipped(5), 0)\n        self.assertEqual(map_clipped(6), 0)\n\n\nclass TestMapFloatToInteger(unittest.TestCase):\n    def test_init(self):\n        self.assertIsNotNone(MapFloatToInteger(0, 1, 3))\n        self.assertRaises(AttributeError, MapFloatToInteger, 1, 0, 3)\n        self.assertRaises(AttributeError, MapFloatToInteger, 0, 0, 3)\n        self.assertRaises(AttributeError, MapFloatToInteger, 0, 1, 1)\n        self.assertRaises(AttributeError, MapFloatToInteger, 0, 1, 2.0)\n\n    def test_map_clipped(self):\n        mapper_clipped = MapFloatToInteger(-1.2, 0.6, 3, clip_flag=True)\n        self.assertEqual(mapper_clipped.map(-12), 0)\n        self.assertEqual(mapper_clipped.map(-1.2), 0)\n        self.assertEqual(mapper_clipped.map(-0.6), 1)\n        self.assertEqual(mapper_clipped.map(0.6), 2)\n        self.assertEqual(mapper_clipped.map(6), 2)\n\n    def test_map_unclipped(self):\n        mapper_unclipped = MapFloatToInteger(-1, 1, 2, clip_flag=False)\n        self.assertEqual(mapper_unclipped.map(-2.1), -1)\n        self.assertEqual(mapper_unclipped.map(-1), 0)\n        self.assertEqual(mapper_unclipped.map(-0.01), 0)\n        self.assertEqual(mapper_unclipped.map(0), 0)\n        self.assertEqual(mapper_unclipped.map(0.01), 1)\n        self.assertEqual(mapper_unclipped.map(1), 1)\n        self.assertEqual(mapper_unclipped.map(2), 2)\n\n    def test_reverse_clipped(self):\n        mapper_clipped = MapFloatToInteger(-1, 1, 5, clip_flag=True)\n        self.assertEqual(mapper_clipped.reverse(-1), -1)\n        self.assertEqual(mapper_clipped.reverse(0), -1)\n        self.assertEqual(mapper_clipped.reverse(1), -.5)\n        self.assertEqual(mapper_clipped.reverse(2), 0)\n        self.assertEqual(mapper_clipped.reverse(3), .5)\n        self.assertEqual(mapper_clipped.reverse(4), 1)\n        self.assertEqual(mapper_clipped.reverse(5), 1)\n\n    def test_reverse_unclipped(self):\n        mapper_unclipped = MapFloatToInteger(-1, 1, 5, clip_flag=False)\n        self.assertEqual(mapper_unclipped.reverse(-1), -1.5)\n        self.assertEqual(mapper_unclipped.reverse(0), -1)\n        self.assertEqual(mapper_unclipped.reverse(1), -.5)\n        self.assertEqual(mapper_unclipped.reverse(2), 0)\n        self.assertEqual(mapper_unclipped.reverse(3), .5)\n        self.assertEqual(mapper_unclipped.reverse(4), 1)\n        self.assertEqual(mapper_unclipped.reverse(5), 1.5)\n\n\nif __name__ == '__main__':\n    unittest.main()\n",
        "branches": [
            "rl.tests.test_misc.TestClip",
            "rl.tests.test_misc.TestMap1D",
            "rl.tests.test_misc.TestMapFloatToInteger"
        ]
    },
    "rl.tests.test_misc.TestClip": {
        "type": "class",
        "code": "class TestClip(unittest.TestCase):\n    def test_clip(self):\n        self.assertEqual(clip(0, 1, 4), 1)\n        self.assertEqual(clip(1, 1, 4), 1)\n        self.assertEqual(clip(.999, 1, 4), 1)\n        self.assertEqual(clip(2, 1, 4), 2)\n        self.assertEqual(clip(3, 1, 4), 3)\n        self.assertEqual(clip(4, 1, 4), 4)\n        self.assertEqual(clip(4.001, 1, 4), 4)\n        self.assertEqual(clip(5, 1, 4), 4)\n\n        self.assertRaises(AttributeError, clip, 3, 3, 1)",
        "branches": [
            "rl.tests.test_misc.TestClip.test_clip"
        ]
    },
    "rl.tests.test_misc.TestClip.test_clip": {
        "type": "class_method",
        "code": "def test_clip(self):\n    self.assertEqual(clip(0, 1, 4), 1)\n    self.assertEqual(clip(1, 1, 4), 1)\n    self.assertEqual(clip(.999, 1, 4), 1)\n    self.assertEqual(clip(2, 1, 4), 2)\n    self.assertEqual(clip(3, 1, 4), 3)\n    self.assertEqual(clip(4, 1, 4), 4)\n    self.assertEqual(clip(4.001, 1, 4), 4)\n    self.assertEqual(clip(5, 1, 4), 4)\n\n    self.assertRaises(AttributeError, clip, 3, 3, 1)",
        "branches": []
    },
    "rl.tests.test_misc.TestMap1D": {
        "type": "class",
        "code": "class TestMap1D(unittest.TestCase):\n    def test_map_init(self):\n        self.assertIsNotNone(Map1D(0, 1, 0, 10))\n        self.assertRaises(AttributeError, Map1D, 1, 0, 0, 10)\n        self.assertRaises(AttributeError, Map1D, 0, 0, 0, 10)\n        self.assertRaises(AttributeError, Map1D, 0, 1, 10, 10)\n\n    def test_map_scale_unclipped(self):\n        map_unclipped = Map1D(4, 5, 0, 10, clip=False)\n        self.assertEqual(map_unclipped(2), -20)\n        self.assertEqual(map_unclipped(3), -10)\n        self.assertEqual(map_unclipped(3.5), -5)\n        self.assertEqual(map_unclipped(4), 0)\n        self.assertEqual(map_unclipped(4.5), 5)\n        self.assertEqual(map_unclipped(5), 10)\n        self.assertEqual(map_unclipped(6), 20)\n\n    def test_map_scale_clipped(self):\n        map_clipped = Map1D(4, 5, 0, 10, clip=True)\n        self.assertEqual(map_clipped(2), 0)\n        self.assertEqual(map_clipped(3), 0)\n        self.assertEqual(map_clipped(3.5), 0)\n        self.assertEqual(map_clipped(4), 0)\n        self.assertEqual(map_clipped(4.5), 5)\n        self.assertEqual(map_clipped(5), 10)\n        self.assertEqual(map_clipped(6), 10)\n\n    def test_map_inv_scale_unclipped(self):\n        map_unclipped = Map1D(4, 5, 10, 0, clip=False)\n        self.assertEqual(map_unclipped(2), 30)\n        self.assertEqual(map_unclipped(3), 20)\n        self.assertEqual(map_unclipped(3.5), 15)\n        self.assertEqual(map_unclipped(4), 10)\n        self.assertEqual(map_unclipped(4.5), 5)\n        self.assertEqual(map_unclipped(5), 0)\n        self.assertEqual(map_unclipped(6), -10)\n\n    def test_map_inv_scale_clipped(self):\n        map_clipped = Map1D(4, 5, 10, 0, clip=True)\n        self.assertEqual(map_clipped(2), 10)\n        self.assertEqual(map_clipped(3), 10)\n        self.assertEqual(map_clipped(3.5), 10)\n        self.assertEqual(map_clipped(4), 10)\n        self.assertEqual(map_clipped(4.5), 5)\n        self.assertEqual(map_clipped(5), 0)\n        self.assertEqual(map_clipped(6), 0)",
        "branches": [
            "rl.tests.test_misc.TestMap1D.test_map_init",
            "rl.tests.test_misc.TestMap1D.test_map_scale_unclipped",
            "rl.tests.test_misc.TestMap1D.test_map_scale_clipped",
            "rl.tests.test_misc.TestMap1D.test_map_inv_scale_unclipped",
            "rl.tests.test_misc.TestMap1D.test_map_inv_scale_clipped"
        ]
    },
    "rl.tests.test_misc.TestMap1D.test_map_init": {
        "type": "class_method",
        "code": "def test_map_init(self):\n    self.assertIsNotNone(Map1D(0, 1, 0, 10))\n    self.assertRaises(AttributeError, Map1D, 1, 0, 0, 10)\n    self.assertRaises(AttributeError, Map1D, 0, 0, 0, 10)\n    self.assertRaises(AttributeError, Map1D, 0, 1, 10, 10)",
        "branches": []
    },
    "rl.tests.test_misc.TestMap1D.test_map_scale_unclipped": {
        "type": "class_method",
        "code": "def test_map_scale_unclipped(self):\n    map_unclipped = Map1D(4, 5, 0, 10, clip=False)\n    self.assertEqual(map_unclipped(2), -20)\n    self.assertEqual(map_unclipped(3), -10)\n    self.assertEqual(map_unclipped(3.5), -5)\n    self.assertEqual(map_unclipped(4), 0)\n    self.assertEqual(map_unclipped(4.5), 5)\n    self.assertEqual(map_unclipped(5), 10)\n    self.assertEqual(map_unclipped(6), 20)",
        "branches": []
    },
    "rl.tests.test_misc.TestMap1D.test_map_scale_clipped": {
        "type": "class_method",
        "code": "def test_map_scale_clipped(self):\n    map_clipped = Map1D(4, 5, 0, 10, clip=True)\n    self.assertEqual(map_clipped(2), 0)\n    self.assertEqual(map_clipped(3), 0)\n    self.assertEqual(map_clipped(3.5), 0)\n    self.assertEqual(map_clipped(4), 0)\n    self.assertEqual(map_clipped(4.5), 5)\n    self.assertEqual(map_clipped(5), 10)\n    self.assertEqual(map_clipped(6), 10)",
        "branches": []
    },
    "rl.tests.test_misc.TestMap1D.test_map_inv_scale_unclipped": {
        "type": "class_method",
        "code": "def test_map_inv_scale_unclipped(self):\n    map_unclipped = Map1D(4, 5, 10, 0, clip=False)\n    self.assertEqual(map_unclipped(2), 30)\n    self.assertEqual(map_unclipped(3), 20)\n    self.assertEqual(map_unclipped(3.5), 15)\n    self.assertEqual(map_unclipped(4), 10)\n    self.assertEqual(map_unclipped(4.5), 5)\n    self.assertEqual(map_unclipped(5), 0)\n    self.assertEqual(map_unclipped(6), -10)",
        "branches": []
    },
    "rl.tests.test_misc.TestMap1D.test_map_inv_scale_clipped": {
        "type": "class_method",
        "code": "def test_map_inv_scale_clipped(self):\n    map_clipped = Map1D(4, 5, 10, 0, clip=True)\n    self.assertEqual(map_clipped(2), 10)\n    self.assertEqual(map_clipped(3), 10)\n    self.assertEqual(map_clipped(3.5), 10)\n    self.assertEqual(map_clipped(4), 10)\n    self.assertEqual(map_clipped(4.5), 5)\n    self.assertEqual(map_clipped(5), 0)\n    self.assertEqual(map_clipped(6), 0)",
        "branches": []
    },
    "rl.tests.test_misc.TestMapFloatToInteger": {
        "type": "class",
        "code": "class TestMapFloatToInteger(unittest.TestCase):\n    def test_init(self):\n        self.assertIsNotNone(MapFloatToInteger(0, 1, 3))\n        self.assertRaises(AttributeError, MapFloatToInteger, 1, 0, 3)\n        self.assertRaises(AttributeError, MapFloatToInteger, 0, 0, 3)\n        self.assertRaises(AttributeError, MapFloatToInteger, 0, 1, 1)\n        self.assertRaises(AttributeError, MapFloatToInteger, 0, 1, 2.0)\n\n    def test_map_clipped(self):\n        mapper_clipped = MapFloatToInteger(-1.2, 0.6, 3, clip_flag=True)\n        self.assertEqual(mapper_clipped.map(-12), 0)\n        self.assertEqual(mapper_clipped.map(-1.2), 0)\n        self.assertEqual(mapper_clipped.map(-0.6), 1)\n        self.assertEqual(mapper_clipped.map(0.6), 2)\n        self.assertEqual(mapper_clipped.map(6), 2)\n\n    def test_map_unclipped(self):\n        mapper_unclipped = MapFloatToInteger(-1, 1, 2, clip_flag=False)\n        self.assertEqual(mapper_unclipped.map(-2.1), -1)\n        self.assertEqual(mapper_unclipped.map(-1), 0)\n        self.assertEqual(mapper_unclipped.map(-0.01), 0)\n        self.assertEqual(mapper_unclipped.map(0), 0)\n        self.assertEqual(mapper_unclipped.map(0.01), 1)\n        self.assertEqual(mapper_unclipped.map(1), 1)\n        self.assertEqual(mapper_unclipped.map(2), 2)\n\n    def test_reverse_clipped(self):\n        mapper_clipped = MapFloatToInteger(-1, 1, 5, clip_flag=True)\n        self.assertEqual(mapper_clipped.reverse(-1), -1)\n        self.assertEqual(mapper_clipped.reverse(0), -1)\n        self.assertEqual(mapper_clipped.reverse(1), -.5)\n        self.assertEqual(mapper_clipped.reverse(2), 0)\n        self.assertEqual(mapper_clipped.reverse(3), .5)\n        self.assertEqual(mapper_clipped.reverse(4), 1)\n        self.assertEqual(mapper_clipped.reverse(5), 1)\n\n    def test_reverse_unclipped(self):\n        mapper_unclipped = MapFloatToInteger(-1, 1, 5, clip_flag=False)\n        self.assertEqual(mapper_unclipped.reverse(-1), -1.5)\n        self.assertEqual(mapper_unclipped.reverse(0), -1)\n        self.assertEqual(mapper_unclipped.reverse(1), -.5)\n        self.assertEqual(mapper_unclipped.reverse(2), 0)\n        self.assertEqual(mapper_unclipped.reverse(3), .5)\n        self.assertEqual(mapper_unclipped.reverse(4), 1)\n        self.assertEqual(mapper_unclipped.reverse(5), 1.5)",
        "branches": [
            "rl.tests.test_misc.TestMapFloatToInteger.test_init",
            "rl.tests.test_misc.TestMapFloatToInteger.test_map_clipped",
            "rl.tests.test_misc.TestMapFloatToInteger.test_map_unclipped",
            "rl.tests.test_misc.TestMapFloatToInteger.test_reverse_clipped",
            "rl.tests.test_misc.TestMapFloatToInteger.test_reverse_unclipped"
        ]
    },
    "rl.tests.test_misc.TestMapFloatToInteger.test_init": {
        "type": "class_method",
        "code": "def test_init(self):\n    self.assertIsNotNone(MapFloatToInteger(0, 1, 3))\n    self.assertRaises(AttributeError, MapFloatToInteger, 1, 0, 3)\n    self.assertRaises(AttributeError, MapFloatToInteger, 0, 0, 3)\n    self.assertRaises(AttributeError, MapFloatToInteger, 0, 1, 1)\n    self.assertRaises(AttributeError, MapFloatToInteger, 0, 1, 2.0)",
        "branches": []
    },
    "rl.tests.test_misc.TestMapFloatToInteger.test_map_clipped": {
        "type": "class_method",
        "code": "def test_map_clipped(self):\n    mapper_clipped = MapFloatToInteger(-1.2, 0.6, 3, clip_flag=True)\n    self.assertEqual(mapper_clipped.map(-12), 0)\n    self.assertEqual(mapper_clipped.map(-1.2), 0)\n    self.assertEqual(mapper_clipped.map(-0.6), 1)\n    self.assertEqual(mapper_clipped.map(0.6), 2)\n    self.assertEqual(mapper_clipped.map(6), 2)",
        "branches": []
    },
    "rl.tests.test_misc.TestMapFloatToInteger.test_map_unclipped": {
        "type": "class_method",
        "code": "def test_map_unclipped(self):\n    mapper_unclipped = MapFloatToInteger(-1, 1, 2, clip_flag=False)\n    self.assertEqual(mapper_unclipped.map(-2.1), -1)\n    self.assertEqual(mapper_unclipped.map(-1), 0)\n    self.assertEqual(mapper_unclipped.map(-0.01), 0)\n    self.assertEqual(mapper_unclipped.map(0), 0)\n    self.assertEqual(mapper_unclipped.map(0.01), 1)\n    self.assertEqual(mapper_unclipped.map(1), 1)\n    self.assertEqual(mapper_unclipped.map(2), 2)",
        "branches": []
    },
    "rl.tests.test_misc.TestMapFloatToInteger.test_reverse_clipped": {
        "type": "class_method",
        "code": "def test_reverse_clipped(self):\n    mapper_clipped = MapFloatToInteger(-1, 1, 5, clip_flag=True)\n    self.assertEqual(mapper_clipped.reverse(-1), -1)\n    self.assertEqual(mapper_clipped.reverse(0), -1)\n    self.assertEqual(mapper_clipped.reverse(1), -.5)\n    self.assertEqual(mapper_clipped.reverse(2), 0)\n    self.assertEqual(mapper_clipped.reverse(3), .5)\n    self.assertEqual(mapper_clipped.reverse(4), 1)\n    self.assertEqual(mapper_clipped.reverse(5), 1)",
        "branches": []
    },
    "rl.tests.test_misc.TestMapFloatToInteger.test_reverse_unclipped": {
        "type": "class_method",
        "code": "def test_reverse_unclipped(self):\n    mapper_unclipped = MapFloatToInteger(-1, 1, 5, clip_flag=False)\n    self.assertEqual(mapper_unclipped.reverse(-1), -1.5)\n    self.assertEqual(mapper_unclipped.reverse(0), -1)\n    self.assertEqual(mapper_unclipped.reverse(1), -.5)\n    self.assertEqual(mapper_unclipped.reverse(2), 0)\n    self.assertEqual(mapper_unclipped.reverse(3), .5)\n    self.assertEqual(mapper_unclipped.reverse(4), 1)\n    self.assertEqual(mapper_unclipped.reverse(5), 1.5)",
        "branches": []
    },
    "rl.tests.test_storage": {
        "type": "module",
        "code": "import unittest\n\nimport pandas as pd\npd.set_option('display.max_columns', None)\n\nfrom rl.core.storage.storage import *\n\n\nclass TestStorage(unittest.TestCase):\n    def test_df_filename(self):\n        pass\n\n    def test_store_dataframe(self):\n        df = pd.get_dummies(pd.Series(list('abc')), dtype=float)\n        store_dataframe(df)\n\n",
        "branches": [
            "rl.tests.test_storage.TestStorage"
        ]
    },
    "rl.tests.test_storage.TestStorage": {
        "type": "class",
        "code": "class TestStorage(unittest.TestCase):\n    def test_df_filename(self):\n        pass\n\n    def test_store_dataframe(self):\n        df = pd.get_dummies(pd.Series(list('abc')), dtype=float)\n        store_dataframe(df)",
        "branches": [
            "rl.tests.test_storage.TestStorage.test_df_filename",
            "rl.tests.test_storage.TestStorage.test_store_dataframe"
        ]
    },
    "rl.tests.test_storage.TestStorage.test_df_filename": {
        "type": "class_method",
        "code": "def test_df_filename(self):\n    pass",
        "branches": []
    },
    "rl.tests.test_storage.TestStorage.test_store_dataframe": {
        "type": "class_method",
        "code": "def test_store_dataframe(self):\n    df = pd.get_dummies(pd.Series(list('abc')), dtype=float)\n    store_dataframe(df)",
        "branches": []
    },
    "rl.tests.__init__": {
        "type": "module",
        "code": "",
        "branches": []
    },
    "rl.__init__": {
        "type": "module",
        "code": "",
        "branches": []
    }
}